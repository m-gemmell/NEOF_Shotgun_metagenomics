--- 
title: "Shotgun Metagenomics"
author: "Sam Haldenby and Matthew R. Gemmell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/NEOF.png", auto_pdf = TRUE)
``` 

# Introduction
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/squid.png", auto_pdf = TRUE)
``` 

This practical session aims to introduce you to the analysis of Shotgun metagenomic data. The topics covered are:

- Overview
- Raw data
- Trimming data
- Host removal
- Taxonomic profiling
- Functional profiling
- Metagenome assembly
- Binning
- Gene prediction
- Functional annotation

<!--chapter:end:01-Shotgun_metagenomics.Rmd-->

# Overview
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/overview.png", auto_pdf = TRUE)
``` 

## What is metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/what.png", auto_pdf = TRUE)
``` 

__Meta /ˈmɛtə/ : prefix meaning “higher” or “beyond”__

Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome.

## Why metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/why.png", auto_pdf = TRUE)
``` 

Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. 

While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species' neighbours, i.e. what else is in its natural environment?  Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture.

## Metagenomics vs Metagenetics
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/16s_vs_shotgun.png", auto_pdf = TRUE)
``` 

Broadly speaking, there are two families of metagenomic analysis: 

- __Amplicon-based__: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions.
- __Shotgun__: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referenced as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session.

## Tutorial overview
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/tutorial.png", auto_pdf = TRUE)
``` 

### Basics

This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. 

The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. 

### Structure

We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. There will be no instructor telling you what to type and click. Instead, everything you require to carry out the practical is written in the document. Take your time; it's important to spend some time understanding why you are running the commands, rather than simply typing them out. 

If at any point you are having trouble or have a question, let one of us know and we'll provide 1-to-1 assistance.

### Content
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/sections.png", auto_pdf = TRUE)
``` 

This practical is broken up into the following broad sections.

1. __Raw data__: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples.
2. __Trimming data__: This entails preprocessing our data to ensure that it is of good quality.
3. __Host removal__: When sequencing the genomic content of host's microbiota (bacteriome, archaeome, mycobiome, and more) it is likely you will also sequence the host's genome. This step shows a method of removing possible host contamination.
3. __Taxonomic profiling__: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples.
4. __Functional profiling__: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples.
5. __Metagenome assembly__: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will 'stitch' the reads together to ensure we get the best assembly possible.
7. __Binning__: This step attempts to seperate each assembled genomes into bins. These genome assemblies are called Metagenome-assembled Genomes (MAGs).
6. __Gene prediction__:  We will take our MAGs, search for genes...
7. __Functional annotation__: ...and then functionally annotate them with information from various databases. We will then visualise some of the output.

__Don't worry if you don't manage to finish the whole practical!__  

The more commonly used analyses have been put at the front of the practical (Section 1-4) with the less standard ones being placed towards the end. We will provide you with all of the intermediate and results files on request.

<!--chapter:end:02-Overview.Rmd-->

# Before we start
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/start.png", auto_pdf = TRUE)
``` 

During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command (ensure this starts with a full stop and a space `. `):

```{bash eval=FALSE}
. useshotgun
```

The `use` scripts in this workshop are custom scripts that set up conda environments. You can look at the above script with `less /usr/local/bin/useshotgun` if you are interested in its contents.

Also, there’s a chance you’re currently not in your home directory, so let’s make sure you are with the following command:

```{bash eval=FALSE}
cd ~
```

<!--chapter:end:03-Start.Rmd-->

# Cluster Introduction
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/cluster.png", auto_pdf = TRUE)
``` 

## Logon instructions
For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent.

You will now be in a logged-in Linux VNC desktop. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance.

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/logon_pic.png", auto_pdf = TRUE)
``` 

If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal.

These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Ubuntu. 

The following link is a guide to install Ubuntu:  
https://www.ubuntu.com/download/desktop/install-ubuntu-desktop.  
If you use a USB you need to create a bootable USB stick. The following link will assist:  
https://www.ubuntu.com/download/desktop/create-a-usb-stick-on-windows 

## The Terminal Window
In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different.
```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/terminal_window.png", auto_pdf = TRUE)
``` 

Already there is useful information for us on the terminal window.

- __nsc065__: This is the login name, also known as the username. In this case nsc065 is a demonstrator's account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into.
- __gauss03__: This is the machine name the user is logged into.
- __\~__: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others __'~'__ is a shortcut to the user's home directory.
- Everything after the __'$'__ is where commands are typed into the terminal. This is also referred to as the command line.

__To open a new terminal window__, right click on the main screen, choose `Applications` -> `Shell` -> `bash`

<!--chapter:end:04-Cluster_Introduction.Rmd-->

# Raw data
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/usb_stick.png", auto_pdf = TRUE)
``` 

The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets.

Have a browse of some of the projects by selecting one of the biomes on this page.

We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558).

## Obtaining the data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/link.png", auto_pdf = TRUE)
``` 

First, we need to create a directory to put the data in and then change directory to it.

```{bash eval=FALSE}
mkdir 1-Raw
cd 1-Raw
```

Now we can generate a symbolic links (i.e. shortcut) to the raw sequence data files, which will appear in the current directory:

```{bash eval=FALSE}
ln -s /pub39/tea/matthew/NEOF/Shotgun_metagenomics/raw_fastq/* .
```

If you would like to know more about the `ln` command please check out: https://linuxize.com/post/how-to-create-symbolic-links-in-linux-using-the-ln-command/.

Now, check the symbolic links are in your current directory:

```{bash eval=FALSE}
ls
```

There should be six files in the directory, two for each sample in the dataset. e.g. `K1_R1.fastq.gz`

The file ID has three components:

- K1 is the sample ID. 
- R1 is for the forward reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). 
- fastq.gz tells us that this is a zipped FASTQ file.

The sample labelling indicates the type treatment samples. The three samples are:

- __K1__: Fecal sample of individual of Korean diets
- __K2__: Fecal sample of individual of Korean diets
- __W1__: Fecal sample of individual of Western diets

So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here:

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/paired_reads.png", auto_pdf = TRUE)
``` 

This results in two sequences generated for each sequenced fragment: One reading in from the 3' end (R1) and the other reading in from the 5' end (R2).

FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files:

```{bash eval=FALSE}
zcat K1_R1.fastq.gz | head -n 4 | less -S
```

The pipe symbol ( `|` ) is used to pass the output of one command as input to the next command. So, this command (1) shows the unzipped contents of the FASTQ file, (2) displays only the first 4 lines, and (3) displays them without wrapping lines (with `–S`, for easy viewing).

The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for '+') and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters.  __To return to the command prompt, press__ `q`. 

Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample).

## Checking quality control
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/mangify_glass.png", auto_pdf = TRUE)
``` 

We can generate and visualise various sequence data metrics for quality control purposes using `FastQC`. We will run `FastQC` on the R1 and R2 reads separately as it is good to visualise them in two different reports. This is beacause R1 and R2 reads have different quality patterns, generally due to the poorer quality of R2. 

Run `FastQC` on the files:

```{bash eval=FALSE}
#R1 fastqc
#Make an output directory
mkdir R1_fastq
#Run fastqc on all the R1.fastq.gz files 
#* matches any pattern
#*R1.fastq.gz matches any file that ends R1.fastq.gz in the current directory
#-t 3 indicates to use 3 threads, chosen as there are three R1 files
fastqc -t 3 -o R1_fastqc *R1.fastq.gz

#R2 fastqc
#Make output directory
mkdir R2_fastq
#Run fastqc
fastqc -t 3 -o R2_fastqc *R2.fastq.gz
```

Once the `FastQC` commands are run we can run `MultiQC` to create interactive html reports for the outputs.

```{bash eval=FALSE}
#R1 multiqc fastqc report
#Create output directory
mkdir R1_fastqc/multiqc
#Create multiqc output
multiqc -o R1_fastqc/multiqc R1_fastqc

#R2 multiqc fastqc report
#Create output directory
mkdir R2_fastqc/multiqc
#Create multiqc report
multiqc -o R2_fastqc/multiqc R2_fastqc
```

Once completed, view the `MultiQC` reports (NB: The `&` runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open):

```{bash eval=FALSE}
firefox R1_fastqc/multiqc/multiqc_report.html \
R2_fastqc/multiqc/multiqc_report.html &
```

The `FastQC` report (via `MultiQC`) contains a number of metrics. The "Sequence Quality Histograms" shows the sequence quality across the length of the reads, you can hover over each line to show which sample it belongs to. Note how quality decreases as the length of the read increases. While this is normal with Illumina sequencing, we will improve the situation a bit in the next chapter.

Once you have finished inspecting, minimise the Firefox window.

<!--chapter:end:05-Raw_data.Rmd-->

# Quality control
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

Now that we've obtained the raw data and had a look at it, we should now clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. 

There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets.

- Remove low quality bases from the end of the reads: These are more likely to be incorrect, so are best trimmed off.
- Remove adapters: Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. 
- Host removal: If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome. Here, we do not need to do this, as the dataset  has next to zero human genome sequences.

## Removing adapters and low quality bases
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/hedge_trimming.png", auto_pdf = TRUE)
``` 

First go back to your home directory and create a new directory where we will clean the sequences up:

```{bash eval=FALSE}
cd ..
mkdir 2-Trimmed
cd 2-Trimmed
```

You are now in your newly created directory. Here we will run `Trim Galore!` which removes low quality bases and adapters.

```{bash eval=FALSE}
trim_galore --paired --quality 20 --stringency 4 \
   ../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz
```

This is a longer command so we've split it across multiple lines (a `\` at the end of a line allows you to press return without running the command, meaning you can continue to add to that command. When this happens, the `$` changes to a `>`. 

__Note__ if you do use the `\` character, the next key you press must be return. If you use `\` in the middle of a line without pressing return afterwards, it will break the command!

This command will remove any low quality regions from the end of both reads in each read pair (quality score < 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too.

__Task__: Rerun this command for the other two samples (K2 and W1).

## Rename the files
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/rename.png", auto_pdf = TRUE)
``` 

Once that is complete if you run:

```{bash eval=FALSE}
ls
```

you will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. For example K1_R1_val_R1.fq.gz could be shortened to K1_R1.fq.gz. So, we'll rename all of the files with the mv command:

```{bash eval=FALSE}
mv K1_R1_val_1.fq.gz K1_R1.fq.gz
mv K1_R2_val_2.fq.gz K1_R2.fq.gz
mv K2_R1_val_1.fq.gz K2_R1.fq.gz
mv K2_R2_val_2.fq.gz K2_R2.fq.gz
mv W1_R1_val_1.fq.gz W1_R1.fq.gz
mv W1_R2_val_2.fq.gz W1_R2.fq.gz
```

__Tip__: If you want to edit and reuse previous commands, press the up arrow key.

__Task__: Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt).

## Inspect the trimmed data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/magnify_glass_good.png", auto_pdf = TRUE)
``` 

To see what difference the trimming made, run `FastQC` and `MultiQC` again on the trimmed output files and view it. 

```{bash eval=FALSE}
#R1 fastqc and multiqc
mkdir R1_fastqc
fastqc -t 3 -o R1_fastqc *R1.fq.gz
mkdir R1_fastqc/multiqc
multiqc -o R1_fastqc/multiqc R1_fastqc
```

__Task__: Run `FastQC` and `MultiQC` for the R2 files and then view the R1 and R2 `MultiQC` reports with firefox. How does the quality compare to the untrimmed data?

<!--chapter:end:06-Trimming_data.Rmd-->

# Host removal
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/bowtie2.png", auto_pdf = TRUE)
``` 

It is good practice to remove any host sequences from your data before further analysis. A good method for this is to align/map your reads to a reference of your host genome and remove the mapped sequences (i.e sequences we believe to belong to the host).

If there is no host genome available before you start your sample collections and sequencing it may be a good idea to attempt to sequence and assemble the host genome. We would recommend long read technologies for single genome assembly projects.

This chapter contains a small example on how to carry out host removal. It uses only a section of a human (host of our samples) reference genome assembly. IN real life you should use the entire reference.

First step is to copy over the reference fasta file we will use.

```{bash eval=FALSE}
cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/GRCh38_slice.fasta .
```

## Index reference
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/index.png", auto_pdf = TRUE)
``` 

We will will use the `Bowtie2` aligner for mapping/aligning. Prior to alignment/mapping we need to index our reference.

```{bash eval=FALSE}
bowtie2-build GRCh38_slice.fasta GRCh38_slice.fasta
```

If you use `ls` you will now see a bunch of files starting with `GRCh38_slice.fasta` and ending with various suffixes that contain `bt`. These are the index files which allow us to use the reference with `Bowtie2`.

## Alignment
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/alignment.png", auto_pdf = TRUE)
``` 

With the indexed reference we will align the K1 reads to the reference. This creates a bam file that contains alignment and read information (`mapped.bam`).

```{bash eval=FALSE}
bowtie2 -x test.fasta -1 R1.fastq.gz -2 R2.fastq.gz \
-p12 2> out.log | samtools view -bSh > mapped.bam
```

## Unmapped read extraction
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/exctractor.png", auto_pdf = TRUE)
``` 

Next step is to extract the reads that did not map to the host reference from the `mapped.bam` file with a `samtools` command (unmapped reads).

```{bash eval=FALSE}
samtools fastq -f 4 -1 K1_R1.u.fastq -2 K1_R2.u.fastq mapped.bam
```
 
The above step may make unmatched paired files. This occurs when a read from R2 is removed but the matching read in R1 is not removed, or vice versa. This will cause issues for further analysis. 

## Re-pair
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/repair.png", auto_pdf = TRUE)
``` 

The below `BBTools` command will re-pair the reads by removing reads with a missing pair. The command ensures the order of the reads are identical in the 2 paired files.

```{bash eval=FALSE}
repair.sh in1=K1_R1.u.fastq in2=K1_R2.u.fastq \
out1=K1_R1.final.fastq out2=K1_R2.final.fastq \
outs=singletons.fastq
```

The file `singletons.fastq` contains the left over singletons (a sequence missing a pair) and can normally be ignored.

As our data has pretty much no human data we will skip this step for the other samples and use the trimmed data for the downstream analysis.

## Host removal considerations

In a real analysis project you would use a whole genome reference for your host. However, that would have taken too long for this practical. The most current Human reference (when this was written) is GRCh38. We used a random 10kb section to align our reads to.

For more resources on the Human reference please see: https://www.ncbi.nlm.nih.gov/genome/guide/human/

The assembly we used was: https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz

<!--chapter:end:07-Host_removal.Rmd-->

# Taxonomic profiling
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/classification.png", auto_pdf = TRUE)
``` 

There are a number of methods for determining the species composition of a metagenomic data-set, but for the purposes of this practical we will use `Kraken2` & `Bracken` (Bayesian Reestimation of Abundance with KrakEN).  `Kraken2` classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. `Bracken` uses the taxonomic labels assigned by `Kraken2` to compute the abundance of species in a set of DNA sequences.

First, we'll make a new directory for it and move into it, after returning home:

```{bash eval=FALSE}
cd ..
mkdir 3-Taxonomy
cd 3-Taxonomy
```

## Kraken2
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/kraken.png", auto_pdf = TRUE)
``` 

Prior to running `Kraken2` we need to set a variable so `Kraken2` knows where to look for the databases it will use.

```{bash eval=FALSE}
export KRAKEN2_DB_PATH=/pub39/tea/matthew/NEOF/Shotgun_metagenomics/kraken2_db
```

__Note__: You can look at the contents of the above directory to see it currently contains the MiniKraken database. This database contains only a subset of the bacteria, archaea, and viral `Kraken2` libraries. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full `Kraken2` database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. See the appendix for a link to the `Kraken2` manual.

### Kraken2: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/enter_key.png", auto_pdf = TRUE)
``` 

Now, run `Kraken2` on sample K1 by running the following command.

__Note__: As there is little human sequence in our data we will not use host removed data.

```{bash eval=FALSE}
kraken2 --paired --db minikraken2_v1_8GB \
--output K1.kraken --report K1.kreport2 \
~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz
```  

While this is running, let's look at what those command line options do:

- `~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz` : the trimmed read pairs for K1, which we will use as input.
- `--paired` : Indicates that we are providing paired reads to `Kraken2`. Internally, `Kraken2` will concatenate the R1 and R2 reads into one sequence with an N between them.
- `--db` : Specify the `Kraken2` database to be used for taxonomic classification. Previous to the command we set the `KRAKEN_DB_PATH` so in this case the command will look for the directory called `minikraken2_v1_8GB` within `KRAKEN_DB_PATH`. Alternatively the full path of the required database could be provided.
- `--threads` : How many CPUs the process will use.
- `--output` : This is the output file. 

### Kraken2: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output.png", auto_pdf = TRUE)
``` 

There are two major output formats from Kraken2:

- `--output`, `.kraken`: Each sequence (or sequence pair, in the case of paired reads) classified by `Kraken2` results in a single line of output. `Kraken2`'s output lines contain five tab-delimited fields; from left to right, they are:
   1. "C"/"U": a one letter code indicating that the sequence was either classified or unclassified.
   2. The sequence ID, obtained from the FASTA/FASTQ header.
   3. The taxonomy ID `Kraken2` used to label the sequence; this is 0 if the sequence is unclassified.
   4. The length of the sequence in bp. In the case of paired read data, this will be a string containing the lengths of the two sequences in bp, separated by a pipe character, e.g. "98|94".
   5. A space-delimited list indicating the LCA mapping of each k-mer in the sequence(s). For example, "562:13 561:4 A:31 0:1 562:3" would indicate that:
      - the first 13 k-mers mapped to taxonomy ID #562
      - the next 4 k-mers mapped to taxonomy ID #561
      - the next 31 k-mers contained an ambiguous nucleotide
      - the next k-mer was not in the database
      - the last 3 k-mers mapped to taxonomy ID #562
      - __Note__: that paired read data will contain a "|:|" token in this list to indicate the end of one read and the beginning of another.

- `--report`, `.kreport2`: The report output format. This is required for bracken. It is tab-delimited with one line per taxon. The fields of the output, from left-to-right, are as follows:
   1. Percentage of fragments covered by the clade rooted at this taxon.
   2. Number of fragments covered by the clade rooted at this taxon.
   3. Number of fragments assigned directly to this taxon.
   4. A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., "G2" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank.
   5. NCBI taxonomic ID number
   6. Indented scientific name

The output to screen will show how many sequences are classified. This will be lower than normal as we are using a mini `Kraken2` database.

In a real analysis you may use the option `--confidence` which represents  the __"Confidence score threshold"__. The default is `0.0`, which is the lowest, with the maximum value being `1`. A good place to start may be `0.1`. Too many classifications are removed if you attempt it with this dataset, due to the mini `Kraken2` database used. More info on the confidence scoring can be found at: https://github.com/DerrickWood/kraken2/wiki/Manual#confidence-scoring

__Task__: Once the Kraken2 command has finished running, run it on the other two samples. 

__Hint__: You will need to change all instances of K1 to K2 or W1 in the above command

## Krona plot
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/krona.png", auto_pdf = TRUE)
``` 

`Krona` is an interactive metagenome species abundance visualisation tool. 

We can use the `Kraken2` report files to create our Krona plots. With the below command we can import our `Kraken2` taxonomy (within the report file) into a `Krona` html.

```{bash eval=FALSE}
ktImportTaxonomy.sh -o kraken2.krona.html *.kreport2
```

`-o` is our output html file, and the final argument `*.kreport2` represents all of our `.kreport2` files in the current directory. The `*` is a wild-card, meaning any characters any number of times. Therefore `*.kreport2` identifies the files `K1.kreport2` `K2.kreport2` and `W1.kreport2`.

You will get a warning that not all taxonomy IDs were found. We will ignore this but in your own future installations this should be addressed with `Krona`'s `updateTaxonomy.sh` command.

Now we can view our interactive chart in a web browser.

```{bash eval=FALSE}
firefox kraken2.krona.html &
```

__Question__: Can you tell which sample looks the most different in terms of bacterial species that are present and absent?

## Bracken
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/bracken.png", auto_pdf = TRUE)
```

`Bracken` (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by `Kraken2` to compute estimated abundances of species in a metagenomic sample. 

### Bracken: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/enter_key.png", auto_pdf = TRUE)
``` 

Just like with `Krona` we can use the `Kraken2` report files to run bracken.

```{bash eval=FALSE}
bracken -d $KRAKEN_DB_PATH/minikraken2_v1_8GB \
-i K1.kreport -o K1.bracken -r 100 -l S -t 5
```

Let's look at what those command line options do:

- `-d` : Specifies the `Kraken2` database that was used for taxonomic classification. In this case bracken requires the variable `$KRAKEN_DB_PATH` so the option is provided the full path to the kraken database. For clarity try the command `ls $KRAKEN_DB_PATH/minikraken2_v1_8GB`. 
- `-i` : The `Kraken2` report file, this will be used as the input.
- `-o` : The output `Bracken` file. Information about its contents is below.
- `-r 100`: This is the ideal length of the reads that were used in the `Kraken2` classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as we used a paired library of 100 bp reads.
- `-l S`: This specifies the taxonomic level/rank of the `Bracken` output. In this case `S` is equal to species with the other options being `D`, `P`, `C`, `O`,`F` and `G`.
- `-t 5`: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10).

### Bracken: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output.png", auto_pdf = TRUE)
``` 

The output file of `Bracken` contains the following columns:

1. __Name__: Name of taxonomy at the specified tax level.
2. __Taxonomy ID__: NCBI taxonomy id
3. __Level ID__: Letter signifying the taxonomic level of the classification
4. __Kraken assigned read__: Number of reads assigned the taxonomy by `Kraken2`.
5. __Added reads with abundance reestimation__: Number of reads added to the taxonomy by Bracken abundance reestimation. 
6. __Total reads after abundance reestimation__: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis.
7. __Fraction of total reads__: Relative abundance of the taxonomy.

Use `less` or `vim` to look at the bracken output.

__Task__: Repeat the above commands for K2 and W1

### Bracken: merging output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/merge.png", auto_pdf = TRUE)
``` 

To make full use of `Bracken` output, it is best to merge the output into one table. However before we do this we’ll copy the `Bracken` output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples.

```{bash eval=FALSE}
cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/bracken/* .
```

Now to merge all the `Bracken` files.

```{bash eval=FALSE}
combine_bracken_outputs.py --files *.bracken -o all.bracken
```

This output file contains the first three columns:

- __name__ = Organism group name. This will be based on the TAX_LVL chosen in the `Bracken` command and will only show the one level.
- __taxonomy_id__ = Taxonomy id number.
- __taxonomy_lvl__ = A single string indicating the taxonomy level of the group. ('D','P','C','O','F','G','S').

Following these columns are the following two columns for each sample.

- `${SampleName}.bracken_num`: The number of reads after abundance reestimation 
- `${SampleName}.bracken_frac`: Relative abundance of the group in the sample

### Bracken: extracting output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/exctractor.png", auto_pdf = TRUE)
``` 

We want a file with only the first column (organism name) and the `bracken_num` columns for each sample. To carry this out we  first create a sequence of numbers that will match the `bracken_num` column numbers. These start at column 4 and are every even numbered column after this. In this case `seq` will create every `2` numbers from the numbers `4` to `50` with commas (`,`) as separators (`-s`). 

__Note__: The number 50 is chosen as 3 (first three info columns) + 24*2 (24 samples with 2 columns each) = 50.

```{bash eval=FALSE}
#Try out the seq command to see its output
seq -s , 4 2 50
#Create variable
bracken_num_columns=$(seq -s , 4 2 50)
echo $bracken_num_columns
```

Now to use the variable to extract the `bracken_num` columns plus the first column (species names).

```{bash eval=FALSE}
cat all.bracken | cut -f 1,$bracken_num_columns > all_num.bracken
```

## LEfSe biomarker detection
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/lefse.png", auto_pdf = TRUE)
```

We will use `LEfSe` (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. `LEfSe` couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions.

In essence it allows for the detection of biomarkers when comparing sample groups. In the `LEfSe` terminology the sample groups are called the class.

### LEfSe: add metadata
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/header.png", auto_pdf = TRUE)
```

We need to add metadata to our `Bracken` file to be ready for `LEfSe.` First we will copy the file so we have a backup in case we do anything wrong.

```{bash eval=FALSE}
cp all_num.bracken all_num.lefse.bracken
```

Using your favourite text editor (e.g. `nano`, `vim`, etc.) add the following line to the top of your `all_num.lefse.bracken file`. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator.

__diet	K	K	K	K	K	K	K	K	K	K	K	K	W	W	W	W	W	W	W	W	W	W	W	W__

__Note__: The above is __diet__ followed by 12 __K__ and 12 __W__.

The singular line should match the order of your samples within the file. This is the metadata line that `LEfSe` will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples.

### LEfSe: conda
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/conda.png", auto_pdf = TRUE)
```

`LEfSe` requires python2 whilst we have been using packages that required python3. We therefore need to use a different `conda` environment. We are currently using the `conda` env called `shotgun_meta` as represented by `(shotgun_meta)`. This was activated with the command `. useshotgun`.

Open a new terminal (right click on the main screen background, choose `Applications` -> `Shell` -> `bash`) and run the following in the new terminal to activate the `lefse` `conda` environment.

```{bash eval=FALSE}
#Setup environment
. uselefse
#Change directory
cd ~/3-Taxonomy
```

### LEfSe: format
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/lefse_format.png", auto_pdf = TRUE)
```

We need to further format and preprocess our file with a `LEfSe` script.

```{bash eval=FALSE}
lefse-format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000
```

- `all_num.lefse.bracken` : Input `Bracken` file.
- `all_num.lefse` : Output file formatted for the run_lefse command, which we will soon run.
- `-c 1` : Specifies the row with the class info. This is used to determine which samples will be compared against which samples. In this case it is the first row with the Ks and Ws.
- `-u 2` : Specifies the row with the sample names. This is the second row in this case.
- `-o 1000000` : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. `LEfSe` developers recommend 1000000 (1 million) when very low values a present. We generally always use 1 million for consistency.

### LEfSe: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/enter_key.png", auto_pdf = TRUE)
``` 

Now to run `LEfSe.` All we need to do is run the command with the formatted input and provide an output file name.

```{bash eval=FALSE}
run_lefse.py all_num.lefse all_num.lefse.out
```

The output file is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a "-" .

- __Biomarker name__
- __Log of highest class average__: I.e. get the class with the greater amounts of the biomarker, average the counts and then get the log of this value.
- __Class with the greater amounts of biomarker__
- __LDA effect size__: A statistical figure for `LEfSe`..
- p-value: Biomarkers must have a p-value of <0.05 to be considered significant.

The __LDA effect__ size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes.

### LEfSe: visualisation
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/bar_chart_horizontal.png", auto_pdf = TRUE)
``` 

Next we can visualise the output.

```{bash eval=FALSE}
lefse-plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png
```

- `--dpi 200` : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. We’ve chosen 200 as it is good quality and we will not be publishing these results.
- `--format png` : Format of output file. png is a commonly used file format for images.
- `all_num.lefse.out` : `LEfSe` output to visualise.
- `biomarkers.png` : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance.

Look at the figure with the program `okular`:

```{bash eval=FALSE}
okular biomarkers.png
```

__Questions__:

- Which species causes the biggest effect in the W class and in the K class? 
-Which class has more biomarkers associated with it?

__Note__: In this instance green bars represent biomarkers in higher abundance in the W samples whilst the red bars represent biomarkers in higher abundance in the K samples.

## Kraken2 and Bracken databases
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/database.png", auto_pdf = TRUE)
``` 

In your own future analysis you will need to create your own `Kraken2` and `Bracken` databases. Please sse the following links on information for this:

- `Kraken2`
   - Standard `Kraken2` databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database
   - Custom `Kraken2` databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases
- `Bracken`
   - https://ccb.jhu.edu/software/bracken/index.shtml?t=manual#step1
   - This requires a `Kraken2` database to be built first.


<!--chapter:end:08-Taxonomic_profiling.Rmd-->

# Functional profiling
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/profile.png", auto_pdf = TRUE)
``` 

It is also possible to investigate functional differences between metagenome (and metatranscriptome) samples by directly interrogating the read data. We will now look at how this can be done with a package called `HUMAnN2` (The HMP Unified Metabolic Analysis Network 2), a pipeline designed to accurately profile the presence/absence and abundance of microbial pathways in metagenomic sequencing data.

## HUMAnN2 
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/humann2.png", auto_pdf = TRUE)
``` 

First, we will carry out an example run of the software and briefly explore the output files. HUMAnN2 can take a long time to run so we will use a small amount of example data.

### HUMAnN2: conda, directories, and files
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/conda.png", auto_pdf = TRUE)
```

We need a new conda environment again. Open a new terminal (right click on the main screen background, choose `Applications` -> `Shell` -> `bash`) and run the below:

```{bash eval=FALSE}
. usehumann2
```

Make a new directory and move into it.

```{bash eval=FALSE}
mkdir ~/4-FunctionalProfiling
cd ~/4-FunctionalProfiling
```

Copy over some test data we will the analysis on. This is a demonstration FASTQ file that we will use. It will be small enough to run HUMAnN2 in a reasonable time.

```{bash eval=FALSE}
cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/humann2/demo.fq.gz .
```

### HUMAnN2: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/enter_key.png", auto_pdf = TRUE)
``` 

Now we will perform the run with HUMAnN2 so we can inspect the output files. 

```{bash eval=FALSE}
humann2 \
--input demo.fq.gz \
--output demo.humann2 \ 
--threads 10
```

Here, we are telling the software to use `demo.fq.gz` as input and to create a new output directory called `demo.humann2` where the results will be generated. 

As the software runs, you might notice that as part of the process, `HUMAnN2` runs `MetaPhlAn2.` The purpose of this is to identify what species are present in the sample, so `HUMAnN2` can tailor generate an appropriate database of genes (from those species) to map against. It will carry out this alignment against the gene database, then a protein database, and finally compute which gene families are present to determine which functional pathways are present and how abundant they are.

### HUMAnN2: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output.png", auto_pdf = TRUE)
``` 

Once the run has completed, change into the newly created output directory and list the files that are there.

```{bash eval=FALSE}
cd demo.humann2
ls
```

You will see that there are three files and one directory. The directory (`demo_humann2_temp`) contains intermediate temporary files and can be disregarded here.

The three output files are:

- `demo_genefamilies.tsv`: A table file showing the number of reads mapping to each UniRef90 gene family, Values are normalised by the length of each gene family (i.e. RPK, or Reads per Kilobase). Additionally, the values are stratified so that they show the overall community abundance but also a breakdown of abundance per species detected. This allows researchers to delve into species specific functions, rather than only looking at the metagenomic functions as a whole,
- `demo_pathabundance.tsv`: As above, a table file showing the normalised abundance of MetaCyc pathways. These abundances are calculated based on the UniRef90 gene family mapping data and are also stratified by species.
- `demo_pathcoverage.tsv`: Similar to above, except instead of abundances of pathways, this table shows the coverage, or completeness, of pathways. For example, a pathway may contain 5 components (or genes/proteins)
   - Pathway1 :         A → B → C → D → E		100% complete
   - A species identified in the sample may only have four of the components, though, 	meaning that the pathway is only 80% complete (represented as 0.8)
    - Pathway1 :         A → B → C → ~~D~~ → E		80% complete

The basic format of these three output files is the same, so let's take a look at the pathway abundance table.

```{bash eval=FALSE}
less demo_pathabundance.tsv
```

You will see that there are two columns:

1. The first column shows the pathways.
   - __UNMAPPED__ indicates reads that could not be aligned.
   - __UNINTEGRATED__ indicates reads that aligned to targets not implicated in any pathways.
2. The second column shows the abundance.

Press `q` to exit and let's look at one specific pathway, COA-PWY-1 (a coenzyme A biosynthesis II pathway).

```{bash eval=FALSE}
grep COA-PWY-1 demo_pathabundance.tsv
```

This shows two entries with two different values, I.e.

- COA-PWY-1: coenzyme A biosynthesis II (mammalian)	6.3694267516 
- COA-PWY-1: coenzyme A biosynthesis II (mammalian)|g__Bacteroides.s__Bacteroides_vulgatus	4.7961630695

This is an example of the species stratification mentioned above. The first line shows the abundance of this pathway across the entire sample, but the second line shows the abundance contributed by __Bacteroides_vulgatis__.

Have a look at the other two output files; note the similar layout.

Finally, return to the parent directory i.e. `4-FunctionalProfiling`

```{bash eval=FALSE}
cd ..
```

__Note__: The directory `demo_humann2_temp` can be very large and so should be deleted in real projects once you are certain they are not needed. However, these files can be useful for debugging. 

## Statistical comparison between samples
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/k_w_venn.png", auto_pdf = TRUE)
``` 

Looking at the functional profile of one sample in isolation is usually not very informative. First, there is nothing to compare it to and second, there are no biological replicates. We will therefore use all the Korean and Western diet samples.

It would take many hours to analyse all of the data using `HUMAnN2` and is outside of the scope of this course. For this reason, samples were analysed prior to the workshop to generate the output files we covered above. 

For the purposes of this comparison, we will look at the pathway abundances only. First copy over the results data directory and have a look in it.

```{bash eval=FALSE}
cp –r /pub39/tea/matthew/NEOF/Shotgun_metagenomics/DietPathAbundance .
ls DietPathAbundance
```

You will see there are 12 files prefixed with K and 12 prefixed with W, for the Korean diet and Western diet samples, respectively. Take a look in one of the files; you might notice that there is no species stratification. The reason for this is that for this test, all pathway abundances have been collapsed to a community level, ignoring differences in species. When you analyse your own data-sets, you can leave this stratification in to identify significant differences between functional profiles at a species level.

__Note__: You can create an unstratified pathabundance file with a command like: `cat pathabundance.tsv | grep -v "|" > pathabundance.unstratified.tsv`. The `grep -v "|"` removes lines that contain `|` which the stratified lines should only contain.

### HUMAnN2 comparison: combining data
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/merge.png", auto_pdf = TRUE)
``` 

First, we need to combine these 24 tables into one large results table. `HUMAnN2` provides a tool to do this:

```{bash eval=FALSE}
humann2_join_tables --input DietPathAbundance/ --output diet.tsv
```

This command will look for all tables in the `DietPathAbundance` directory and generate a large, 24 column table called `diet.tsv`. You can take a look to see that this has worked correctly.

```{bash eval=FALSE}
less -S diet.tsv
```

### HUMAnN2 comparison: renormalising data
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/normalisation.png", auto_pdf = TRUE)
``` 

The next step is to renormalise the data. Currently, all of the abundance values are only normalised within each sample, i.e. accounting for the size of the pathways/length of genes. However, they are not normalised between samples, and this is very important. For example, if we had sequenced two samples, A and B, and we obtained 5 million reads for sample A and 20 million reads for sample B, without normalisation, it might look that there was up to 4x as much functional activity in sample B! 

To correct for this, we normalise the abundance values based on the number of reads in each sample. We will normalise by counts per million, or cpm. (We could also normalise to relative abundance where all abundances for each sample add up to 1).


__Equation:__
$$
cpm = p * (\frac{1000000}{t})
$$

Where:

- __cpm__ = counts per million.
- __p__ = count of pathway in sample.
- __t__ = total count of sample.

Renormalisation command:

```{bash eval=FALSE}
humann2_renorm_table \
--units cpm \
--input diet.tsv \
--special n \
--output diet.cpm.tsv
```

This command generates the normalised data in the new table `diet.cpm.tsv`. The `--special n` option tells the script to remove all unmapped and unassigned values (__UNMAPPED__ & __UNINTEGRATED__) from the table.

### HUMAnN2 comparison: PCA plot
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/Scatterplot.png", auto_pdf = TRUE)
``` 

Now that we have our data normalised, we can visualise the dataset to see how the two groups look, i.e. do samples in the same diet group appear to correlate well with each other and are they distinguishable from those from the other diet group? To do this, we will draw a PCA plot. We will need to provide the script with some metadata. This has been prepared and can be copied over as follows

```{bash eval=FALSE}
cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/diet.metadata.tsv .
```

This is a table file where the first column is the sample name with subsequent columns representing categories of metadata. Here, the column of interest is the second one, 'Diet', and samples are labelled as either 'Western' or 'Korean'. We can now run the PCA plot generation script and look at the result.

```{bash eval=FALSE}
PCA_plot.r diet.cpm.tsv diet.metadata.tsv Diet diet.pca.pdf
```

This command takes our data table as the first argument, our metadata table as the second argument, the metadata category of interest as the third argument and the output file as the final argument.

__Note__: `PCA_plot.r` is a custom R script that will be provided after the course.

Now we can view the plot.

```{bash eval=FALSE}
okular diet.pca.pdf &
```

You should see 3 plots, each one plotting 2 of the first 3 principal components against each other. The axis of each plot also shows how much of the overall variance that that particular component accounts for. Blue and red dots show the Western and Korean diet samples, respectively, and samples that cluster closely together are more similar to each other.

From this, we can see that there is some separation between the two dietary groups, but that they are not completely separated, i.e. there is some overlap. This can happen frequently with datasets such as this, that are likely to be much less controlled than laboratory samples and therefore show more between-group variance.

```{r, fig.align = 'center',out.width= '60%', echo=FALSE }
knitr::include_graphics(path = "figures/PCA1.png", auto_pdf = TRUE)
```

__Questions__: 

- Is there anything unexpected about any of the samples, from these plots? 
- If so, what might be a sensible course of action before proceeding with statistical analysis?

### HUMAnN2 comparison: LEfSe
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/lefse_flip.png", auto_pdf = TRUE)
``` 

For the final part of this section, we will see if there are any statistically significant differences between the two sample groups. There are several ways in which this can be achieved but we will carry out `LEfSe` again.

__Task__: Go back to your `LEfSe` terminal (or create a new one and use `. uselefse`). Then change directory to `4-FunctionalProfiling`)

First make a new copy of the file we will work on:

```{bash eval=FALSE}
cp diet.cpm.tsv diet.cpm.lefse.tsv
```

Change the file `diet.cpm.lefse.tsv` with you favourite text editor to make it `LEfSe` compatible. 2 edits are required:

1. Change the `# Pathway` in the first line to `name`.
2. Add a new line at the top for the diet metadata (same as we did for the `Bracken` data).

As you have carried out `LEfSe` already we will have one code box showing all the commands.

```{bash eval=FALSE}
#LEfSe format
lefse-format_input.py diet.cpm.lefse.tsv diet.cpm.lefse -c 1 -u 2 -o 1000000
#Run LEfSe
run_lefse.py diet.cpm.lefse diet.cpm.lefse.out
#Produce LEfSe plot
lefse-plot_res.py --dpi 200 --format png diet.cpm.lefse.out biomarkers.png
#View plot
okular biomarkers.png
```

Look at the output and see what pathways count as biomarkers for the 2 groups.

That completes the non assembly approach to shotgun metagenomic analysis. The next chapters will cover an assembly approach.

<!--chapter:end:09-Functional_profiling.Rmd-->

# Metagenome assembly
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/jigsaw.png", auto_pdf = TRUE)
``` 

So far we have directly analysed the read data itself which is perfectly fine for taxonomic profiling and for certain methods of functional profiling. However, Illumina reads are generally short and therefore can not provide us with much data on larger constructs that are in the metagenomic samples, e.g. genes. While it is possible to predict from which gene a sequence read might originate, the short nature of the query can sometimes lead to ambiguous results. 

Additionally, depending on the application it can become computationally intensive to analyse large numbers of reads. Here, we are only using samples with 1 million reads. Some metagenome samples consist of 50-100 million+ read pairs. If such a sample belonged to a set of 100 samples, that would be up to 10 billion read pairs, or 2 trillion bases of sequence data, with many of these being redundant.

For this reason, it is sometimes advantageous to assemble the reads into contigs, using a meta-genome assembler. This has the dual effect of:

- Reducing the overall size of the data for analysis. If a metagenome was sequenced at 50x depth, then by assembling it you could theoretically reduce the amount of sequence to analyse by 50-fold.
- Increase the size of the fragments you will analyse. This is the main advantage of an assembly, as the ~100 bp reads can be pieced together to form 100,000 kb+ contigs. These contigs will contain complete genes, operons and regulatory elements: Reconstructed genome sections.

Here, we will carry out a couple of assemblies on our dataset.

We will use the `shotgun_meta` conda environment so use a terminal where this is activated or open a new one and run `. useshotgun_meta`.

## A primer on short read assembly
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/dna_laptop.png", auto_pdf = TRUE)
``` 

Illumina reads are too short and numerous to use traditional overlap-layout-consensus assemblers as such an approach would be far too computationally intensive. Instead, we use De Bruijn graph based assemblers. Briefly, these operate as follows:

1. All reads are broken down into k-length overlapping fragments (k-mers). e.g. if we choose a k-mer size of 5 bp, the following two sequences (blue) would be broken down into the k-mers below them (red):

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/reads_aligned.png", auto_pdf = TRUE)
``` 

2. All k-mers are linked to other k-mers which match with a k-1 length overlap (i.e. that overlap by all but one base:

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/graph_simple.png", auto_pdf = TRUE)
``` 

3. Paths are routed through the graph and longer contigs are generated:

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/graph_complex.png", auto_pdf = TRUE)
``` 

The example here is a vast oversimplification of the complexity of a De Bruijn graph (i.e. there are no branches!). Routing through the graph is never as simple as this as some k-mers will lead to multiple k-mers, which can result in the break point of a contig. This is especially true for complex metagenomic data.

Generally speaking, the shorter the k-mer, the more branches there will be, the trickier the graph is to resolve, so the resulting contigs are smaller. Assemblers usually perform better with longer k-mer lengths but even then there might not be enough depth of sequencing to generate all k-mers that form overlaps, therefore leading to break points. Finding the right k-mer size usually involves testing several. 

Fortunately, the assembler we will use, MEGAHIT, allows us to build an assembly using multiple k-mer lengths iteratively. The other great advantage about MEGAHIT is that it is quick and efficient.  We will use MEGAHIT on our data soon, but first there is an additional processing step for our sequences...

## Stitching read pairs
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/stitch.png", auto_pdf = TRUE)
``` 

As mentioned, longer k-mers generally perform better, but as our maximum read length is 100 bp, we are limited to a maximum k-mer length of 99 bp. However, we can get even longer k-mers if we stitch our read pairs together.

__Note__: This method will not work if your reads have no overlap. If you are not sure if your reads have overlap ask the team who sequenced them.

Remember that a read pair consists of two sequences read from each end of a fragment of DNA (or RNA). If the two sequences meet in the middle of the fragment and then overlap, there will be a region of homology which we can use to merge the two reads in the pair together (See next image).

First, we obtain our forward and reverse reads, derived from different ends of the same fragment. Second, we look for sufficient overlap between the 3' ends of our sequences. Third, if there is sufficient overlap, we combine, or stitch, the two reads together to form one long sequence.

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/merging_paired_reads.png", auto_pdf = TRUE)
``` 

Once we have longer stitched reads, we can increase the k-mer length for our assembly. 

There are a number of pieces of software that can be used to stitch reads (e.g. Pear,  Pandaseq) but today we will use one called FLASH:

Make a new output directory for the stitched reads and run FLASH:

```{bash eval=FALSE}
#Change directory to home
cd ~
#Make and move into new directory
mkdir 5-Stitched
cd 5-Stitched
#Run flash
flash  -o K1 -z -t 12 -d . \
../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz
```

Here, we are telling flash to use an output file name prefix of K1, that the input is zipped, that the output directory is here (.) and to use the two read files for Sample K1. Once FLASH has finished running, it will display on screen how well the stitching process went, in this case a low amount of reads were combined. Have a look what files have been generated.

```{bash eval=FALSE}
ls
```

We have three new fastq.gz files. One containing the stitched reads (`K1.extendedFrags.fastq.gz`) and two containing the reads from pairs that could not be combined (`K1.notCombined_1.fastq.gz` and `K1.notCombined_2.fastq.gz)`.

We can also see what the new read lengths are:

```{bash eval=FALSE}
less K1.histogram
```

Scroll down with the down key and you will see that we are looking at a histogram showing the proportion of reads at different lengths. We can now start assembling our stitched reads for this sample.

## Assembly
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/assembly_line.png", auto_pdf = TRUE)
``` 

Create a new directory to store our assembly in and run the metagenome assembler MEGAHIT using our newly stitched read data.

```{bash eval=FALSE}
cd ..
mkdir 6-Assembly
cd 6-Assembly
megahit \
-r ../5-Stitched/K1.extendedFrags.fastq.gz \
-1 ../5-Stitched/K1.notCombined_1.fastq.gz \
-2 ../5-Stitched/K1.notCombined_2.fastq.gz \
-o K1 \
-t 12 \
--k-list 29,49,69,89,109,129,149,169,189
```

Here, you have instructed MEGAHIT to use both the stitched and unstitched reads, to output the assembly in a subdirectory called K1 and to use 12 CPUs. 

The last option `--k-list` instructs MEGAHIT to first generate an assembly using a k-mer size of 29 bp and when that is complete, integrate the results into an assembly using a k-mer size of 49 bp, and so on up to a final iteration using a k-mer size of 189 bp. This large range of k-mer lengths should give us a good assembly, given the data. However, it may take a while to run so this might be a good time to either read on. 

If you need a command prompt (your current one is gone because MEGAHIT is running), right click on the main screen, choose `Applications` -> `Shell` -> `bash`.

Once the assembly is completed, we can look at the output FASTA file containing the contigs:

```{bash eval=FALSE}
less K1/final.contigs.fa
```

## QUAST

We can also generate some metrics based on the assembly.

Due to python version conflict we need to use another conda environment.

Open a new terminal (right click on the main screen, choose `Applications` -> `Shell` -> `bash`) and run the below.

```{bash eval=FALSE}
#use script to activate conda env
. usegenoassess
```

We will use QUAST for genome contiguity assessment but first we will change directory to `6-Assembly` and create a directory for the QUAST output.

```{bash eval=FALSE}
#Change directory
cd ~/6-Assembly
#Create QUAST output directory
#The option -p will create and parent directories tha
mkdir -p quast/K1
```

The `-p` option of `mkdir` will cause the command to create any parent directories that are required to create the noted directory. I.e. `quast` will be created so `K1` can be created.

Now to run QUAST. 

```{bash eval=FALSE}
#Generate contiguity statistics
quast -o quast/K1 K1/final.contigs.fa
#View QUAST html report
firefox quast/K1/report.html
```

The report tells us quite a bit about the assembly quality. Two definitions that you may not be aware of are N50 and N50 length (or, somewhat confusingly, L50 and N50, respectively!). If we were to order our contigs from largest to smallest, and total up the sizes from biggest downwards, the contig we reach where our total is 50% of the size of the whole assembly is the N50 contig (the smaller the number the better). The N50 length is the length of this contig; a weighted median contig length.

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/n50_n90.png", auto_pdf = TRUE)
``` 

__Quastions__
- How do the contig metrics compare to the original reads? 

- Now we have an assembly, albeit not a brilliant one due to us only having used 1 million reads, we can start to explore it.

There is also a metaQUAST specifically for metagenome assemblies but it requires reference assemblies be provided.

<!--chapter:end:10-Metagenome_assembly.Rmd-->

# Genome binning
```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/yarn_binning.png", auto_pdf = TRUE)
``` 

A metagenome assembly consists of contigs from many different genomes. At this stage we don't know which contigs are from which species. We could try to taxonomically classify each contig but there are 2 problems with this approach:

1. Some contigs may be misclassified which can lead to multiple contigs from the same genome/organism being classified as various taxa.
2. Databases are incomplete and so some contigs will not be classified at all (microbial dark matter).

To alleviate these issues genomic binning can be carried out. This will cluster contigs into bins based on:

- __Coverage__: Contigs with similar coverage are more likely to be from the same genome.
- __Composition__: Contigs with similar GC content are more likely to belong to the same genome.

Genomic binning has been used to discover many new genomes. Additionally, it makes downstream analyses quicker as the downstream steps will be carried out on the sets of bins rather than on one large metagenome assembly.

Binning produces "bins" of contigs of various quality (e.g. draft, complete). These bins are also know as MAGs (Metagenome-assembled genomes). In other words a MAG is a single assembled genome that was assembled with other genomes in a metagenome assembly but later separated from the other assemblies. The term MAG  has been adopted by the GSC (Genomics Standards Consortium).

It is recommended to ensure you do not have a poor quality metagenome assembly. Binning requires contigs of good length and good coverage. Extremely low coverage and very short contigs will be excluded from binning.

## MetaBAT2
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/bat.png", auto_pdf = TRUE)
``` 

We will use `MetaBAT2` for our genome binning. It is a relatively new binning tool with three major upsides that makes it very popular:

1. It has very reliable default parameters meaning virtually no parameter optimisation is required.
2. It performs very well amongst genome binners.
3. It is computationally efficient compared to other binners (requires less RAM, cores etc.)

### MetaBAT2: Conda environment & directory
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/conda.png", auto_pdf = TRUE)
``` 

We will use the `shotgun_meta` conda environment to start for this chapter.

Additionally, make a new directory and move into it.

```{bash eval=FALSE}
#Make directory
mkdir -p ~/7-Binning/K1
#Move into it
cd ~/7-Binning/K1
```

### MetaBAT2: depth calculation
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/submarine.png", auto_pdf = TRUE)
``` 

To carry out effective genome binning `MetaBAT2` uses coverage information of the contigs. To calculate depth we need to align the reads to the metagenome assembly.

For the alignment we will use `bwa`. We need to index our assembly file prior to alignment.

```{bash eval=FALSE}
bwa index ~/6-Assembly/K1/final.contigs.fa
```

Next we will align our trimmed paired reads we used to create the stitched reads. We will carry this out with the `bwa mem` command. `bwa mem` is a good aligner for short reads. If you are using long reads (PacBio or Nanopore) `minimap2` will be more appropriate.

```{bash eval=FALSE}
bwa mem ~/6-Assembly/K1/final.contigs.fa \
~/2-Trimmed/K1_R1.final.fastq ~/2-Trimmed/K1_R2.final.fastq > \
K1.sam
```

After alignment we need to get the file ready for the contig depth summarisation step. This requires converting the `sam` file to a `bam` (binary form of a `sam` file) file and then sorting the `bam` file.

```{bash eval=FALSE}
# Convert sam to bam file
samtools view -bu K1.sam > K1.bam
# Created sorted bam file
samtools sort K1.bam > K1.sort.bam
```

Now we can summarise the contig depths from the sorted `bam` files with `MetaBAT2`'s `jgi_summarize_bam_contig_depths` command.

```{bash eval=FALSE}
jgi_summarize_bam_contig_depths --outputDepth K1.depth.txt K1.sort.bam
```

You can have a look at the depth file and you will notice there are many contigs with low coverage (<10) and of short length (<1500).

```{bash eval=FALSE}
less K1.depth.txt
```

To get a better look we will open the file in `R` and look at a summary of the file's table.

Activate `R`:

```{bash eval=FALSE}
R
```

Now in `R` we will read in the file and get a `summary()` of it.

```{r eval=FALSE}
#Read in the table as an object called df (short for data frame)
#We want the first row to be the column names (header=TRUE)
#We do not want R to check the column names and "fix" them (check.names=FALSE)
df <- read.table("K1.depth.txt", header=TRUE, check.names=FALSE)
#Create a summary of the data
summary(df)
```

We can see the numbers of the `contigLen` and `totalAvgDepth` are very low. However, this is most likely due to a bunch of short low coverage contigs which will be ignored by `MetaBAT2`. Therefore we will remove rows with information on contigs shorter than 1500 and rerun the summary. `MetaBAT2`'s documentation dictates the minimum contig length should be >=1500 with its default being 2500.

```{r eval=FALSE}
#Set the new object "df_min1500len" as all rows
#where the value in the column "contigLen" of "df"
#Is greater than or equal to 1500
df_min1500len <- df[df$contigLen >= 1500,]
#Summary of our new data frame
summary(df_min1500len)
```

That is looking better. The minimum average coverage for `MetaBAT2` is 1 and our minimum is higher than that. Now you can quit R and continue.

```{r eval=FALSE}
#quit R
q()
#On the prompt to save your workspace press "n" and then enter.
```

__Note__: One of the reasons for our short contigs is that we only used a subset of our sequencing dataset for this tutorial due to time concerns.

### MetaBAT2: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/enter_key.png", auto_pdf = TRUE)
``` 

With our assembly and its depth information we can run `MetaBAT2` for binning.

```{bash eval=FALSE}
#make a diretcory for the bins
mkdir bins
#Run MetaBAT2
metabat2 \
--inFile ~/6-Assembly/K1/final.contigs.fa \
--outFile bins/K1 \
--abdFile K1.depth.txt \
--minContig 1500
```

List the contents of the output directory and you'll see there are 2 files with the prefix of `K1`. These are the 2 bins that will hopefully contain 1 MAG (Metagenome-Assembled Genome).

```{bash eval=FALSE}
ls bins
```

## CheckM
```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/CheckM.png", auto_pdf = TRUE)
``` 

`CheckM` is a useful tool to assess the quality of assembled bacterial and archaeal genomes. This can be used on assemblies produced from single cell, single isolate, or metagenome data. Additionally, it can be used to identify bins that are likely candidates for merging. This occurs when one genome has been separated into different bins.

An important part of CheckM is the ubiquitous and single-copy genes it utilises. It has sets of these genes for different phylogenetic lineages. With these it can determine:

- What lineage a bin/MAG belongs to. 
   - Does it contain genes only found in _Escherichia_?
- How complete the bin/MAG is. A set of lineage specific genes should all be found in a genome belonging to the lineage (ubiquitous).
   - What percentage of these lineage specific genes are present in the MAG? 
   - \>95% is very good
   - \>80% is good
   - \>70% is ok
   - <70% is poor to poorer.
- How contaminated is the bin/MAG?
   - Only one copy of each gene should be present (single-copy).
   - Are there any markers for other lineages present?

### CheckM: Conda
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/conda.png", auto_pdf = TRUE)
``` 

Due to program version conflicts we will use the `checkm` conda environment for this section.

Open a new terminal and activate the `checkm` environment.

```{bash eval=FALSE}
. usecheckm
```

Ensure you are in the correct directory.

```{bash eval=FALSE}
cd ~/7-Binning/K1/
```

### CheckM: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/enter_key.png", auto_pdf = TRUE)
``` 

`CheckM` has many different commands. We will use one of the common workflows it provides called `lineage_wf`. This carries out five of its commands in a workflow (i.e. the next step uses output from the previous step).

1. `tree` - Places bins in the reference genome tree. This reference tree comes with `CheckM`.
2. `tree_qa` - Assess the phylogenetic markers found in each bin.
3. `lineage_set` - Infers lineage-specific marker sets for each bin. 
4. `analyze` - Identifies marker genes in bins.
5. `qa` - Assesses the bins for contamination and completeness.

Run the `CheckM` command (this will take a while):

```{bash eval=FALSE}
checkm lineage_wf \
--tab_table -f MAGS_checkm.tsv \
-x fa \
bins/ checkm_output
```

The options we used are:

- `--tab_table` : Prints results to a tab separated table.
- `-f` : File name to print result to (if not specified results will go to stdout).
- `-x` : Suffix/extension of bin files. Other files are ignored in the specified bin directory.
   - `fa` is used as our `MetaBAT2` produce fasta files end in `.fa`.
- `bins` : The second last argument is the `bin_dir`, the directory containg all the bins in fasta format.
- `checkm_output` : The last argument is the directory to store the output to. This directory should not exist prior to running.

### CheckM: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output.png", auto_pdf = TRUE)
``` 

As we have only used a subset of data the results are not very good so let us use premade results. These premade results were produced using the entire K1 dataset. First you will need to copy it over.

```{bash eval=FALSE}
cd ~/7-Binning
cp -r /pub39/tea/matther/NEOF/Shotgun_metagenomics/binning/K1_fullset/ .
cd K1_fullset
```

Now we can look at the results table that is in your current directory.

```{bash eval=FALSE}
less MAGS_checkm.tsv
```

__Tip__: The definition of the statistics can be found at: https://github.com/Ecogenomics/CheckM/wiki/Reported-Statistics

One quick way to calculate the overall quality of a bin is with the following equation:

$$
q = comp - (5 * cont)
$$
Where:
__q__ = Overall quality
__comp__ = Completeness
__cont__ = Contamination

A score of 70-80% would be the aim. Therefore let us calculate this for the bins with R.

First the tab delimited file has some issues for `R` so we'll make a comma separated version.

```{bash eval=FALSE}
cat MAGS_checkm.tsv | tr "\t" "," > MAGS_checkm.csv
```

Activate `R`:

```{bash eval=FALSE}
R
```

Now to do the calculation in R for each bin:

```{r eval=FALSE}
#Read in csv file as data frame
df <- read.csv("MAGS_checkm.csv", header=TRUE, check.names=FALSE)
#Add a column for overall quality
df$quality <- df$Completeness - (5 * df$Contamination)
#View the bin name, lineage, completeness, contamination and the quality columns
df[,c(1,2,12,13,15)]
```

__Questions__:

- What lineages were assigned to the bins?
- How many genomes are present in the bins?
- What is the completeness and contamination of the bins and how is the overall quality?
- With this information how successful was the binning? Was the binning able to create MAGs (i.e. one genome per bin)?
- Which bins look particularly good?

Once you are happy you can quit `R`.

```{r eval=FALSE}
q()
```

<!--chapter:end:11-Genome_binning.Rmd-->

# MetaWRAP
```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/yarn_binning.png", auto_pdf = TRUE)
``` 

`MetaWRAP` is a very useful program with many tools for metagenome analysis. We will only use it to improve our bins and carry out some downstream analysis. However, it can be used form the start of shotgun metagenomic analysis. I recommend look at the manual for the future (link in appendix).

## Bin refinement

`MetaWRAP` has a function called `bin_refinement`. This function can utilise bins from different programs and parameters to produce a consolidated, improved bin set. Additionally, `MetaWRAP` has a `binning` function which will produce three sets of bins from a reads. This is carried out with `MaxBin2`, `metaBAT2`, and `CONCOCT`.

We are not carrying this method as running multiple binners would take too many resources and too much time for this workshop. However, if you have a powerful HPC and the time this would be a good method.

## Bin reassembly

The first `MetaWRAP` command we will run is `reassemble_bins`. This aims to improve the previous assembled bins. It carried this out by mapping paired reads to the metagenome assembly. Then the program separates the mapped reads based on the bin the contig it aligned to belongs to. Reassembly is carried out with SPAdes using the `--carefull` option and contigs shorter than 1kp are removed. The program then has 3 assemblies for each bin:

- The original assembled bin.
- A strict reassembled bin: This is assembled with reads that perfectly aligned to the bin contigs.
- A permissive reassembled bin: This is assembled with reads that mapped with <3 mismatches.

The best assembly is chosen based on the highest score of Completion-(5*Contamination). This assembly is then added ot the final bin set.

For speed (even this will take a while) we will run the bin reassembly on our small dataset of K1.

```{bash eval=FALSE}
metawrap reassemble_bins \
-o K1 -b ~/7-Binning/K1/bins/ \
-1 ~/2-Trimmed/K1_R1.final.fastq \
-2 ~/2-Trimmed/K1_R2.final.fastq \
-t 8
```

Initially only 2 bins were created with `MetaBAT2` (this can be checked with `less ~/7-Binning/K1/MAGS_checkm.tsv`).

<!--chapter:end:12-MetaWrap.Rmd-->

# Appendix
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

## Manuals

Conda: https://conda.io/projects/conda/en/latest/user-guide/getting-started.html

FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/

MultiQC: https://multiqc.info/

Trim Galore: https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/

Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml

samtools: http://www.htslib.org/

BBTools: https://jgi.doe.gov/data-and-tools/bbtools/

Kraken2: https://github.com/DerrickWood/kraken2/wiki/Manual

Krona: https://github.com/marbl/Krona/wiki/KronaTools

Bracken: https://ccb.jhu.edu/software/bracken/index.shtml?t=manual

LEfSe: https://huttenhower.sph.harvard.edu/lefse/

HUMAnN2: https://github.com/biobakery/biobakery/wiki/humann2

MetaPhlAn2: https://huttenhower.sph.harvard.edu/metaphlan2/

Biobakery: https://github.com/biobakery/biobakery

MegaHit: https://github.com/voutcn/megahit

BWA: https://github.com/lh3/bwa

minimap2: https://github.com/lh3/minimap2

MetaBAT2: https://bitbucket.org/berkeleylab/metabat/src/master/

CheckM: https://github.com/Ecogenomics/CheckM/wiki

MetaWRAP: https://github.com/bxlab/metaWRAP

## COG Cateories

- CELLULAR PROCESSES AND SIGNALING
   - [D] Cell cycle control, cell division, chromosome partitioning
   - [M] Cell wall/membrane/envelope biogenesis
   - [N] Cell motility
   - [O] Post-translational modification, protein turnover, and chaperones
   - [T] Signal transduction mechanisms
   - [U] Intracellular trafficking, secretion, and vesicular transport
   - [V] Defense mechanisms
   - [W] Extracellular structures
   - [Y] Nuclear structure
   - [Z] Cytoskeleton
- INFORMATION STORAGE AND PROCESSING
   - [A] RNA processing and modification
   - [B] Chromatin structure and dynamics
   - [J] Translation, ribosomal structure and biogenesis
   - [K] Transcription
   - [L] Replication, recombination and repair
- METABOLISM
   - [C] Energy production and conversion
   - [E] Amino acid transport and metabolism
   - [F] Nucleotide transport and metabolism
   - [G] Carbohydrate transport and metabolism
   - [H] Coenzyme transport and metabolism
   - [I] Lipid transport and metabolism
   - [P] Inorganic ion transport and metabolism
   - [Q] Secondary metabolites biosynthesis, transport, and catabolism
- POORLY CHARACTERIZED
   - [R] General function prediction only
   - [S] Function unknown
   
## Obtaining Read Data

The following commands can be used to obtain the sequence data used in this practical, directly from the EBI metagenomics site. It is worth noting that these are the full set of data, not like the miniaturised version you have used in the tutorial.

```{bash eval=FALSE}
wget -O K1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_1.fastq.gz
wget -O K1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_2.fastq.gz
wget -O K2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_1.fastq.gz
wget -O K2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_2.fastq.gz
wget -O W1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_1.fastq.gz
wget -O W1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_2.fastq.gz
```

<!--chapter:end:16-Appendix.Rmd-->

