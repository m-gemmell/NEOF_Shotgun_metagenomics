--- 
title: "Shotgun Metagenomics"
author: "Sam Haldenby and Matthew Gemmell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/NEOF.png", auto_pdf = TRUE)
``` 

# Introduction
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/squid.png", auto_pdf = TRUE)
``` 

This practical session aims to introduce you to the analysis of Shotgun metagenomic data. The topics covered are:

- Overview
- Raw data
- Trimming data
- Taxonomic profiling
- Functional profiling
- Metagenome assembly
- Gene prediction
- Functional annotation
- Co-assembly
- Comparative analysis

<!--chapter:end:01-Shotgun_metagenomics.Rmd-->

# Overview
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/overview.png", auto_pdf = TRUE)
``` 

## What is metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/what.png", auto_pdf = TRUE)
``` 

__Meta /ˈmɛtə/ : prefix meaning “higher” or “beyond”__

Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome.

## Why metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/why.png", auto_pdf = TRUE)
``` 

Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. 

While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species neighbours, i.e. what else is in its natural environment?  Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture.

## Metagenomics vs Metagenetics
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/16s_vs_shotgun.png", auto_pdf = TRUE)
``` 

Broadly speaking, there are two families of metagenomic analysis: 

- __Amplicon-based__: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions.
- __Shotgun__: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referenced as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session.

## Tutorial overview
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/tutorial.png", auto_pdf = TRUE)
``` 

### Basics

This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. 

The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. 

Virtually the entire tutorial will be carried out on the command line, which you will hopefully now be more comfortable with.

### Structure

We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. So, there will be no instructor telling you what to type and click: Instead, everything you require to carry out the practical is written in the document. Take your time; it's important to spend some time understanding why you are running the commands, rather than simply typing them out. 

If at any point you are having trouble or have a question, let one of us know and we'll provide 1-to-1 assistance.

### Content
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/sections.png", auto_pdf = TRUE)
``` 

This practical is broken up into the following broad sections.

1. __Raw data__: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples.
2. __Trimming data__: This entails preprocessing our data to ensure that it is of good quality.
3. __Taxonomic profiling__: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples.
4. __Functional profiling__: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples.
5. __Metagenome assembly__: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will 'stitch' the reads together to ensure we get the best assembly possible.
6. __Gene prediction__:  We will take our metagenome assembly, search for genes...
7. __Functional annotation__: ...and then functionally annotate them with information from various databases. We will then visualise some of the output.
8. __Co-assembly__: Instead of just looking at the functional composition of one metagenome sample, we will discuss methods of combining all samples to carry out a co-assembly and then obtain normalised gene coverage statistics for use in comparative analyses between samples.
9. __Comparative analysis__: Using our data from the previous step, we will look at a couple of different ways of comparing the functional profiles of our samples
All the analyses here are just examples of how you could interrogate a metagenomic dataset: There are, of course, many other ways to tackle such a set. 

__Don't worry if you don't manage to finish the whole practical!__  

The more commonly used analyses have been put at the front of the practical (Section 1-4) with the less standard ones being placed towards the end. We will provide you with all of the intermediate and results files on request.

<!--chapter:end:02-Overview.Rmd-->

# Before we start
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/start.png", auto_pdf = TRUE)
``` 

During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command:

```{bash eval=FALSE}
source /pub21/sam/metagenomicsWorkshop2019.sh
```

Also, there’s a chance you’re currently not in your home directory, so let’s make sure you are with the following command:

```{bash eval=FALSE}
cd ~
```

<!--chapter:end:03-Start.Rmd-->

# Raw data
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/usb_stick.png", auto_pdf = TRUE)
``` 

The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets.

Have a browse of some of the projects by selecting one of the biomes on this page.

We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558).

## Obtaining the data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/link.png", auto_pdf = TRUE)
``` 

First, we need to create a directory to put the data in and then change directory to it.

```{bash eval=FALSE}
mkdir 1-Raw
cd 1-Raw
```

Now we can generate a symbolic link (i.e. shortcut) to the raw sequence data files, which will appear in the current directory:

```{bash eval=FALSE}
linkFiles
```

All that this command did is run a script which creates a ‘symbolic link’ (like a shortcut in Windows) to the read files that we will be using (The appendix of this document contains the commands used to download these files directly from the EBI metagenomics site) Now, check they are there with:

```{bash eval=FALSE}
ls
```

There should be six files in the directory, two for each sample in the dataset. e.g. `K1_R1.fastq.gz`

The file ID has three components:

- K1 is the sample ID. 
- R1 is for the first reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). 
- fastq.gz tells us that this is a zipped FASTQ file.

The sample labelling indicates the type treatment samples. The three samples are:

- __K1__: Fecal sample of individual of Korean diets
- __K2__: Fecal sample of individual of Korean diets
- __W1__: Fecal sample of individual of Western diets

So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here:

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/paired_reads.png", auto_pdf = TRUE)
``` 

This results in two sequences generated for each sequenced fragment: One reading in from the 3' end (R1) and the other reading in from the 5' end (R2).

FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files:

```{bash eval=FALSE}
zcat K1_R1.fastq.gz | head -n 4 | less -S
```

The pipe symbol ( `|` ) is used to pass the output of one command as input to the next command. So, this command (1) shows the unzipped contents of the FASTQ file, (2) displays only the first 4 lines, and (3) displays them without wrapping lines (with `–S`, for easy viewing).

The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for '+') and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters.  __To return to the command prompt, press__ `q`. 

Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample). At a later point in the tutorial, you will be asked to link to results derived from the full dataset for further processing.

## Checking quality control
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/mangify_glass.png", auto_pdf = TRUE)
``` 

We can generate and visualise various sequence data metrics for quality control purposes using the FastQC. Run FastQC on one of the files:

```{bash eval=FALSE}
fastqc K1_R1.fastq.gz
```

Once completed, view the output (NB: The & runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open):

```{bash eval=FALSE}
firefox K1_R1_fastqc.html &
```

The FastQC report contains a number of metrics. The first graph shows the sequence quality across the length of the reads: Note how it decreases as the length of the read increases. While this is normal with Illumina sequencing, we can improve the situation a bit...

Briefly inspect the FastQC report for yourself – There are examples of typical (and atypical!) FastQC data in the appendix of this document

Once you have finished looking, minimise the Firefox window.

<!--chapter:end:04-Raw_data.Rmd-->

# Quality control
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

Now that we've obtained the raw data and had a look at it, we should now clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets.

- Remove low quality bases from the end of the reads. These are more likely to be incorrect, so are best trimmed off.
- Remove adapters. Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. 
- Remove host sequences. If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome. Here, we do not need to do this, as the dataset contains barely any human genome sequences.

## Removing adapters and low quality bases
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/hedge_trimming.png", auto_pdf = TRUE)
``` 

First go back to your home directory and create a new directory where we will clean the sequences up:

```{bash eval=FALSE}
cd ..
```

This will move you one directory up, i.e. back to your home directory. Alternatively, you could use `cd ~` which will take you to your home directory. This is a good idea if you ever get lost!

```{bash eval=FALSE}
mkdir 2-Trimmed
cd 2-Trimmed
```

You are now in your newly created directory. Here we will run Trim Galore! which carries out both of these steps.

```{bash eval=FALSE}
trim_galore --paired --quality 20 --stringency 4 \
   ../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz
```

This is a longer command so we've split it across multiple commands (a `\` at the end of a line allows you to press return without running the command, meaning you can continue to add to that command. When this happens, the `$` changes to a `>`. __Note__ that if you do use the `\` character, the next character immediately after it must be return. If you use `\` in the middle of a line without pressing return afterwards, it will break the command!

This command will remove any low quality regions from the end of both reads in each read pair (quality score < 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too. We use the two read files for sample K1 as input, from the previous directory we were in.

Run this command two more times, but for the other two samples (K2 and W1)

## Rename the files
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/rename.png", auto_pdf = TRUE)
``` 

Once that is complete if you run:

```{bash eval=FALSE}
ls
```

you will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. e.g. K1_R1_val_R1.fq.gz could be shortened to K1_R1.fq.gz. So, we'll rename all of the files with the mv command:

```{bash eval=FALSE}
mv K1_R1_val_1.fq.gz K1_R1.fq.gz
mv K1_R2_val_2.fq.gz K1_R2.fq.gz
mv K2_R1_val_1.fq.gz K2_R1.fq.gz
mv K2_R2_val_2.fq.gz K2_R2.fq.gz
mv W1_R1_val_1.fq.gz W1_R1.fq.gz
mv W1_R2_val_2.fq.gz W1_R2.fq.gz
```

__Tip__: If you want to edit and reuse previous commands, press the up arrow key.

Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt)

## Inspect the trimmed data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/magnify_glass_good.png", auto_pdf = TRUE)
``` 

To see what difference the trimming made, run FastQC again on the trimmed output file K1_R1.fq.gz and view it. 

```{bash eval=FALSE}
fastqc K1_R1.fq.gz
firefox K1_R1_fastqc.html &
```

Inspect the FastQC report for yourself. How does it compare to the untrimmed data?

Now that we have trimmed data, we can start the analyses!



<!--chapter:end:05-Trimming_data.Rmd-->

# Taxonomic profiling
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

There are a number of methods for determining the species composition of a metagenomic data-set, but for the purposes of this practical we will use Kraken & Bracken (Bayesian Reestimation of Abundance with KrakEN).  Kraken classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. We will be using Kraken 1 as Kraken 2 is still in beta. Bracken uses the taxonomic labels assigned by Kraken to compute the abundance of species in a set of DNA sequences.

First, we'll make a new directory for it and move into it, after returning home:

```{bash eval=FALSE}
cd ..
mkdir 3-Taxonomy
cd 3-Taxonomy
```

## Running Kraken

Prior to running kraken we can set a variable so kraken knows where to look for the databases it will use.

export KRAKEN_DB_PATH=/pub39/tea/matthew/teaching/Kraken_db/MiniKraken/

__Note__: You can look at the contents of the above directory to see it currently contains the MiniKraken 4GB database. This database contains 2.7% of the kmers from the full Kraken database. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full Kraken database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. As of October 2017, this includes ~25,000 genomes, requiring 33GB of disk space.

Now, run Kraken on sample K1 by running the following command.

```{bash eval=FALSE}
kraken --paired --db minikraken_4GB --threads 4 --output K1.kraken.out \
../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz
```  

While this is running, let's look at what those command line options do:

- `../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz` : the trimmed read pairs for K1, which we use as input.
- `--paired` : Indicate that we are providing paired reads to Kraken. Internally, Kraken will concatenate the R1 and R2 reads into one sequence with an N between them.
- `--db` : Specify the kraken database to be used for taxonomic classification. Previous to the command we set the KRAKEN_DB_PATH so in this case the command will look for the directory called ‘minikraken_4GB’ within the KRAKEN_DB_PATH. Alternatively the full path of the required database could be provided.
- `--threads` : How many CPUs the process will use.
- `--output` : This is the output file. 

The output file of kraken contains the following columns:

- __"C"/"U"__: one letter code indicating that the sequence was either classified or unclassified.
- __The sequence ID__: Obtained from the FASTA/FASTQ header.
- __Taxonomy ID__: The ID Kraken used to label the sequence; this is 0 if the sequence is unclassified.
- __Length__: Nucleotide length of the sequence.
- A space-delimited list indicating the LCA (lowest common ancestor) mapping of each k-mer in the sequence. For example, "562:13 561:4 A:31 0:1 562:3" would indicate that:
   - the first 13 k-mers mapped to taxonomy ID #562
   - the next 4 k-mers mapped to taxonomy ID #561
   - the next 31 k-mers contained an ambiguous nucleotide
   - the next k-mer was not in the database
   - the last 3 k-mers mapped to taxonomy ID #562

The output to screen will show how many sequences are classified. This will be lower than normal as we are using the mini Kraken database.

In a real analysis you may use the command “kraken-filter” to ensure your Kraken classifications are high quality. Too many classifications are removed if you attempt it with this dataset, due to the minikraken database used, so we will skip this step.

Once the Kraken commands have finished running, run it on the other two samples. HINT: You will need to change all instances of K1 to K2 or W1 in the above command

## Visualising Kraken output

### Krona plot

Krona is an interactive metagenome species abundance visualisation tool. We need to get our data in the correct format before we view it, however, so we first use a Kraken-supplied script to convert the files:

```{bash eval=FALSE}
kraken-translate --db minikraken_4GB \
K1.kraken.out > K1.kraken.label
```

Next we convert these to a file format that krona will be able to read.

```{bash eval=FALSE}
cat K1.kraken.label | cut -f 2 | sort | uniq -c | tr ‘;’ ‘\t’ > K1.krona.txt
```

The above command uses the following steps:
- `cat K1.kraken.label`: cat the file to print out its contents.
- The pipe symbols `|` are used to pipe the output of one command to the next.
- `cut -f 2`: The cut command is used to cut out the 2nd column (field).
- `sort`: The output of the 2nd column is sorted, this is required for the next step.
- `uniq -c`: Unique count, find every unique string/entry and count how many of each there are.
- `tr ‘;’ ‘\t’`: Converts ‘;’ for ‘\t’ (tab) characters
- `> K1.krona.txt`: Redirect the output to the file called `K1.krona.txt`

If you would like to see exactly what the file is doing you can run the command bit by bit i.e

```{bash eval=FALSE}
cat K1.kraken.label
cat K1.kraken.label | cut -f 2
cat K1.kraken.label | cut -f 2 | sort
#etc.
```

Repeat the above commands for K2 and W1

Once we have all three files converted, we can combine them with a Krona packaged script to generate our charts.

```{bash eval=FALSE}
ImportText.pl -o all.krona.html ??.krona.txt
```

`-o` is our output html file, and the final argument `??.krona.txt` represents all of our .krona files in this directory: The `?` is a wild-card, meaning any character, so this identifies the files `K1.krona.txt` `K2.krona.txt` and `W1.krona.txt`.

Now we can view our chart in a web browser.

```{bash eval=FALSE}
firefox all.krona.html &
```

Which sample is the most different in terms of species that are present and absent?

## Running Bracken

Bracken (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by Kraken to compute the abundance of species in a metagenomic sample. 

Prior to Bracken we need to create a Kraken report file

```{bash eval=FALSE}
kraken-report --db minikraken_4GB K1.kraken.out > K1.kreport
```

Now it’s time for the bracken command

```{bash eval=FALSE}
bracken -d $KRAKEN_DB_PATH/minikraken_4GB \
-i K1.kreport -o K1.bracken -r 100 -l S -t 5
```

Let's look at what those command line options do:

- `-d` : Specify the Kraken database that was used for taxonomic classification. In this case bracken requires the variable `$KRAKEN_DB_PATH` so the option is provided the full path to the kraken database. For clarity try the command `ls ${KRAKEN_DB_PATH}/minikraken_4GG`
- `-i` : The Kraken report file, this will be used as the input.
- `-o` : The output bracken file. Information about its contents is below.
- `-r 100`: This is the ideal length of the reads that were used in the kraken classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as it is a paired library of 100 bp reads.
- `-l S`: This specifies the taxonomic level/rank of the Bracken output. In this case S is equal to species with the other options being 'D','P','C','O','F' and 'G'.
- `-t 5`: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10).

The output file of Bracken contains the following columns:

1. Name: Name of taxonomy at the specified tax level.
2. Taxonomy ID: NCBI taxonomy id
3. Level ID: Letter signifying the taxonomic level of the classification
4. Kraken assigned read: Number of reads assigned assigned the taxonomy by Kraken.
5. Added reads with abundance reestimation: Number of reads added to the taxonomy by Bracken abundance reestimation. 
6. Total reads after abundance reestimation: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis.
7. Fraction of total reads: Relative abundance of the taxonomy.

Use `less` or `vim` to look at the bracken output.

Repeat the above commands for K2 and W1

To make full use of Bracken output, it is best to merge the output into one table. However before we do this we’ll copy the Bracken output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples.

```{bash eval=FALSE}
cp $DB/bracken/* .
```

Now to merge all the Bracken files.

```{bash eval=FALSE}
combine_bracken_outputs.py --files *.bracken -o all.bracken
```

This output file contains the first three columns:
- __name__ = Organism group name. This will be based on the TAX_LVL chosen in the bracken command and will only show the one level
- __taxonomy_id__ = Taxonomy id number
- __taxonomy_lvl__ = A single string indicating the taxonomy level of the group. ('D','P','C','O','F','G','S').

Following these columns are two columns for each sample.

- `${SampleName}.bracken_num`: The number of reads after abundance 
- `${SampleName}.bracken_frac`: Relative abundance of group in sample

We want a file with only the first column and the bracken_num column for each sample. We can therefore use the following commands.

Create a sequence of numbers that will match the bracken_num column numbers. Use “seq --help” to see how the seq command works.

```{bash eval=FALSE}
bracken_num_columns=$(seq -s , 4 2 50)
echo $bracken_num_columns
```

Now to use the variable to extract the bracken_num columns plus the first column (species names).

```{bash eval=FALSE}
cat all.bracken | cut -f 1,$bracken_num_columns > all_num.bracken
```

## LEfSe biomarker detection

We will use LEfSe (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. LEfSe couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions.

In essence it allows for the detection of biomarkers when comparing sample groups. In the LEfSe terminology the sample groups are called the class.

We need to format our bracken file to be ready for LEfSe. First we will copy the file so we have a backup in case we do anything wrong.

```{bash eval=FALSE}
cp all_num.bracken all_num.lefse.bracken
```

This next part must be done for further commands to work

Using your favourite text editor add the following line to the top of your all_num.lefse.bracken file. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator.

diet	K	K	K	K	K	K	K	K	K	K	K	K	W	W	W	W	W	W	W	W	W	W	W	W

i.e. the above is diet followed by 12 Ks and 12 Ws

The singular line should match the order of your samples within the file. This is the metadata line that LEfSe will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples.

The easiest way to install LEfSe is through conda (for info to install conda and LEfSe through conda please see the following instructions https://conda.io/en/latest/, https://anaconda.org/bioconda/lefse). We will activate a preconstructed LEfSe conda environment with the following script.

```{bash eval=FALSE}
lefse_env.sh
```

Although we have formatted the input file already we need to further format and preprocess it with a LEfSe script.

```{bash eval=FALSE}
lefse-format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000
```

- `all_num.lefse.bracken` : Input Bracken file.
- `all_num.lefse` : Output file formatted for the run_lefse command, which we will soon run
- `-c 1` : Specifies the row with the class info. This is used to determine which sample will be compared against which samples. In this case it is the 1st row with the Ks and Ws.
- `-u 2` : Specifies the row with the sample names. This is the second row in this case.
- `-o 1000000` : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. LEfSe developers recommend 1000000 (1 million) when very low values a present. We generally always use 1 million for consistency.

Now to run LEfSe. All we need to do is run the command with the formatted input and provide an output file name.

```{bash eval=FALSE}
run_lefse.py all_num.lefse all_num.lefse.out
```

The output file is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a "-".

- Biomarker name
- Log of highest class average, i.e. Get the class with the greater amounts of the biomarker, average the counts and then get the log of this value.
- Class with the greater amounts of biomarker
- LDA effect size: A statistical figure for LEfSe. Only features with an LDA >2 are detected as biomarkers by default. The higher the LDA effect size is the more of an effect the biomarker causes.
- p-value: biomarkers must have a p-value of <0.05 to be considered significant.
T
he LDA effect size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes.

Next we can visualise the output.

```{bash eval=FALSE}
lefse-plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png
```

- `--dpi 200` : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. We’ve chosen 200 as it is good quality and we will not be publishing these results.
- `--format png` : Format of output file. png is a commonly used file format for images.
- `all_num.lefse.out` : LEfSe output to visualise.
- `biomarkers.png` : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance.

Look at the figure with the program okular:

```{bash eval=FALSE}
okular biomarkers.png
```

Which species causes the biggest effect in the W class and in the K class? Which class has more biomarkers associated with it?

<!--chapter:end:06-Taxonomic_profiling.Rmd-->

