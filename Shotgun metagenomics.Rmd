--- 
title: "Shotgun Metagenomics"
author: "Sam Haldenby and Matthew Gemmell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/NEOF.png", auto_pdf = TRUE)
``` 

# Introduction
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/squid.png", auto_pdf = TRUE)
``` 

This practical session aims to introduce you to the analysis of Shotgun metagenomic data. The topics covered are:

- Overview
- Raw data
- Trimming data
- Taxonomic profiling
- Functional profiling
- Metagenome assembly
- Gene prediction
- Functional annotation
- Co-assembly
- Comparative analysis

<!--chapter:end:01-Shotgun_metagenomics.Rmd-->

# Overview
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/overview.png", auto_pdf = TRUE)
``` 

## What is metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/what.png", auto_pdf = TRUE)
``` 

__Meta /ˈmɛtə/ : prefix meaning “higher” or “beyond”__

Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome.

## Why metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/why.png", auto_pdf = TRUE)
``` 

Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. 

While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species neighbours, i.e. what else is in its natural environment?  Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture.

## Metagenomics vs Metagenetics
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/16s_vs_shotgun.png", auto_pdf = TRUE)
``` 

Broadly speaking, there are two families of metagenomic analysis: 

- __Amplicon-based__: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions.
- __Shotgun__: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referenced as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session.

## Tutorial overview
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/tutorial.png", auto_pdf = TRUE)
``` 

### Basics

This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. 

The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. 

Virtually the entire tutorial will be carried out on the command line, which you will hopefully now be more comfortable with.

### Structure

We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. So, there will be no instructor telling you what to type and click: Instead, everything you require to carry out the practical is written in the document. Take your time; it's important to spend some time understanding why you are running the commands, rather than simply typing them out. 

If at any point you are having trouble or have a question, let one of us know and we'll provide 1-to-1 assistance.

### Content
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/sections.png", auto_pdf = TRUE)
``` 

This practical is broken up into the following broad sections.

1. __Raw data__: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples.
2. __Trimming data__: This entails preprocessing our data to ensure that it is of good quality.
3. __Taxonomic profiling__: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples.
4. __Functional profiling__: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples.
5. __Metagenome assembly__: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will 'stitch' the reads together to ensure we get the best assembly possible.
6. __Gene prediction__:  We will take our metagenome assembly, search for genes...
7. __Functional annotation__: ...and then functionally annotate them with information from various databases. We will then visualise some of the output.
8. __Co-assembly__: Instead of just looking at the functional composition of one metagenome sample, we will discuss methods of combining all samples to carry out a co-assembly and then obtain normalised gene coverage statistics for use in comparative analyses between samples.
9. __Comparative analysis__: Using our data from the previous step, we will look at a couple of different ways of comparing the functional profiles of our samples
All the analyses here are just examples of how you could interrogate a metagenomic dataset: There are, of course, many other ways to tackle such a set. 

__Don't worry if you don't manage to finish the whole practical!__  

The more commonly used analyses have been put at the front of the practical (Section 1-4) with the less standard ones being placed towards the end. We will provide you with all of the intermediate and results files on request.

<!--chapter:end:02-Overview.Rmd-->

# Before we start
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/start.png", auto_pdf = TRUE)
``` 

During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command:

```{bash eval=FALSE}
source /pub21/sam/metagenomicsWorkshop2019.sh
```

Also, there’s a chance you’re currently not in your home directory, so let’s make sure you are with the following command:

```{bash eval=FALSE}
cd ~
```

<!--chapter:end:03-Start.Rmd-->

# Raw data
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/usb_stick.png", auto_pdf = TRUE)
``` 

The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets.

Have a browse of some of the projects by selecting one of the biomes on this page.

We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558).

## Obtaining the data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/link.png", auto_pdf = TRUE)
``` 

First, we need to create a directory to put the data in and then change directory to it.

```{bash eval=FALSE}
mkdir 1-Raw
cd 1-Raw
```

Now we can generate a symbolic link (i.e. shortcut) to the raw sequence data files, which will appear in the current directory:

```{bash eval=FALSE}
linkFiles
```

All that this command did is run a script which creates a ‘symbolic link’ (like a shortcut in Windows) to the read files that we will be using (The appendix of this document contains the commands used to download these files directly from the EBI metagenomics site) Now, check they are there with:

```{bash eval=FALSE}
ls
```

There should be six files in the directory, two for each sample in the dataset. e.g. `K1_R1.fastq.gz`

The file ID has three components:

- K1 is the sample ID. 
- R1 is for the first reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). 
- fastq.gz tells us that this is a zipped FASTQ file.

The sample labelling indicates the type treatment samples. The three samples are:

- __K1__: Fecal sample of individual of Korean diets
- __K2__: Fecal sample of individual of Korean diets
- __W1__: Fecal sample of individual of Western diets

So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here:

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/paired_reads.png", auto_pdf = TRUE)
``` 

This results in two sequences generated for each sequenced fragment: One reading in from the 3' end (R1) and the other reading in from the 5' end (R2).

FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files:

```{bash eval=FALSE}
zcat K1_R1.fastq.gz | head -n 4 | less -S
```

The pipe symbol ( `|` ) is used to pass the output of one command as input to the next command. So, this command (1) shows the unzipped contents of the FASTQ file, (2) displays only the first 4 lines, and (3) displays them without wrapping lines (with `–S`, for easy viewing).

The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for '+') and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters.  __To return to the command prompt, press__ `q`. 

Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample). At a later point in the tutorial, you will be asked to link to results derived from the full dataset for further processing.

## Checking quality control
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/mangify_glass.png", auto_pdf = TRUE)
``` 

We can generate and visualise various sequence data metrics for quality control purposes using the FastQC. Run FastQC on one of the files:

```{bash eval=FALSE}
fastqc K1_R1.fastq.gz
```

Once completed, view the output (NB: The & runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open):

```{bash eval=FALSE}
firefox K1_R1_fastqc.html &
```

The FastQC report contains a number of metrics. The first graph shows the sequence quality across the length of the reads: Note how it decreases as the length of the read increases. While this is normal with Illumina sequencing, we can improve the situation a bit...

Briefly inspect the FastQC report for yourself – There are examples of typical (and atypical!) FastQC data in the appendix of this document

Once you have finished looking, minimise the Firefox window.

<!--chapter:end:04-Raw_data.Rmd-->

# Quality control
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

Now that we've obtained the raw data and had a look at it, we should now clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets.

- Remove low quality bases from the end of the reads. These are more likely to be incorrect, so are best trimmed off.
- Remove adapters. Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. 
- Remove host sequences. If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome. Here, we do not need to do this, as the dataset contains barely any human genome sequences.

## Removing adapters and low quality bases
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/hedge_trimming.png", auto_pdf = TRUE)
``` 

First go back to your home directory and create a new directory where we will clean the sequences up:

```{bash eval=FALSE}
cd ..
```

This will move you one directory up, i.e. back to your home directory. Alternatively, you could use `cd ~` which will take you to your home directory. This is a good idea if you ever get lost!

```{bash eval=FALSE}
mkdir 2-Trimmed
cd 2-Trimmed
```

You are now in your newly created directory. Here we will run Trim Galore! which carries out both of these steps.

```{bash eval=FALSE}
trim_galore --paired --quality 20 --stringency 4 \
   ../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz
```

This is a longer command so we've split it across multiple commands (a `\` at the end of a line allows you to press return without running the command, meaning you can continue to add to that command. When this happens, the `$` changes to a `>`. __Note__ that if you do use the `\` character, the next character immediately after it must be return. If you use `\` in the middle of a line without pressing return afterwards, it will break the command!

This command will remove any low quality regions from the end of both reads in each read pair (quality score < 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too. We use the two read files for sample K1 as input, from the previous directory we were in.

Run this command two more times, but for the other two samples (K2 and W1)

## Rename the files
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/rename.png", auto_pdf = TRUE)
``` 

Once that is complete if you run:

```{bash eval=FALSE}
ls
```

you will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. e.g. K1_R1_val_R1.fq.gz could be shortened to K1_R1.fq.gz. So, we'll rename all of the files with the mv command:

```{bash eval=FALSE}
mv K1_R1_val_1.fq.gz K1_R1.fq.gz
mv K1_R2_val_2.fq.gz K1_R2.fq.gz
mv K2_R1_val_1.fq.gz K2_R1.fq.gz
mv K2_R2_val_2.fq.gz K2_R2.fq.gz
mv W1_R1_val_1.fq.gz W1_R1.fq.gz
mv W1_R2_val_2.fq.gz W1_R2.fq.gz
```

__Tip__: If you want to edit and reuse previous commands, press the up arrow key.

Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt)

## Inspect the trimmed data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/magnify_glass_good.png", auto_pdf = TRUE)
``` 

To see what difference the trimming made, run FastQC again on the trimmed output file K1_R1.fq.gz and view it. 

```{bash eval=FALSE}
fastqc K1_R1.fq.gz
firefox K1_R1_fastqc.html &
```

Inspect the FastQC report for yourself. How does it compare to the untrimmed data?

Now that we have trimmed data, we can start the analyses!



<!--chapter:end:05-Trimming_data.Rmd-->

# Taxonomic profiling
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

There are a number of methods for determining the species composition of a metagenomic data-set, but for the purposes of this practical we will use Kraken & Bracken (Bayesian Reestimation of Abundance with KrakEN).  Kraken classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. We will be using Kraken 1 as Kraken 2 is still in beta. Bracken uses the taxonomic labels assigned by Kraken to compute the abundance of species in a set of DNA sequences.

First, we'll make a new directory for it and move into it, after returning home:

```{bash eval=FALSE}
cd ..
mkdir 3-Taxonomy
cd 3-Taxonomy
```

## Running Kraken

Prior to running kraken we can set a variable so kraken knows where to look for the databases it will use.

export KRAKEN_DB_PATH=/pub39/tea/matthew/teaching/Kraken_db/MiniKraken/

__Note__: You can look at the contents of the above directory to see it currently contains the MiniKraken 4GB database. This database contains 2.7% of the kmers from the full Kraken database. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full Kraken database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. As of October 2017, this includes ~25,000 genomes, requiring 33GB of disk space.

Now, run Kraken on sample K1 by running the following command.

```{bash eval=FALSE}
kraken --paired --db minikraken_4GB --threads 4 --output K1.kraken.out \
../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz
```  

While this is running, let's look at what those command line options do:

- `../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz` : the trimmed read pairs for K1, which we use as input.
- `--paired` : Indicate that we are providing paired reads to Kraken. Internally, Kraken will concatenate the R1 and R2 reads into one sequence with an N between them.
- `--db` : Specify the kraken database to be used for taxonomic classification. Previous to the command we set the KRAKEN_DB_PATH so in this case the command will look for the directory called ‘minikraken_4GB’ within the KRAKEN_DB_PATH. Alternatively the full path of the required database could be provided.
- `--threads` : How many CPUs the process will use.
- `--output` : This is the output file. 

The output file of kraken contains the following columns:

- __"C"/"U"__: one letter code indicating that the sequence was either classified or unclassified.
- __The sequence ID__: Obtained from the FASTA/FASTQ header.
- __Taxonomy ID__: The ID Kraken used to label the sequence; this is 0 if the sequence is unclassified.
- __Length__: Nucleotide length of the sequence.
- A space-delimited list indicating the LCA (lowest common ancestor) mapping of each k-mer in the sequence. For example, "562:13 561:4 A:31 0:1 562:3" would indicate that:
   - the first 13 k-mers mapped to taxonomy ID #562
   - the next 4 k-mers mapped to taxonomy ID #561
   - the next 31 k-mers contained an ambiguous nucleotide
   - the next k-mer was not in the database
   - the last 3 k-mers mapped to taxonomy ID #562

The output to screen will show how many sequences are classified. This will be lower than normal as we are using the mini Kraken database.

In a real analysis you may use the command “kraken-filter” to ensure your Kraken classifications are high quality. Too many classifications are removed if you attempt it with this dataset, due to the minikraken database used, so we will skip this step.

Once the Kraken commands have finished running, run it on the other two samples. HINT: You will need to change all instances of K1 to K2 or W1 in the above command

## Visualising Kraken output

### Krona plot

Krona is an interactive metagenome species abundance visualisation tool. We need to get our data in the correct format before we view it, however, so we first use a Kraken-supplied script to convert the files:

```{bash eval=FALSE}
kraken-translate --db minikraken_4GB \
K1.kraken.out > K1.kraken.label
```

Next we convert these to a file format that krona will be able to read.

```{bash eval=FALSE}
cat K1.kraken.label | cut -f 2 | sort | uniq -c | tr ‘;’ ‘\t’ > K1.krona.txt
```

The above command uses the following steps:
- `cat K1.kraken.label`: cat the file to print out its contents.
- The pipe symbols `|` are used to pipe the output of one command to the next.
- `cut -f 2`: The cut command is used to cut out the 2nd column (field).
- `sort`: The output of the 2nd column is sorted, this is required for the next step.
- `uniq -c`: Unique count, find every unique string/entry and count how many of each there are.
- `tr ‘;’ ‘\t’`: Converts ‘;’ for ‘\t’ (tab) characters
- `> K1.krona.txt`: Redirect the output to the file called `K1.krona.txt`

If you would like to see exactly what the file is doing you can run the command bit by bit i.e

```{bash eval=FALSE}
cat K1.kraken.label
cat K1.kraken.label | cut -f 2
cat K1.kraken.label | cut -f 2 | sort
#etc.
```

Repeat the above commands for K2 and W1

Once we have all three files converted, we can combine them with a Krona packaged script to generate our charts.

```{bash eval=FALSE}
ImportText.pl -o all.krona.html ??.krona.txt
```

`-o` is our output html file, and the final argument `??.krona.txt` represents all of our .krona files in this directory: The `?` is a wild-card, meaning any character, so this identifies the files `K1.krona.txt` `K2.krona.txt` and `W1.krona.txt`.

Now we can view our chart in a web browser.

```{bash eval=FALSE}
firefox all.krona.html &
```

Which sample is the most different in terms of species that are present and absent?

## Running Bracken

Bracken (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by Kraken to compute the abundance of species in a metagenomic sample. 

Prior to Bracken we need to create a Kraken report file

```{bash eval=FALSE}
kraken-report --db minikraken_4GB K1.kraken.out > K1.kreport
```

Now it’s time for the bracken command

```{bash eval=FALSE}
bracken -d $KRAKEN_DB_PATH/minikraken_4GB \
-i K1.kreport -o K1.bracken -r 100 -l S -t 5
```

Let's look at what those command line options do:

- `-d` : Specify the Kraken database that was used for taxonomic classification. In this case bracken requires the variable `$KRAKEN_DB_PATH` so the option is provided the full path to the kraken database. For clarity try the command `ls ${KRAKEN_DB_PATH}/minikraken_4GG`
- `-i` : The Kraken report file, this will be used as the input.
- `-o` : The output bracken file. Information about its contents is below.
- `-r 100`: This is the ideal length of the reads that were used in the kraken classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as it is a paired library of 100 bp reads.
- `-l S`: This specifies the taxonomic level/rank of the Bracken output. In this case S is equal to species with the other options being 'D','P','C','O','F' and 'G'.
- `-t 5`: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10).

The output file of Bracken contains the following columns:

1. Name: Name of taxonomy at the specified tax level.
2. Taxonomy ID: NCBI taxonomy id
3. Level ID: Letter signifying the taxonomic level of the classification
4. Kraken assigned read: Number of reads assigned assigned the taxonomy by Kraken.
5. Added reads with abundance reestimation: Number of reads added to the taxonomy by Bracken abundance reestimation. 
6. Total reads after abundance reestimation: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis.
7. Fraction of total reads: Relative abundance of the taxonomy.

Use `less` or `vim` to look at the bracken output.

Repeat the above commands for K2 and W1

To make full use of Bracken output, it is best to merge the output into one table. However before we do this we’ll copy the Bracken output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples.

```{bash eval=FALSE}
cp $DB/bracken/* .
```

Now to merge all the Bracken files.

```{bash eval=FALSE}
combine_bracken_outputs.py --files *.bracken -o all.bracken
```

This output file contains the first three columns:
- __name__ = Organism group name. This will be based on the TAX_LVL chosen in the bracken command and will only show the one level
- __taxonomy_id__ = Taxonomy id number
- __taxonomy_lvl__ = A single string indicating the taxonomy level of the group. ('D','P','C','O','F','G','S').

Following these columns are two columns for each sample.

- `${SampleName}.bracken_num`: The number of reads after abundance 
- `${SampleName}.bracken_frac`: Relative abundance of group in sample

We want a file with only the first column and the bracken_num column for each sample. We can therefore use the following commands.

Create a sequence of numbers that will match the bracken_num column numbers. Use “seq --help” to see how the seq command works.

```{bash eval=FALSE}
bracken_num_columns=$(seq -s , 4 2 50)
echo $bracken_num_columns
```

Now to use the variable to extract the bracken_num columns plus the first column (species names).

```{bash eval=FALSE}
cat all.bracken | cut -f 1,$bracken_num_columns > all_num.bracken
```

## LEfSe biomarker detection

We will use LEfSe (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. LEfSe couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions.

In essence it allows for the detection of biomarkers when comparing sample groups. In the LEfSe terminology the sample groups are called the class.

We need to format our bracken file to be ready for LEfSe. First we will copy the file so we have a backup in case we do anything wrong.

```{bash eval=FALSE}
cp all_num.bracken all_num.lefse.bracken
```

This next part must be done for further commands to work

Using your favourite text editor add the following line to the top of your all_num.lefse.bracken file. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator.

diet	K	K	K	K	K	K	K	K	K	K	K	K	W	W	W	W	W	W	W	W	W	W	W	W

i.e. the above is diet followed by 12 Ks and 12 Ws

The singular line should match the order of your samples within the file. This is the metadata line that LEfSe will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples.

The easiest way to install LEfSe is through conda (for info to install conda and LEfSe through conda please see the following instructions https://conda.io/en/latest/, https://anaconda.org/bioconda/lefse). We will activate a preconstructed LEfSe conda environment with the following script.

```{bash eval=FALSE}
lefse_env.sh
```

Although we have formatted the input file already we need to further format and preprocess it with a LEfSe script.

```{bash eval=FALSE}
lefse-format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000
```

- `all_num.lefse.bracken` : Input Bracken file.
- `all_num.lefse` : Output file formatted for the run_lefse command, which we will soon run
- `-c 1` : Specifies the row with the class info. This is used to determine which sample will be compared against which samples. In this case it is the 1st row with the Ks and Ws.
- `-u 2` : Specifies the row with the sample names. This is the second row in this case.
- `-o 1000000` : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. LEfSe developers recommend 1000000 (1 million) when very low values a present. We generally always use 1 million for consistency.

Now to run LEfSe. All we need to do is run the command with the formatted input and provide an output file name.

```{bash eval=FALSE}
run_lefse.py all_num.lefse all_num.lefse.out
```

The output file is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a "-".

- Biomarker name
- Log of highest class average, i.e. Get the class with the greater amounts of the biomarker, average the counts and then get the log of this value.
- Class with the greater amounts of biomarker
- LDA effect size: A statistical figure for LEfSe. Only features with an LDA >2 are detected as biomarkers by default. The higher the LDA effect size is the more of an effect the biomarker causes.
- p-value: biomarkers must have a p-value of <0.05 to be considered significant.
T
he LDA effect size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes.

Next we can visualise the output.

```{bash eval=FALSE}
lefse-plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png
```

- `--dpi 200` : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. We’ve chosen 200 as it is good quality and we will not be publishing these results.
- `--format png` : Format of output file. png is a commonly used file format for images.
- `all_num.lefse.out` : LEfSe output to visualise.
- `biomarkers.png` : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance.

Look at the figure with the program okular:

```{bash eval=FALSE}
okular biomarkers.png
```

Which species causes the biggest effect in the W class and in the K class? Which class has more biomarkers associated with it?

<!--chapter:end:06-Taxonomic_profiling.Rmd-->

# Functional profiling
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

It is also possible to investigate functional differences between metagenome (and metatranscriptome) samples by directly interrogating the read data. We will now look at how this can be done with a package called HUMANn2 (The HMP Unified Metabolic Analysis Network 2), a pipeline designed to accurately profile the presence/absence and abundance of microbial pathways in metagenomic sequencing data.

## A demo run of HUMANn2 

First, we will carry out an example run of the software and briefly explore the output files. Make a new directory and move into it, after moving back to your home directory.

```{bash eal=FALSE}
cd ..
mkdir 4-FunctionalProfiling
cd 4-FunctionalProfiling
```

Now we will  perform the run with HUMANn2 so we can inspect the output files. First, copy over some test data. $DB is simply an alias used for a directory containing database files used in this tutorial.

```{bash eval=FALSE}
cp $DB/demo.fq.gz .
```

This is a demonstration FASTQ file that we will use. It will be small enough to run HUMANn2 in a reasonable time.

```{bash eval=FALSE}
humann2 \
  --input demo.fq.gz \
  --output demo.humann2 \ 
  --threads 10
```

Here, we are telling the software to use demo.fq.gz as input and to create a new output directory where results will be generated. 

As the software runs, you might notice that as part of the process, HUMANn2 runs MetaPhlAn2. The purpose of this is to identify what species are present in the sample, so that it can then tailor generate an appropriate database of genes (from those species) to map against. It will carry out this alignment against the gene database, then a protein database, and finally compute which gene families are present to determine which functional pathways are present and how abundant they are.

Once the run has completed, change into the newly created output directory and list the files that are there.

```{bash eval=FALSE}
cd demo.humann2
ls
```

You will see that there are three files and one directory. The directory (demo_humann2_temp) contains intermediate temporary files and can be disregarded here. Sometimes, these files can be useful for debugging, however. The three files are:

- `demo_genefamilies.tsv`: A table file showing the number of reads mapping to each UniRef90 gene family, Values are normalised by the length of each gene family (i.e. RPK, or Reads per Kilobase). Additionally, the values are stratified so that they show the overall community abundance but also a breakdown of abundance per species detected. This allows researchers to delve into species specific functions, rather than only looking at the metagenomic functions as a whole,
- `demo_pathabundance.tsv`: As above, a table file showing the normalised abundance of MetaCyc pathways. These abundances are calculated based on the UniRef90 gene family mapping data and are also stratified by species.
- `demo_pathcoverage.tsv`: Similar to above, except instead of abundances of pathways, this table shows the coverage, or completeness, of pathways. For example, a pathway may contain 5 components (or genes/proteins)
   - Pathway1 :         A → B → C → D → E		100% complete
   - A species identified in the sample may only have four of the components, though, 	meaning that the pathway is only 80% complete (represented as 0.8)
    - Pathway1 :         A → B → C → ~~D~~ → E		80% complete

The basic format of these three output files is the same, so let's take a look at the pathway abundance table.

```{bash eval=FALSE}
less demo_pathabundance.tsv
```

You will see that there are two columns. The first shows the pathway (or UNMAPPED where reads could not be aligned, and UNINTEGRATED where reads could be aligned, but to targets not implicated in any pathways). The second column shows the abundance. Press `q` to exit and let's look at one specific pathway, COA-PWY-1 (a coenzyme A biosynthesis II pathway).

```{bash eval=FALSE}
grep COA-PWY-1 demo_pathabundance.tsv
```

This shows two entries with two different values, I.e.

- COA-PWY-1: coenzyme A biosynthesis II (mammalian)	6.3694267516 
- COA-PWY-1: coenzyme A biosynthesis II (mammalian)|g__Bacteroides.s__Bacteroides_vulgatus	4.7961630695

This is an example of the species stratification mentioned above. The first line shows the abundance of this pathway across the whole sample, but the second line shows the abundance contributed from Bacteroides_vulgatis.

Have a look at the other two output files; note the similar layout.

Finally, return to the parent directory i.e. `4-FunctionalProfiling`

```{bash eval=FALSE}
cd ..
```

## Statistical comparison between samples

Looking at the functional profile of one sample in isolation is usually not very informative. First, there is nothing to compare it to and second, there are no biological replicates. We will therefore use all the Korean and Western diet samples.

It would take many hours to analyse all of the data using HUMANn2 and is outside of the scope of this practical. For this reason, samples were analysed prior to the workshop to generate the output files we covered above. For the purposes of this comparison, we will look at the pathway abundances only. First copy over the results data directory and have a look in it (when copying directories, we need to use –r with the cp command, to recursively copy all files across)

```{bash eval=FALSE}
cp –r $DB/DietPathAbundance .
ls DietPathAbundance
```

You will see there are 12 files prefixed with K and 12 prefixed with W, for the Korean diet and Western diet samples, respectively. Take a look in one of the files; you might notice that there is no species stratification: The reason for this is for this test, all pathway abundances have been collapsed to a community level, ignoring differences in species. When you analyse your own data-sets, you can leave this stratification in to identify significant differences between functional profiles at a species level.

### Combining data

First, we need to combine these 24 tables into one large results table. HUMANn2 provides a tool to do this:

```{bash eval=FALSE}
humann2_join_tables --input DietPathAbundance/ --output diet.tsv
```

This command will look for all tables in the DietPathAbundance directory and generate a large, 24 column table called diet.tsv. You can take a look to see that this has worked correctly.

```{bash eval=FALSE}
less -S diet.tsv
```

### Renormalising data

The next step is to renormalise the data. Currently, all of the abundance values are only normalised within each sample, i.e. accounting for the size of the pathways/length of genes. However, they are not normalised between samples, and this is very important. For example, if we had sequenced two samples, A and B, and we obtained 5 million reads for sample A and 20 million reads for sample B, without normalisation, it might look that there was up to 4x as much functional activity in sample B! To correct for this, we normalise the abundance values based on the number of reads in each samples, i.e. counts per million, or cpm. (We could also normalise to relative abundance where all abundances for each sample add up to 1).

```{bash eval=FALSE}
humann2_renorm_table \
  --units cpm \
  --input diet.tsv \
  --special n \
  --output diet.cpm.tsv
```

This command generates the normalised data in the new table `diet.cpm.tsv`. The `--special n` option tells the script to remove all unmapped and unassigned values from the table.

### Visualisation
Now that we have our data normalised, we can visualise the dataset to see how the two groups look, i.e. do samples in the same diet group appear to correlate well with each other and are they distinguishable from those from the other diet group? To do this, we will draw a PCA plot. So that the plot will show which samples are from which group, we will need to provide the script with some metadata. This has been prepared and can be copied over as follows

```{bash eval=FALSE}
cp $DB/diet.metadata.tsv .
```

This is a table file where the first column is the sample name with subsequent columns representing categories of metadata. Here, the column of interest is the second one, 'Diet', and samples are labelled as either 'Western' or 'Korean'. We can now run the PCA plot generation script and look at the result.
```{bash eval=FALSE}
PCA_plot.r diet.cpm.tsv diet.metadata.tsv Diet diet.pca.pdf
```

This command takes our data table as the first argument, our metadata table as the second argument, the metadata category of interest as the third argument and the output file as the final argument. 

Now we can view the plot.

```{bash eval=FALSE}
okular diet.pca.pdf &
```

You should see 3 plots, each one plotting 2 of the first 3 principal components against each other. (see following image). The axis of each plot also shows how much of the overall variance that that particular component accounts for. Blue and red dots show the Western and Korean diet samples, respectively, and samples that cluster closely together are more similar to each other. 
From this, we can see that there is some separation between the two dietary groups, but that they are not completely separated, i.e. there is some overlap. This can happen frequently with datasets such as this, that are likely to be much less controlled than laboratory samples and therefore show more between-group variance. 

Is there anything unexpected about any of the samples, from these plots? If so, what might be a sensible course of action before proceeding with statistical analysis?

### Finding statistically significant differences

For the final part of this section, we will see if there are any statistically significant differences between the two sample groups. There are several ways in which this can be achieved but we will carry out a Kruskal-Wallis Test, using a script supplied with HUMAnN2. This script is suitable for test datasets but more robust methods should be used for other datasets (these will be discussed in day 3).  Before we can do this, we need to add the metadata for the samples to the data table itself, as a new row just below the header. 

```{bash eval=FALSE}
addMetadataToHumann2Table \
  --table diet.cpm.tsv \
  --metadata diet.metadata.tsv \
  --output diet.cpm.anno.tsv
```

You can take a look at the new metadata-annotated table as follows.

```{bash eval=FALSE}
less -S diet.cpm.anno.tsv
```

Now we can run the statistical test.

```{bash eval=FALSE}
humann2_associate \
  --input diet.cpm.anno.tsv \
  --last-metadatum Group \
  --focal-metadatum Diet \
  --focal-type categorical \
  --output diet.sigdif.tab
```

Besides the input and output arguments, we also tell the script which category to use (`--focal-metadatum`),  and what sort of category it is, i.e. whether it is continuous or categorical data (`--focal-type`). We also need to tell it what the last category of metadata is in the header so that it knows where the actual table data begins (`--last-metadatum`). Let's look at the results and see if there are any pathways which are significantly different in abundance between the Western and Korean diet groups.

```{bash eval=FALSE}
less -S diet.sigdif.tab
```

Here, there are 4 columns. Feature (i.e. pathway), Level means (mean abundance per group for that pathway), P-value, and Q-value (false-discovery rate adjusted P-value). By this test, there are 6 significantly (5% cut-off) differentially abundant pathways between the two groups. These would make an ideal starting point for further scientific investigation, but for this workshop, we will now leave this set and return to our original dataset!

<!--chapter:end:07-Functional_profiling.Rmd-->

# Metagenome assembly
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

So far we have directly analysed the read data itself which is perfectly fine for taxonomic profiling and for certain methods of functional profiling. However, Illumina reads are generally short and therefore can not provide us with much data on larger constructs that are in the metagenomic samples, e.g. genes. While it is possible to predict from which gene a sequence read might originate, the short nature of the query can sometimes lead to ambiguous results. 

Additionally, depending on the application it can become computationally intensive to analyse large numbers of reads. Here, we are only using samples with 1 million reads. Some metagenome samples consist of 50-100 million+ read pairs. If such a sample belonged to a set of 100 samples, that would be up to 10 billion read pairs, or 2 trillion bases of sequence data, with many of these being redundant.

For this reason, it is sometimes advantageous to assemble the reads into contigs, using a meta-genome assembler. This has the dual effect of:

- Reducing the overall size of the data for analysis. If a metagenome was sequenced at 50x depth, then by assembling it you could theoretically reduce the amount of sequence to analyse by 50-fold.
- Increase the size of the fragments you will analyse. This is the main advantage of an assembly, as the ~100 bp reads can be pieced together to form 100,000 kb+ contigs. These contigs will contain complete genes, operons and regulatory elements: Reconstructed genome sections.

Here, we will carry out a couple of assemblies on our dataset.

## A primer on short read assembly

Illumina reads are too short and numerous to use traditional overlap-layout-consensus assemblers as such an approach would be far too computationally intensive. Instead, we use De Bruijn graph based assemblers. Briefly, these operate as follows:

1. All reads are broken down in to k-length overlapping fragments (k-mers). e.g. if we choose a k-mer size of 5 bp, the following two sequences (blue) would be broken down into the k-mers below them (red):

__(A plot need to go here, see old manual)__

2. All k-mers are linked to other k-mers which match with a k-1 length overlap (i.e. that overlap by all but one base:
   - GGCAT→GCATGC→CATGC→ATGCA→TGCAG→GCAGG→CAGGA

3. Paths are routed through the graph and longer contigs are generated:
   - __G__GCAT→__G__CATGC→__C__ATGC→__A__TGCA→__T__GCAG→__G__CAGG→__CAGGA__
   - → GGCATGCAGGA

The example here is a vast oversimplification of the complexity of a De Bruijn graph (i.e. there are no branches!). Routing through the graph is never as simple as this as some k-mers will lead to multiple k-mers, which can result in the break point of a contig. This is especially true for complex metagenomic data.

Generally speaking, the shorter the k-mer, the more branches there will be, the trickier the graph is to resolve, so the resulting contigs are smaller. Assemblers usually perform better with longer k-mer lengths but even then there might not be enough depth of sequencing to generate all k-mers that form overlaps, therefore leading to break points. Finding the right k-mer size usually involves testing several. 

Fortunately, the assembler we will use, MEGAHIT, allows us to build an assembly using multiple k-mer lengths iteratively. The other great advantage about MEGAHIT is that it is quick and efficient.  We will use MEGAHIT on our data soon, but first there is an additional processing step for our sequences...

## Stitching read pairs

As mentioned, longer k-mers generally perform better, but as our maximum read length is 100 bp, we are limited to a maximum k-mer length of 99 bp. However, we can get even longer k-mers if we stitch our read pairs together. 

Remember that a read pair consists of two sequences read from each end of a fragment of DNA (or RNA). If the two sequences meet in the middle of the fragment and then overlap, there will be a region of homology which we can use to merge the two reads in the pair together (See next image).

First, we obtain our forward and reverse reads, derived from different ends of the same fragment. Second, we look for sufficient overlap between the 3' ends of our sequences. Third, if there is sufficient overlap, we combine, or stitch, the two reads together to form one long sequence.

```{r, fig.align = 'center',out.width= '50%', echo=FALSE }
knitr::include_graphics(path = "figures/merging_paired_reads.png", auto_pdf = TRUE)
``` 

Once we have longer stitched reads, we can increase the k-mer length for our assembly. 

There are a number of pieces of software that can be used to stitch reads (e.g. Pear,  Pandaseq) but today we will use one called FLASH:

Make a new output directory for the stitched reads and run FLASH:

```{bash eval=FALSE}
cd ..

mkdir 5-Stitched

cd 5-Stitched

flash  -o K1 -z -t 12 -d . \
../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz
```

Here, we are telling flash to use an output file name prefix of K1, that the input is zipped, that the output directory is here (.) and to use the two read files for Sample K1. Once FLASH has finished running, it will display on screen how well the stitching process went, in this case a low amount of reads were combined. Have a look what files have been generated
```{bash eval=FALSE}
ls
```

We have three new fastq.gz files. One containing the stitched reads (`K1.extendedFrags.fastq.gz`) and two containing the reads from pairs that could not be combined (`K1.notCombined_1.fastq.gz` and `K1.notCombined_2.fastq.gz)`.

We can also see what the new read lengths are:

```{bash eval=FALSE}
less K1.histogram
```

Scroll down with the down key and you will see that we are looking at a histogram showing the proportion of reads at different lengths. We can now start assembling our stitched reads for this sample.

## Assembly

Create a new directory to store our assembly in and run the metagenome assembler MEGAHIT using our newly stitched read data.

```{bash eval=FALSE}
cd ..
mkdir 6-Assembly
cd 6-Assembly
megahit \
-r ../5-Stitched/K1.extendedFrags.fastq.gz \
-1 ../5-Stitched/K1.notCombined_1.fastq.gz \
-2 ../5-Stitched/K1.notCombined_2.fastq.gz \
-o K1 \
-t 12 \
--k-list 29,49,69,89,109,129,149,169,189
```

Here, you have instructed MEGAHIT to use both the stitched and unstitched reads, to output the assembly in a subdirectory called K1 and to use 12 CPUs. 

The last option `--k-list` instructs MEGAHIT to first generate an assembly using a k-mer size of 29 bp and when that is complete, integrate the results into an assembly using a k-mer size of 49 bp, and so on up to a final iteration using a k-mer size of 189 bp. This large range of k-mer lengths should give us a good assembly, given the data. However, it may take a while to run so this might be a good time to either read on, or go back and look at some of the questions/suggestions in green that you have missed. If you need a command prompt (your current one is gone because MEGAHIT is running), Right-click on the grey background → Applications → Terminal Emulators → Xterm, to open a new terminal.

Once the assembly is completed, we can look at the output FASTA file containing the contigs:

```{bash eval=FALSE}
less K1/final.contigs.fa
```

We can also generate some metrics based on the assembly

```{bash eval=FALSE}
assembly.py \
  -i K1/final.contigs.fa \
  -o K1/final.contigs.stats \
  -c stats

less K1/final.contigs.stats
```

This stats file tells us quite a bit about the assembly quality. Two definitions that you may not be aware of are N50 and N50 length (or, somewhat confusingly, L50 and N50, respectively!).  If we were to order our contigs from largest to smallest, and total up the sizes from biggest downwards, the contig we reach where our total is 50% of the size of the whole assembly is the N50 contig (the smaller the number the better). The N50 length is the length of this contig; a weighted median contig length.

__(add vis to explain N50)__

How do the contig metrics compare to the original reads? 
Now we have an assembly, albeit not a brilliant one due to us only having used 1 million reads, we can start to explore it.




<!--chapter:end:08-Metagenome_assembly.Rmd-->

