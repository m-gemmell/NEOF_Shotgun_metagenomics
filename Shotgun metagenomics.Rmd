---
title: "Shotgun Metagenomics"
author: "Sam Haldenby and Matthew R. Gemmell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
favicon: figures/NEOF_favicon.png
description: NEOF book for the Microbial shotgun metagenomics workshop
cover-image: "figures/NEOF.png"
---
```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```

```{r cite-packages, include = FALSE}
# automatically create a bib database for R packages
# add any packages you want to cite here
knitr::write_bib(c(
  .packages(), 'bookdown', 'webexercises'
), 'packages.bib')
```

```{r, echo=FALSE}
#Change colour, border, and text of code chunks
#Check style.css for .Rchunk
#https://stackoverflow.com/questions/65627531/change-r-chunk-background-color-in-bookdown-gitbook
#https://bookdown.org/yihui/rmarkdown-cookbook/chunk-styling.html
knitr::opts_chunk$set(class.source="Rchunk") 
```

```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/NEOF.png", auto_pdf = TRUE)
```

# Introduction

```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/squid.png", auto_pdf = TRUE)
```

This practical session aims to introduce you to the analysis of Shotgun metagenomic data. The topics covered are:

+:-------------------------------------------------------------------------:+:-----------------------------------------------------------:+
| [**Overview**](#overview)                                                 | [**Raw data**](#rawdata)                                    |
|                                                                           |                                                             |
| [![](figures/overview.png){width="135"}](#overview)                       | [![](figures/usb_stick.png){width="154"}](#rawdata)         |
+---------------------------------------------------------------------------+-------------------------------------------------------------+
| [**Trimming data**](#qualcont)                                            | [**Host removal**](#hostremoval)                            |
|                                                                           |                                                             |
| [![](figures/quality_trimming_and_filtering.png){width="274"}](#qualcont) | [![](figures/bowtie2.png){width="208"}](#hostremoval)       |
+---------------------------------------------------------------------------+-------------------------------------------------------------+
| [**Taxonomic profiling**](#taxprofile)                                    | [**Functional profiling**](#functprofile)                   |
|                                                                           |                                                             |
| [![](figures/classification.png){width="158"}](#taxprofile)               | [![](figures/profile.png){width="113"}](#functprofile)      |
+---------------------------------------------------------------------------+-------------------------------------------------------------+
| [**Metagenome assembly**](#metagenomeassembly)                            | [**Binning**](#binning)                                     |
|                                                                           |                                                             |
| [![](figures/jigsaw.png){width="211"}](#metagenomeassembly)               | [![](figures/yarn_binning.png){width="343"}](#binning)      |
+---------------------------------------------------------------------------+-------------------------------------------------------------+
| [**Functional annotation**](#funcanno)                                    | [**Appendix**](#mamba-installs)                             |
|                                                                           |                                                             |
| [![](figures/pathways.png){width="150"}](#funcanno)                       | [![](figures/mamba_logo.png){width="217"}](#mamba-installs) |
+---------------------------------------------------------------------------+-------------------------------------------------------------+

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" alt="Creative Commons Licence" style="border-width:0"/></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.

<!--chapter:end:01-Shotgun_metagenomics.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Overview {#overview}
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/overview.png", auto_pdf = TRUE)
``` 

## What is metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/what.png", auto_pdf = TRUE)
``` 

__Meta /ˈmɛtə/ : prefix meaning “higher” or “beyond”__

Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome.

## Why metagenomics?
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/why.png", auto_pdf = TRUE)
``` 

Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. 

While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species' neighbours, i.e. what else is in its natural environment?  Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture.

## Metagenomics vs Metagenetics
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/16s_vs_shotgun.png", auto_pdf = TRUE)
``` 

Broadly speaking, there are two families of metagenomic analysis: 

- __Amplicon-based__: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions.
- __Shotgun__: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referenced as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session.

## Tutorial overview
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/tutorial.png", auto_pdf = TRUE)
``` 

### Basics

This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. 

The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. 

### Structure

We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. There will be no instructor telling you what to type and click. Instead, everything you require to carry out the practical is written in the document. Take your time; it's important to spend some time understanding why you are running the commands, rather than simply typing them out. 

If at any point you are having trouble or have a question, let one of us know and we'll provide 1-to-1 assistance.

### Content
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/sections.png", auto_pdf = TRUE)
``` 

This practical is broken up into the following broad sections.

1. __Raw data__: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples.
2. __Trimming data__: This entails preprocessing our data to ensure that it is of good quality.
3. __Host removal__: When sequencing the genomic content of host's microbiota (bacteriome, archaeome, mycobiome, and more) it is likely you will also sequence the host's genome. This step shows a method of removing possible host contamination.
3. __Taxonomic profiling__: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples.
4. __Functional profiling__: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples.
5. __Metagenome assembly__: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will 'stitch' the reads together to ensure we get the best assembly possible.
7. __Binning__: This step attempts to seperate each assembled genomes into bins. These genome assemblies are called Metagenome-assembled Genomes (MAGs).
8. __Functional annotation__: We will take our MAGs, predict genes and then functionally annotate them with MetaCyc.

<!--chapter:end:02-Overview.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Cluster Introduction
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/cluster.png", auto_pdf = TRUE)
``` 

## Logon instructions
For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent.

You will now be in a logged-in Linux VNC desktop. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance.

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/logon_pic.png", auto_pdf = TRUE)
``` 

If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal.

These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux 

The following link is a guide to install Mint Linux:  
https://linuxmint-installation-guide.readthedocs.io/en/latest/

## The Terminal Window
In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different.
```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/terminal_window.png", auto_pdf = TRUE)
``` 

Already there is useful information for us on the terminal window.

- __nsc065__: This is the login name, also known as the username. In this case nsc065 is a demonstrator's account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into.
- __gauss03__: This is the machine name the user is logged into.
- __\~__: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others __'~'__ is a shortcut to the user's home directory.
- Everything after the __'$'__ is where commands are typed into the terminal. This is also referred to as the command line.

__To open a new terminal window__, right click on the main screen, choose `Applications` -> `Shell` -> `bash`

<!--chapter:end:03-Cluster_Introduction.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Startup & mamba
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/mamba_logo.png", auto_pdf = TRUE)
```

During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command (ensure this starts with a full stop and a space `. `):

```{bash eval=FALSE}
. useshotgun
```

The `use` scripts in this workshop are custom scripts that set up `mamba` environments. You can look at the above script with `less /usr/local/bin/useshotgun` if you are interested in its contents.

Also, there’s a chance you’re currently not in your home directory, so let’s make sure you are with the following command:

```{bash eval=FALSE}
cd ~
```

<!--chapter:end:04-Start_conda.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# (PART\*) Data & QC {-}

# Raw data {#rawdata}
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/usb_stick.png", auto_pdf = TRUE)
``` 

The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets.

Have a browse of some of the projects by selecting one of the biomes on the website.

We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558).

## Obtaining the data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/link.png", auto_pdf = TRUE)
``` 

First, we need to create a directory to put the data in and then change directory to it.

```{bash eval=FALSE}
mkdir 1-Raw
cd 1-Raw
```

Now we can generate a symbolic links (i.e. shortcut) to the raw sequence data files, which will appear in the current directory:

```{bash eval=FALSE}
ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/raw_fastq/* .
```

If you would like to know more about the `ln` command please check out: https://linuxize.com/post/how-to-create-symbolic-links-in-linux-using-the-ln-command/.

Now, check the symbolic links are in your current directory:

```{bash eval=FALSE}
ls
```

There should be six files in the directory, two for each sample in the dataset. e.g. `K1_R1.fastq.gz`

The file ID has three components:

- K1 is the sample ID. 
- R1 is for the forward reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). 
- fastq.gz tells us that this is a gzipped FASTQ file.

The three samples are:

- __K1__: Fecal sample of individual of Korean diets
- __K2__: Fecal sample of individual of Korean diets
- __W1__: Fecal sample of individual of Western diets

So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here:

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/paired_reads.png", auto_pdf = TRUE)
``` 

This results in two sequences generated for each sequenced fragment: One reading in from the 3' end (R1) and the other reading in from the 5' end (R2).

FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files:

```{bash eval=FALSE}
zcat K1_R1.fastq.gz | head -n 4 | less -S
```

The pipe symbol ( `|` ) is used to pass the output of one command as input to the next command. So, this command (1) shows the unzipped contents of the FASTQ file, (2) displays only the first 4 lines, and (3) displays them without wrapping lines (with `–S`, for easy viewing).

The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for '+') and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters.  

__To return to the command prompt, press__ `q`. 

Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample).

## Checking quality control
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/mangify_glass.png", auto_pdf = TRUE)
``` 

We can generate and visualise various sequence data metrics for quality control purposes using `FastQC`. We will run `FastQC` on the R1 and R2 reads separately as it is good to visualise them in two different reports. This is because R1 and R2 reads have different quality patterns, generally due to the poorer quality of R2. 

Run `FastQC` on the files:

```{bash eval=FALSE}
#R1 fastqc
#Make an output directory
mkdir R1_fastqc
#Run fastqc on all the R1.fastq.gz files 
#* matches any pattern
#*R1.fastq.gz matches any file that ends R1.fastq.gz in the current directory
#-t 3 indicates to use 3 threads, chosen as there are three R1 files
fastqc -t 3 -o R1_fastqc *R1.fastq.gz

#R2 fastqc
#Make output directory
mkdir R2_fastqc
#Run fastqc
fastqc -t 3 -o R2_fastqc *R2.fastq.gz
```

Once the `FastQC` commands are run we can run `MultiQC` to create interactive html reports for the outputs.

```{bash eval=FALSE}
#R1 multiqc fastqc report
#Create output directory
mkdir R1_fastqc/multiqc
#Create multiqc output
multiqc -o R1_fastqc/multiqc R1_fastqc

#R2 multiqc fastqc report
#Create output directory
mkdir R2_fastqc/multiqc
#Create multiqc report
multiqc -o R2_fastqc/multiqc R2_fastqc
```

Once completed, view the `MultiQC` reports (NB: The `&` runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open).

This is a longer command so we've split it across multiple lines with bash escape. A `\` at the end of a line allows you to press return/enter without running the command, meaning you can continue to add to that command. When this happens, the `$` changes to a `>`. For more information please see our [Intro to Unix materials](http://www.cgr.liv.ac.uk/illum/NEOFworkshops_5bfa93ca0482d69d/Intro_to_Unix/05-Tips_and_tricks.html#bash-escape)

__Note__ if you do use the `\` character, the next key you press must be return/enter. If you use `\` in the middle of a line without pressing return afterwards, it will break the command!

```{bash eval=FALSE}
firefox R1_fastqc/multiqc/multiqc_report.html \
R2_fastqc/multiqc/multiqc_report.html &
```

The `FastQC` report (via `MultiQC`) contains a number of metrics. The "Sequence Quality Histograms" shows the sequence quality across the length of the reads, you can hover over each line to show which sample it belongs to. Note how quality decreases as the length of the read increases. While this is normal with Illumina sequencing, we will improve the situation a bit in the next chapter.

For more information on the plots of FactQC please see this [online resource](https://rtsf.natsci.msu.edu/genomics/tech-notes/fastqc-tutorial-and-faq/#:~:text=FastQC%2C%20written%20by%20Simon%20Andrews,on%20a%20sequence%20data%20set).

Once you have finished inspecting, minimise the Firefox window.

<!--chapter:end:05-Raw_data.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Quality control {#qualcont}
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_trimming_and_filtering.png", auto_pdf = TRUE)
``` 

Now that we've obtained the raw data and had a look at it, we should clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. 

There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets.

- Remove low quality bases from the end of the reads: These are more likely to be incorrect, so are best trimmed off.
- Remove adapters: Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. 
- Host removal: If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome.

## Removing adapters and low quality bases
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/hedge_trimming.png", auto_pdf = TRUE)
``` 

Go back to your home directory and create a new directory where we will clean the sequences up:

```{bash eval=FALSE}
cd ..
mkdir 2-Trimmed
cd 2-Trimmed
```

You are now in your newly created directory. Here we will run `Trim Galore!` which removes low quality bases and adapters.

```{bash eval=FALSE}
trim_galore --paired --quality 20 --stringency 4 \
../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz
```

This command will remove any low quality regions from the end of both reads in each read pair (quality score < 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too.

__Task__: Rerun this command for the other two samples (K2 and W1). Try to run these without looking at the help box below.

`r hide("trim_galore commands")`
```{bash eval=FALSE}
#K2
trim_galore --paired --quality 20 --stringency 4 \
../1-Raw/K2_R1.fastq.gz ../1-Raw/K2_R2.fastq.gz
#W1
trim_galore --paired --quality 20 --stringency 4 \
../1-Raw/W1_R1.fastq.gz ../1-Raw/W1_R2.fastq.gz
```
`r unhide()`

## Rename the files
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/rename.png", auto_pdf = TRUE)
``` 

Once that is complete list the contents of your directory:

```{bash eval=FALSE}
ls
```

You will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. For example K1_R1_val_1.fq.gz could be shortened to K1_R1.fq.gz. So, we'll rename all of the files with the mv command:

```{bash eval=FALSE}
mv K1_R1_val_1.fq.gz K1_R1.fq.gz
mv K1_R2_val_2.fq.gz K1_R2.fq.gz
mv K2_R1_val_1.fq.gz K2_R1.fq.gz
mv K2_R2_val_2.fq.gz K2_R2.fq.gz
mv W1_R1_val_1.fq.gz W1_R1.fq.gz
mv W1_R2_val_2.fq.gz W1_R2.fq.gz
```

__Tip__: If you want to edit and reuse previous commands, press the up arrow key.

__Task__: Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt).

## Inspect the trimmed data
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/magnify_glass_good.png", auto_pdf = TRUE)
``` 

To see what difference the trimming made, run `FastQC` and `MultiQC` again on the trimmed output files and view it. 

```{bash eval=FALSE}
#R1 fastqc and multiqc
mkdir R1_fastqc
fastqc -t 3 -o R1_fastqc *R1.fq.gz
mkdir R1_fastqc/multiqc
multiqc -o R1_fastqc/multiqc R1_fastqc
```

__Task__: Run `FastQC` and `MultiQC` for the R2 files and then view the R1 and R2 `MultiQC` reports with firefox. Try to run the commands without looking at the help box below.

How does the quality compare to the untrimmed data?

`r hide("R2 commands")`
```{bash eval=FALSE}
#R2 fastqc and multiqc
mkdir R2_fastqc
fastqc -t 3 -o R2_fastqc *R2.fq.gz
mkdir R2_fastqc/multiqc
multiqc -o R2_fastqc/multiqc R2_fastqc
```
`r unhide()`

<!--chapter:end:06-Trimming_data.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Host removal {#hostremoval}
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/bowtie2.png", auto_pdf = TRUE)
``` 

It is good practice to remove any host sequences from your data before further analysis. A good method for this is to align/map your reads to a reference of your host genome and remove the mapped sequences (i.e sequences we believe to belong to the host).

If there is no host genome available before you start your sample collections and sequencing it may be a good idea to attempt to sequence and assemble the host genome. We would recommend long read technologies for single genome assembly projects.

This chapter contains a small example on how to carry out host removal. It uses only a section of a human (host of our samples) reference genome assembly. In real life you should use the entire reference.

The first step is to copy over the reference fasta file we will use.

```{bash eval=FALSE}
cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/GRCh38_slice.fasta .
```

## Index reference
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/index.png", auto_pdf = TRUE)
``` 

We will will use the `Bowtie2` aligner for mapping/aligning. Prior to alignment/mapping we need to index our reference.

```{bash eval=FALSE}
bowtie2-build GRCh38_slice.fasta GRCh38_slice.fasta
```

If you use `ls` you will now see a bunch of files starting with `GRCh38_slice.fasta` and ending with various suffixes that contain `bt2`. These are the index files which allow us to use the reference with `Bowtie2`.

## Alignment
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/alignment.png", auto_pdf = TRUE)
``` 

With the indexed reference we will align the K1 reads to the reference. This creates a BAM file that contains alignment and read information (`K1_mapped.bam`).

```{bash eval=FALSE}
bowtie2 -x GRCh38_slice.fasta -1 K1_R1.fq.gz -2 K1_R2.fq.gz \
-p12 2> K1_bowtie2_out.log | samtools view -b -S -h > K1_mapped.bam
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_black.png", auto_pdf = TRUE)
```

This command is split into two commands. The first is `bowtie2` that creates the alignment. The parameters for the command are:

- __`-x`__: Indexed reference the reads will be aligned to.
- __`-1`__: The forward reads.
- __`-2`__: The reverse reads.
- __`-p`__: Number of threads to be used. 12 in this case.
- __`2>`__: This will cause the standard error to be redirected to the chosen file. In this case `out.log`.
  - This useful for commands that may produce a lot of output to screen. If an error occurs you can view this file to see the error messages.
  
The alignment is then piped (`|`) to the command `samtools view`. For more information on pipes please see our [Intro to Unix course book](http://www.cgr.liv.ac.uk/illum/NEOFworkshops_5bfa93ca0482d69d/Intro_to_Unix/12-Advanced_linux_practice.html#pipes).

The parameters for `samtools view` are:

- __`-b`__: Output the alignment as a BAM file.
  - BAM files are a binary form of SAM files so they are smaller in memory size.
  - If you are interested in the SAM format please see its [specification file](https://samtools.github.io/hts-specs/SAMv1.pdf).
- __`-S`__: Auto detect input format.
- __`-h`__: Include header.

The binary alignment is redirected to a new file called `K1_mapped.bam`. For more information on redirection please see our [Intro to Unix course book](http://www.cgr.liv.ac.uk/illum/NEOFworkshops_5bfa93ca0482d69d/Intro_to_Unix/12-Advanced_linux_practice.html#redirection).

## Unmapped read extraction
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/exctractor.png", auto_pdf = TRUE)
``` 

Next step is to extract the reads that did not map to the host reference from the `K1_mapped.bam` file with the `samtools fastq` command (unmapped reads).

```{bash eval=FALSE}
samtools fastq -f 4 -1 K1_R1.u.fastq -2 K1_R2.u.fastq K1_mapped.bam
```
 
#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_green.png", auto_pdf = TRUE)
```

- __`-f`__: Output reads that only include the SAM flag.
  - In this case `4` stands for unmapped reads. Therefore, our resulting fastq files will only contain unmapped reads. 
  - The following link is very useful to create a SAM flag you may need: https://broadinstitute.github.io/picard/explain-flags.html.
- __`-1`__: The output R1 fastq file of unmapped reads.
- __`-2`__: The output R2 fastq file of unmapped reads.

This step may make unmatched paired files (why we have `.u.` in the output file names). This occurs when a read from R2 is removed but the matching read in R1 is not removed, or vice versa. This will cause issues for further analysis. 

## Re-pair
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/repair.png", auto_pdf = TRUE)
``` 

The below `BBTools` command will re-pair the reads by removing reads with a missing pair. The command ensures the order of the reads are identical in the 2 output paired files.

```{bash eval=FALSE}
repair.sh in1=K1_R1.u.fastq in2=K1_R2.u.fastq \
out1=K1_R1.final.fastq out2=K1_R2.final.fastq \
outs=singletons.fastq
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_blue.png", auto_pdf = TRUE)
```

- __`in1=`__: The input R1 fastq file of unmapped unpaired reads.
- __`in2=`__: The input R2 fastq file of unmapped unpaired reads.
- __`out1=`__: The output R1 fastq file of unmapped paired reads.
- __`out2=`__: The output R2 fastq file of unmapped paired reads.
- __`outs=`__: The output fastq file containing the left over singletons (a sequence missing a pair).
  - This file can normally be ignored.

## Host removal summary
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/sum_blue.png", auto_pdf = TRUE)
```

We have run through quality control including host removal for our K1 sample. As our data has pretty much no human data we will skip this step for the other samples and use the trimmed data for the downstream analysis.

In a real analysis project you would use a whole genome reference for your host. However, that would have taken too long for this practical. The most current Human reference (when this was written) is GRCh38. We used a random 10kb section to align our reads to.

For more resources on the Human reference please see: https://www.ncbi.nlm.nih.gov/genome/guide/human/

The assembly we used was: https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz

<!--chapter:end:07-Host_removal.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# (PART\*) Taxonomy of reads {-}

# Taxonomic profiling {#taxprofile}
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/classification.png", auto_pdf = TRUE)
``` 

There are a number of methods for determining the species composition of a metagenomic dataset, but for the purposes of this practical we will use `Kraken2` & `Bracken` (Bayesian Reestimation of Abundance with KrakEN).  `Kraken2` classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. `Bracken` uses the taxonomic labels assigned by `Kraken2` to compute the abundance of species in a set of DNA sequences.

First, we'll make a new directory for it and move into it, after returning home:

```{bash eval=FALSE}
cd ..
mkdir 3-Taxonomy
cd 3-Taxonomy
```

<!--chapter:end:08-Taxonomic_profiling.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Kraken2
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/kraken.png", auto_pdf = TRUE)
``` 

Prior to running `Kraken2` we need to set a variable so `Kraken2` knows where to look for the databases it will use.

```{bash eval=FALSE}
export KRAKEN2_DB_PATH=/pub14/tea/nsc206/NEOF/Shotgun_metagenomics/kraken2_db
```

__Note__: You can look at the contents of the above directory to see it currently contains the MiniKraken database. This database contains only a subset of the bacteria, archaea, and viral `Kraken2` libraries. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full `Kraken2` database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. See the following links for info on installing the databases.

- Standard `Kraken2` databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database
- Custom `Kraken2` databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases

## Kraken2: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_blue.png", auto_pdf = TRUE)
``` 

Now, run `Kraken2` on sample K1 by running the following command.

__Note__: We are not using the host removed data. This is to save time. In your own analysis ensure you are using host removed data.

```{bash eval=FALSE}
kraken2 --paired --db minikraken2_v1_8GB \
--output K1.kraken --report K1.kreport2 \
~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz
```  

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_blue.png", auto_pdf = TRUE)
```

- __`--paired`__: Indicates that we are providing paired reads to `Kraken2`. Internally, `Kraken2` will concatenate the R1 and R2 reads into one sequence with an N between them.
- __`--db`__: Specify the `Kraken2` database to be used for taxonomic classification. 
  - Previous to the command we set the `KRAKEN_DB_PATH` so in this case the command will look for the directory called `minikraken2_v1_8GB` within `KRAKEN_DB_PATH`. 
  - Alternatively the full path of the required database could be provided (`/pub39/tea/nsc006/NEOF/Shotgun_metagenomics/kraken2_db/minikraken2_v1_8GB`).
- __`--threads`__: Number of CPUs the process will use.
- __`--output`__: The output file. More info below.
- __`--report`__: The output report file. More info below.
- __`~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz`__: The trimmed read pairs for K1, which we will use as input.

## Kraken2: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_blue.png", auto_pdf = TRUE)
``` 

There are two major output formats from Kraken2:

- `--output`, `.kraken`: Each sequence (or sequence pair, in the case of paired reads) classified by `Kraken2` results in a single line of output. `Kraken2`'s output lines contain five tab-delimited fields; from left to right, they are:
   1. "C"/"U": a one letter code indicating that the sequence was either classified or unclassified.
   2. The sequence ID, obtained from the FASTA/FASTQ header.
   3. The taxonomy ID `Kraken2` used to label the sequence; this is 0 if the sequence is unclassified.
   4. The length of the sequence in bp. In the case of paired read data, this will be a string containing the lengths of the two sequences in bp, separated by a pipe character, e.g. "98|94".
   5. A space-delimited list indicating the LCA mapping of each k-mer in the sequence(s). For example, "562:13 561:4 A:31 0:1 562:3" would indicate that:
      - the first 13 k-mers mapped to taxonomy ID #562
      - the next 4 k-mers mapped to taxonomy ID #561
      - the next 31 k-mers contained an ambiguous nucleotide
      - the next k-mer was not in the database
      - the last 3 k-mers mapped to taxonomy ID #562
      - __Note__: that paired read data will contain a "|:|" token in this list to indicate the end of one read and the beginning of another.

- `--report`, `.kreport2`: The report output format. This is required for bracken. It is tab-delimited with one line per taxon. The fields of the output, from left-to-right, are as follows:
   1. Percentage of paired reads covered by the clade rooted at this taxon.
   2. Number of paired reads covered by the clade rooted at this taxon.
   3. Number of paired reads assigned directly to this taxon.
   4. A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., "G2" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank.
   5. NCBI taxonomic ID number
   6. Indented scientific name

The output to screen will show how many sequences are classified. This will be lower than normal as we are using a mini `Kraken2` database.

In a real analysis you may use the option `--confidence` which represents  the __"Confidence score threshold"__. The default is `0.0`, which is the lowest, with the maximum value being `1`. A good place to start may be `0.1`. Too many classifications are removed if you attempt it with this dataset, due to the mini `Kraken2` database used. More info on the confidence scoring can be found at: https://github.com/DerrickWood/kraken2/wiki/Manual#confidence-scoring

__Task__: Once the Kraken2 command has finished running, run it on the other two samples. Attempt the commands without looking at the help box.

__Hint__: You will need to change all instances of K1 to K2 or W1 in the above command

`r hide("K2 & W1 Kraken 2 commands")`
```{bash eval=FALSE}
#K2
kraken2 --paired --db minikraken2_v1_8GB \
--output K2.kraken --report K2.kreport2 \
~/2-Trimmed/K2_R1.fq.gz ~/2-Trimmed/K2_R2.fq.gz
#W1
kraken2 --paired --db minikraken2_v1_8GB \
--output W1.kraken --report W1.kreport2 \
~/2-Trimmed/W1_R1.fq.gz ~/2-Trimmed/W1_R2.fq.gz
```
`r unhide()`

## Kraken 2: MCQs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/question_bubble_blue.png", auto_pdf = TRUE)
```

Viewing the Kraken2 output files with your favourite text viewer (`less`, `nano`, `vim`, etc.), attempt the below MCQs.

```{r, echo = FALSE}
opts_p <- c("__2__", "__486,723__", answer="__555,162__")
```
1. How many paired reads were unclassified for K1? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__2__", answer="__486,723__", "__555,162__")
```
2. How many paired reads were classified for K2 (i.e. number of reads classified at root level and below)? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__2__", "__486,723__", "__555,162__")
```
3. How many paired reads were assigned directly to root level for W1? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__0.12__", "__0.59__", answer="__14.89__")
```
4. What percentage of W1's paired reads were assigned to the clade of Bacteroidetes (Phylum)? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__0.12__", answer="__0.59__", "__14.89__")
```
5. What percentage of K2's paired reads were assigned to the clade of Rikenellaceae (Family)? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__0.12__", "__0.59__", "__14.89__")
```
6. What percentage of K1's paired reads were assigned to the clade of Bacteroides helcogenes (Species)? `r longmcq(opts_p)`

<!--chapter:end:09-Kraken2.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Krona plot
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/krona.png", auto_pdf = TRUE)
``` 

`Krona` is an interactive metagenome species abundance visualisation tool. 

## Krona: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_red.png", auto_pdf = TRUE)
``` 

We can use the `Kraken2` report files to create our Krona plots. With the below command we can import our `Kraken2` taxonomy (within the report file) into a `Krona` html.

```{bash eval=FALSE}
ktImportTaxonomy -o kraken2.krona.html *.kreport2
```

`-o` is our output html file, and the final argument `*.kreport2` represents all of our `.kreport2` files in the current directory. The `*` is a wild-card, meaning any characters any number of times. Therefore `*.kreport2` identifies the files `K1.kreport2` `K2.kreport2` and `W1.kreport2`.

You will get a warning that not all taxonomy IDs were found. We will ignore this but in your own future installations this should be addressed with `Krona`'s `updateTaxonomy.sh` command.

## Krona: visualise
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/eye_red.png", auto_pdf = TRUE)
``` 

We can view our interactive chart in a web browser. This shows the percentage of reads that were classified to various taxonomies at different levels.

```{bash eval=FALSE}
firefox kraken2.krona.html &
```

This is an interactive multi-tiered pie chart with many options. Some of the interactivity is described below:

- To choose a sample to view, click on the sample name in the top left.
- To zoom into a lower taxonomy, double click on the taxonomy's name on the pie chart.
  - E.g. double click the word "Bacteria" on the pie chart to zoom into Bacteria and therefore ignore Eukaryota, and "[other Root]".
- To zoom out to a higher taxonomy, click on the taxonomy's name in the middle of the pie chart.
  - E.g. click the word "root" in the middle of the pie chart to zoom back out from any lower level of taxonomy.
- To view percentage stats of a taxonomy, click on the taxonomy name on the pie chart. 
  - This will display the percentage this taxonomy covers of all the different taxonomies above it that it belongs to. This is displayed on the right side of the screen.
  - E.g. Click on Proteobacteria to see the percentage of the Root it accounts for and to see the percentage of Bacteria it accounts for in the sample.

## Krona: MCQs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/question_bubble_red.png", auto_pdf = TRUE)
```

Viewing the Krona, attempt the below MCQs.

```{r, echo = FALSE}
opts_p <- c("__2%__", "__34%__", answer="__35%__")
```
1. What percentage of the __root__ was classified to __Bacteria__ for __K1__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__2%__",answer="__34%__", "__35%__")
```
2. What percentage of __bacteria__ was classified to __Pseudomonadota__ for __K2__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__2%__","__34%__", "__35%__")
```
3. What percentage of the __root__ was classified to __Myxococcota__ for __W1__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__39%__", "__8%__", "__6%__")
```
4. What percentage of the __Terrabacteria groups__ was classified to __Cellulomonas gilvus__ for __K2__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__39%__", "__8%__", answer="__6%__")
```
5. What percentage of __Alphaproteobacteria__ was classified to __Phenylobacterium immobile__ for __K1__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__39%__", answer="__8%__", "__6%__")
```
6. What percentage of __Viridiplantae__ (in Eukaryota) was classified to __Parasponia__ for __W1__?  `r longmcq(opts_p)`

<!--chapter:end:10-Krona.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Bracken
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/bracken.png", auto_pdf = TRUE)
```

`Bracken` (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by `Kraken2` to compute estimated abundances of species in a metagenomic sample. 

### Bracken: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_green.png", auto_pdf = TRUE)
``` 

Just like with `Krona` we can use the `Kraken2` report files to run bracken.

```{bash eval=FALSE}
bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \
-i K1.kreport2 -o K1.bracken -w K1.breport2 -r 100 -l S -t 5
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_green.png", auto_pdf = TRUE)
```

- `-d` : Specifies the `Kraken2` database that was used for taxonomic classification. In this case bracken requires the variable `$KRAKEN_DB_PATH` so the option is provided the full path to the kraken database. For clarity try the command `ls $KRAKEN2_DB_PATH/minikraken2_v1_8GB`. 
- `-i` : The `Kraken2` report file, this will be used as the input.
- `-o` : The output `Bracken` file. Information about its contents is below.
- `-w`: Output report file. This contains the `Bracken` read counts in a kraken-style report. This is an essential file if you want to use the `Bracken` output in R using the `phyloseq` object. This is covered in our R community analysis workshop. We won't cover it more here.
- `-r 100`: This is the ideal length of the reads that were used in the `Kraken2` classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as we used a paired library of 100bp*2 reads.
- `-l S`: This specifies the taxonomic level/rank of the `Bracken` output. In this case `S` is equal to species with the other options being `D`, `P`, `C`, `O`,`F` and `G`.
- `-t 5`: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10).

## Bracken: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_green.png", auto_pdf = TRUE)
``` 

The output file of `Bracken` contains the following columns:

1. __Name__: Name of taxonomy at the specified taxonomic level.
2. __Taxonomy ID__: NCBI taxonomy id
3. __Level ID__: Letter signifying the taxonomic level of the classification
4. __Kraken assigned read__: Number of reads assigned to the taxonomy by `Kraken2`.
5. __Added reads with abundance reestimation__: Number of reads added to the taxonomy by Bracken abundance reestimation. 
6. __Total reads after abundance reestimation__: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis.
7. __Fraction of total reads__: Relative abundance of the taxonomy.

__Task__: Repeat the above commands for K2 and W1

`r hide("K2 & W1 Bracken commands")`
```{bash eval=FALSE}
#K2
bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \
-i K2.kreport2 -o K2.bracken -r 100 -l S -t 5
#W1
bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \
-i W1.kreport2 -o W1.bracken -r 100 -l S -t 5
```
`r unhide()`

## Bracken: MCQs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/question_bubble_green.png", auto_pdf = TRUE)
```

Viewing the Bracken output files (`.bracken`) with your favourite text viewer (`less`, `nano`, `vim`, etc.), attempt the below MCQs.

```{r, echo = FALSE}
opts_p <- c("__0.00011__",answer="__16__", "__702__")
```
1. In __K1__, how many total reads after abundance reestimation are there for __Prevotella fusca__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__0.00011__","__16__",answer="__702__")
```
2. In __K2__, how many reads after abundance reestimation were added for __Bacteroides caccae__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__0.00011__","__16__", "__702__")
```
3. In __W1__, what is the fraction of total reads (after abundance reestimation) for __Tannerella forsythia__? `r longmcq(opts_p)`

## Bracken: merging output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/merge.png", auto_pdf = TRUE)
``` 

To make full use of `Bracken` output, it is best to merge the output into one table. Before we do this we’ll copy the `Bracken` output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples.

```{bash eval=FALSE}
cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/bracken/* .
```

Now to merge all the K and W `Bracken` files.

```{bash eval=FALSE}
combine_bracken_outputs.py --files [KW]*.bracken -o all.bracken
```

This output file contains the first three columns:

- __name__ = Organism group name. This will be based on the TAX_LVL chosen in the `Bracken` command and will only show the one level.
- __taxonomy_id__ = Taxonomy id number.
- __taxonomy_lvl__ = A single string indicating the taxonomy level of the group. ('D','P','C','O','F','G','S').

After these columns are the following two columns for each sample.

- `${SampleName}.bracken_num`: The number of reads after abundance reestimation 
- `${SampleName}.bracken_frac`: Relative abundance of the group in the sample

## Bracken: extracting output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/exctractor.png", auto_pdf = TRUE)
``` 

We want a file with only the first column (organism name) and the `bracken_num` columns for each sample. To carry this out we  first create a sequence of numbers that will match the `bracken_num` column numbers. These start at column 4 and are every even numbered column after this. We will use `seq` to create a sequence of numbers starting at `4` and including every second (`2`) number up to and including `50` with commas (`,`) as separators (`-s`). 

__Note__: The number 50 is chosen as 3 (first three info columns) + 24*2 (24 samples with 2 columns each) = 50.

```{bash eval=FALSE}
#Try out the seq command to see its output
seq -s , 4 2 50
#Create variable
bracken_num_columns=$(seq -s , 4 2 50)
echo $bracken_num_columns
```

Now to use the variable to extract the `bracken_num` columns plus the first column (species names).

```{bash eval=FALSE}
cat all.bracken | cut -f 1,$bracken_num_columns > all_num.bracken
```

<!--chapter:end:11-Bracken.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# LEfSe biomarker detection
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/lefse.png", auto_pdf = TRUE)
```

We will use `LEfSe` (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. `LEfSe` couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions.

In essence it allows for the detection of biomarkers when comparing sample groups. In the `LEfSe` terminology the sample groups are called the class.

## LEfSe: add metadata {#kraken2_lefse_metadata}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/header.png", auto_pdf = TRUE)
```

We need to add metadata to our `Bracken` file to be ready for `LEfSe.` First we will copy the file so we have a backup in case we do anything wrong.

```{bash eval=FALSE}
cp all_num.bracken all_num.lefse.bracken
```

Using your favourite text editor (e.g. `nano`, `vim`, etc.) add the following line to the top of your `all_num.lefse.bracken` file. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator.

__diet	K	K	K	K	K	K	K	K	K	K	K	K	W	W	W	W	W	W	W	W	W	W	W	W__

__Note__: The above is __diet__ followed by 12 __K__ and 12 __W__.

The singular line should match the order of your samples within the file. This is the metadata line that `LEfSe` will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples.

`r hide("Issues with creating file?")`
If you are having issues with creating and editing the file `all_num.lefse.bracken` you can copy a pre-made version.
```{bash, eval=FALSE}
cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/lefse/all_num.lefse.bracken .
```
`r unhide()`

## LEfSe: format
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/lefse_format.png", auto_pdf = TRUE)
```

We need to further format and preprocess our file with a `LEfSe` script.

```{bash eval=FALSE}
lefse_format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_purple.png", auto_pdf = TRUE)
```

- `all_num.lefse.bracken` : Input `Bracken` file.
- `all_num.lefse` : Output file formatted for the run_lefse command, which we will soon run.
- `-c 1` : Specifies the row with the class info. This is used to determine which samples will be compared against which samples. In this case it is the first row with the Ks and Ws.
- `-u 2` : Specifies the row with the sample names. This is the second row in this case.
- `-o 1000000` : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. `LEfSe` developers recommend 1000000 (1 million) when very low values a present. We generally always use 1 million for consistency.

## LEfSe: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_purple.png", auto_pdf = TRUE)
``` 

Now to run `LEfSe.` All we need to do is run the command with the formatted input and provide an output file name.

```{bash eval=FALSE}
lefse_run.py all_num.lefse all_num.lefse.out
```

#### Output {-}

The output file is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a "-" .

- __Biomarker name__
- __Log of highest class average__: I.e. get the class with the greater amounts of the biomarker, average the counts and then get the log of this value.
- __Class with the greater amounts of biomarker__
- __LDA effect size__: A statistical figure for `LEfSe`..
- __p-value__: Biomarkers must have a p-value of <0.05 to be considered significant.

The __LDA effect__ size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes.

## LEfSe: visualisation
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/bar_chart_horizontal.png", auto_pdf = TRUE)
``` 

Next we can visualise the output.

```{bash eval=FALSE}
lefse_plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png
```

- `--dpi 200` : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. We’ve chosen 200 as it is good quality and we will not be publishing these results.
- `--format png` : Format of output file. png is a commonly used file format for images.
- `all_num.lefse.out` : `LEfSe` output to visualise.
- `biomarkers.png` : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance.

Look at the figure with `firefox`:

```{bash eval=FALSE}
firefox biomarkers.png
```

## LEfSe: MCQs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/question_bubble_purple.png", auto_pdf = TRUE)
```

Interrogating the biomarkers.png plot and the all_num.lefse.out file, attempt the below MCQs.

__Note__: In this instance green bars represent biomarkers in higher abundance in the W samples whilst the red bars represent biomarkers in higher abundance in the K samples.

```{r, echo = FALSE}
opts_p <- c("__Adlercreutziaequolifaciens__",answer="__Alistipesshahhii__","__Methanosphaerastadtmanae__")
```
1. Which species biomarker causes the biggest effect in the __W__ class? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__Adlercreutziaequolifaciens__","__Alistipesshahhii__",answer="__Methanosphaerastadtmanae__")
```
2. Which species biomarker causes the biggest effect in the __K__ class? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__Adlercreutziaequolifaciens__","__Alistipesshahhii__","__Methanosphaerastadtmanae__")
```
3. Which species biomarker (i.e. present in the plot) causes the lowest effect in the __W__ class? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__Korean__",answer="__Western__")
```
4. Which class has more biomarkers associated with it? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__0.0014961642897455565__","__3.0272759627443127__","__3.3145045719482846__")
```
5. What is the LEfSe p-value for __Campylobactercoli__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__0.0014961642897455565__",answer="__3.0272759627443127__","__3.3145045719482846__")
```
6. What is the Log of highest class average for __Streptococcussuis__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__0.0014961642897455565__","__3.0272759627443127__",answer="__3.3145045719482846__")
```
7. What is the LDA effect size for __Bifidobacteriumlongum__? `r longmcq(opts_p)`

## Kraken2 and Bracken databases
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/database.png", auto_pdf = TRUE)
``` 

In your own future analysis you will need to create your own `Kraken2` and `Bracken` databases. Please sse the following links on information for this:

- `Kraken2`
   - Standard `Kraken2` databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database
   - Custom `Kraken2` databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases
- `Bracken`
   - https://ccb.jhu.edu/software/bracken/index.shtml?t=manual#step1
   - This requires a `Kraken2` database to be built first.

<!--chapter:end:12-Lefse.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# (PART\*) Functional profiling of reads {-}

# HUMAnN Functional profiling {#functprofile}
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/profile.png", auto_pdf = TRUE)
``` 

It is possible to investigate functional differences between metagenome (and metatranscriptome) samples by directly interrogating the read data. We will look at how this can be done with a package called `HUMAnN` (The HMP Unified Metabolic Analysis Network), a [bioBakery](https://github.com/biobakery/biobakery) pipeline designed to accurately profile the presence/absence and abundance of microbial pathways in metagenomic sequencing data.

`HUMAnN` is on its third version and was developed in tandem with the third version of `MetaPhlAn`, a computational tool for profiling the composition of microbial communities from metagenomic data.

It is highly recommended you use the new version of `HUMAnN` as it contains 2 times more species pangenomes and 3 times more gene families than `HUMAnN2`.

Please see below for a diagram showing the pipline of `HUMAnN`:

```{r, fig.align = 'center',out.width= '75%', echo=FALSE }
knitr::include_graphics(path = "figures/humann3_workflow.png", auto_pdf = TRUE)
``` 


## HUMAnN 
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/humann2.png", auto_pdf = TRUE)
``` 

First, we will carry out an example run of the software and briefly explore the output files. `HUMAnN` can take a long time to run so we will use a small amount of example data. Additionally, we will use a subset of the `HUMAnN` databases for the analysis but when running analysis on your own data you should use the full databases. information on installing `HUMAnN` and its databases can be found on its online [Home Page](https://huttenhower.sph.harvard.edu/humann/).

### HUMAnN: mamba, directories, and files
```{r, fig.align = 'center',out.width= '25%', echo=FALSE }
knitr::include_graphics(path = "figures/mamba_logo.png", auto_pdf = TRUE)
```

We need a new `mamba` environment. Open a new terminal (right click on the main screen background, choose `Terminal`) and run the below:

```{bash eval=FALSE}
. usebiobakery3
```

Make a new directory and move into it.

```{bash eval=FALSE}
mkdir ~/4-FunctionalProfiling
cd ~/4-FunctionalProfiling
```

Copy over some test data we will carry out the analysis on. This is a demonstration FASTQ file that we will use. It will be small enough to run `HUMAnN` in a reasonable amount of time.

```{bash eval=FALSE}
cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/humann/demo.fastq.gz .
```

### HUMAnN: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_orange.png", auto_pdf = TRUE)
``` 

Run the `HUMAnN` pipeline with our demo data:

```{bash eval=FALSE}
humann \
--input demo.fastq.gz \
--output demo.humann \ 
--threads 10
```

Here, we are telling the software to use `demo.fastq.gz` as input and to create a new output directory called `demo.humann` where the results will be generated. 

As the software runs you might notice that `HUMAnN` runs `MetaPhlAn`. The purpose of this is to identify what species are present in the sample, so `HUMAnN` can tailor generate an appropriate database of genes (from those species) to map against. It will carry out this alignment against the gene database, then a protein database, and finally compute which gene families are present. `HUMAnN` will determine which functional pathways are present and how abundant they are.

If you are using paired end reads the `HUMAnN` developers recommend concatenating your reads into one file and running `HUMAnN` on the concatenated file [source](https://forum.biobakery.org/t/humann3-paired-end-reads/862/3).

For example (don't run the below):

```{bash, eval=FALSE}
cat K1_R1.fq.gz K1_R2.fq.gz > K1.fq.gz
humann --input K1.fq.gz --output humann_output --threads
```

### HUMAnN: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_orange.png", auto_pdf = TRUE)
``` 

Once the run has completed, change into the newly created output directory and list the files that are present.

```{bash eval=FALSE}
cd demo.humann
ls
```

You will see that there are three files and one directory. The directory (`demo_humann2_temp`) contains intermediate temporary files and can be disregarded here.

The three output files are:

- __`demo_genefamilies.tsv`__: A table file showing the number of reads mapping to each UniRef90 gene family. Values are normalised by the length of each gene family (i.e. RPK, or Reads per Kilobase). Additionally, the values are stratified so that they show the overall community abundance but also a breakdown of abundance per species detected. This allows researchers to delve into species specific functions, rather than only looking at the metagenomic functions as a whole,
- __`demo_pathabundance.tsv`__: A table file showing the normalised abundance of MetaCyc pathways (RPKs). These abundances are calculated based on the UniRef90 gene family mapping data and are also stratified by species.
- __`demo_pathcoverage.tsv`__: A table file that shows the coverage, or completeness, of pathways. For example, a pathway may contain 5 components (or genes/proteins)
   - Pathway1 :         A → B → C → D → E		100% complete
   - A species identified in the sample may only have four of the components, meaning that the pathway is only 80% complete (represented as 0.8)
    - Pathway1 :         A → B → C → ~~D~~ → E		80% complete

The basic format of these three output files is the same, so let's take a look at the pathway abundance table.

```{bash eval=FALSE}
less demo_pathabundance.tsv
```

You will see that there are two columns:

1. The first column shows the pathways.
   - __UNMAPPED__ indicates reads that could not be aligned.
   - __UNINTEGRATED__ indicates reads that aligned to targets not implicated in any pathways.
2. The second column shows the abundance.

This file is not too interesting to look at as it is only demo data. Therefore, press `q` to exit `less` and let's look at some real data.

__Note__: The directory `demo_humann2_temp` can be very large and so should be deleted in real projects once you are certain they are not needed. However, these files can be useful for debugging.

__Note__: Link to more detail on [Output files](https://github.com/biobakery/humann#output-files)

## Multi sample processing {#humannmultisampleprocessing}
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/k_w_venn.png", auto_pdf = TRUE)
``` 

Looking at the functional profile of one sample in isolation is usually not very informative. First, there is nothing to compare it to and second, there are no biological replicates. We will therefore use all the Korean and Western diet samples.

It would take many hours to analyse all of the data using `HUMAnN` and is outside the scope of this course. For this reason, samples were analysed prior to the workshop to generate the output files we covered above. 

For the purposes of this comparison, we will look at the pathway abundances only. First copy over the data directory containg the gene families tables and have a look in it.

```{bash eval=FALSE}
#Ensure you are in the correct directory
cd ~/4-FunctionalProfiling
#Copy directory with pre made results
cp –r /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/DietPathAbundance .
#Move into the copied directory
cd DietPathAbundance
#List files
ls
```

You will see there are 12 files prefixed with K and 12 prefixed with W, for the Korean diet and Western diet samples, respectively. Take a look at the file for K1.

```{bash eval=FALSE}
less K1_pathabundance.tsv
```

There are a lot of pathways in the file. Quit out of the `less` viewer (`q`) and look at the entries for one specific pathway, COA-PWY-1 (a coenzyme A biosynthesis II pathway).

```{bash eval=FALSE}
grep COA-PWY-1 K1_pathabundance.tsv | less
```

This shows 30 entries/lines with the top entry/line:

**`COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals)        6790.1517478104`**

This shows the abundance of the pathway across the entire sample (6790.1517478104).

The other entries show the species stratification information (mentioned above) of the pathway. I.e. the second line:

**`COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals)|g__Bacteroides.s__Bacteroides_dorei    1292.7711872228`**

shows the abundance of the pathway that is contributed by the species _Bacteroides dorei_ (1292.7711872228).

__Note__: The species stratified pathway abundances may not equal the total community pathway abundance. Please see  this [forum post](https://forum.biobakery.org/t/humann3-pathway-abundance-table-pathway-sum-and-species-sum-different/1471) for details.

With this information we will carry out some comparisons including biomarker detection to determine which pathways are differentially abundant between the Western diet and Korean diet samples.

__Note__: The following methods/pipeline can be used for the genefamilies and pathcoverage tables in your own future analyses.

### HUMAnN: combining data
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/merge.png", auto_pdf = TRUE)
``` 

First, we need to combine these 24 tables into one large results table. `HUMAnN` provides a tool to do this:

```{bash eval=FALSE}
#Change directory to main Functional profiling directory
cd ~/4-FunctionalProfiling
#Join the tables
humann_join_tables --input DietPathAbundance/ --output diet.tsv
```

This command will look for all tables in the `DietPathAbundance` directory and generate a large, 25 column table called `diet.tsv`. You can inspect the file to ensure that this has worked correctly.

```{bash eval=FALSE}
less -S diet.tsv
```

### HUMAnN: split stratified table
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/split.png", auto_pdf = TRUE)
``` 

For this tutorial we do not want the species stratification information. We will therefore split the table to create 2 new files:

- `diet_unstratified.tsv`: This table only contains the total abundance values for the pathways. It does not contain any species stratification information.
- `diet_stratified.tsv`: This table only contains the species stratification abundance values for the pathways. It does not contain the total abundance information.

To create the split files and output them to your current directory, run the following command:

```{bash eval=FALSE}
humann_split_stratified_table --input diet.tsv --output .
```

We will use the file `diet_unstratified.tsv` for our downstream analysis. 

Before you move on feel free to inspect the output files with the `less` command.

__Note__: You can use any of the three tables (unsplit table, unstratified table, or stratified table) in your own analysis. This depends on your question and data.

### HUMAnN: renormalising data {#humannrenormalise}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/normalisation.png", auto_pdf = TRUE)
``` 

The next step is to renormalise the data. Currently, all of the abundance values are only normalised within each sample (RPKs). However, they are not normalised between samples, and this is very important to do. For example, if we had sequenced two samples, A and B, and we obtained 5 million reads for sample A and 20 million reads for sample B, without normalisation, it might look that there was up to 4x as much functional activity in sample B! 

To correct for this, we normalise the abundance values based on the number of reads in each sample. We will normalise to relative abundance (`--units relab`) where all abundances for each sample add up to 1.

Renormalisation command:

```{bash eval=FALSE}
humann_renorm_table \
--units relab \
--input diet_unstratified.tsv \
--special n \
--output diet_unstratified.relab.tsv
```

This command generates the normalised data in the new table `diet_unstratified.relab.tsv`. The `--special n` option tells the script to remove all unmapped and unassigned values (__UNMAPPED__ & __UNINTEGRATED__) from the table.

__Note:__ With the gene families information ensure you normalise by CPM (counts per million) with the option `--units cpm`. More info can be found on the [Normalizing RPKs to relative abundance](https://github.com/biobakery/biobakery/wiki/humann3#31-normalizing-rpks-to-relative-abundance) section of the HUMAnN 3.0 tutorial.

## Heatmap
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/heatmap.png", auto_pdf = TRUE)
``` 

Now that we have our combined, unstratified, and normalised table, we can visualise the dataset to see how the two groups compare. 

- Do samples in the same diet group appear to correlate well with each other? 
- Are samples from one diet group distinguishable from those from the other diet group? 

To visualise this we will create a heatmap with `hclust2`.

Before carrying out the command we will need edit the file. Carry out the following alterations:

Remove the `_Abundance` part of the sample names whilst creating a copy that we will use (It is always a good idea to keep the original file in case a mistake happens).

```{bash eval=FALSE}
cat diet_unstratified.relab.tsv | sed "s/_Abundance//g" > diet_unstratified.relab.comp.tsv
```

__Intro to unix links__: 

- [Text editing with __`sed`__](http://www.cgr.liv.ac.uk/illum/NEOFworkshops_5bfa93ca0482d69d/Intro_to_Unix/12-Advanced_linux_practice.html#sed)
- [Redirection with __`>`__](http://www.cgr.liv.ac.uk/illum/NEOFworkshops_5bfa93ca0482d69d/Intro_to_Unix/12-Advanced_linux_practice.html#redirection)

Next using your text editor of choice carry out the following changes on the file `diet_unstratified.relab.comp.tsv`.

- Remove the `# ` (including the one space after the `#`) from the start of the header so it starts as `Pathway`.
- Add in the same metadata line as we did for [8.4.1](#kraken2_lefse_metadata) but this time below the header line, i.e. as the 2nd line (ensure you are using tabs instead of spaces).

`r hide("Issues with creating file?")`
If you are having issues with creating and editing the file `all_num.lefse.bracken` you can copy a pre-made version.
```{bash, eval=FALSE}
cp /pub14/tea/nsc206/Shotgun_metagenomics/lefse/diet_unstratified.relab.comp.tsv
```
`r unhide()`

Now we can use the `hclust2` tool to create a heatmap of our pathway abundances.

```{bash eval=FALSE}
hclust2.py \
-i diet_unstratified.relab.comp.tsv \
-o diet_unstratified.relab.heatmap.png \
--ftop 40 \
--metadata_rows 1 \
--dpi 300
```

__Note:__ You will get 2 `MatplotlibDeprecationWarnings`, these are normal and can be ignored. However, ensure these ar ethe only warnings/errors before continuing.

#### Parameters {-}

- `-i`: The input table file.
- `-o`: The output image file. The tool does not specify what types of image files you can use but `.png` is always a good image file format.
- `--ftop`: Specifies how many of the top features (pathways in this case) to be included in the heatmap. 
- `--metadata_rows`: Specifies which row/s contain the metadata information to be used for the group colouring at the top of the heatmap.
   - Row numbers start at 0 for this tool. Therefore our sample names are in row 0 and the diet info is in row 1.
   - Multiple rows can be specified if you have multiple rows of metadata.
      - e.g. `--metadata_rows 1,2,3`.
- `--dpi`: The image resolution in dpi (dots per inch). 300 dpi is used for publication quality images.

There are many more options that can be seen on the [hclust2 github](https://github.com/SegataLab/hclust2).

#### Visualise {-}

Now we can view the plot.

```{bash eval=FALSE}
firefox diet_unstratified.relab.heatmap.png
```

From this, we can see that there is a small amount of clustering caused by the differences between the Korean and Western diet. Other factors that we do not know about the samples must also come into play. This is normal as we cannot account for everything but it is good to try to account for as much as possible.

```{r, fig.align = 'center',out.width= '75%', echo=FALSE }
knitr::include_graphics(path = "figures/humann_heatmap.png", auto_pdf = TRUE)
```

#### MCQs {-}

```{r, echo = FALSE}
opts_p <- c("__ARO-PWY__",answer="__DTDPRHAMSYN__","__PWY-6385__")
```
1. Which pathway stands out the most? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__1__",answer="__2__","__3__")
```
2. How many clusters are formed based on diet (Colours on tree at top of heatmap)? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__1__","__2__",answer="__3__")
```
3. How many clusters are formed based on pathways (Colours on tree at the side of heatmap)? `r longmcq(opts_p)`

You can look up the pathway names in the table file to see a fuller description.

## LEfSe {#humann_lefse}
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/lefse_flip.png", auto_pdf = TRUE)
``` 

For the final part of this section, we will see if there are any statistically significant differences between the two sample groups. There are several ways in which this can be achieved but we will carry out `LEfSe` again.

__Task__: Go back to your `LEfSe` terminal (or create a new one and use `. uselefse`). Then change directory to `4-FunctionalProfiling`)

Thankfully we already formatted the file to work with `LEfSe` when we formatted it for `hclust2`

```{bash eval=FALSE}
#LEfSe format
lefse-format_input.py \
diet_unstratified.relab.comp.tsv \
diet_unstratified.relab.comp.lefse \
-c 2 -u 1 -o 1000000
#Run LEfSe
run_lefse.py \
diet_unstratified.relab.comp.lefse \
diet_unstratified.relab.comp.lefse.out
#Produce LEfSe plot
lefse-plot_res.py \
--dpi 200 \
--format png \
diet_unstratified.relab.comp.lefse.out \
diet_unstratified.relab.comp.lefse.png
#View plot
firefox diet_unstratified.relab.comp.lefse.png
```

Look at the output and see what pathways count as biomarkers for the 2 groups.

## Optional task

Carry out all the steps starting from [Multi sample processing](#humannmultisampleprocessing) with the gene families information.

- Copy the gene families data from `/pub14/tea/nsc206/NEOF/Shotgun_metagenomics/DietGeneFamilies`
- Ensure you set the `--units` option to `cpm` in the [renormalising data](#humannrenormalise) step.
- On top of analysing the unstratified data you can also analyse the stratified data.

That completes the non assembly approach to shotgun metagenomic analysis. The next chapters will cover an assembly approach.

<!--chapter:end:13-Functional_profiling.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# (PART\*) Assembly Approach {-}

# Metagenome assembly {#metagenomeassembly}
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/jigsaw.png", auto_pdf = TRUE)
``` 

So far we have directly analysed the read data itself which is perfectly fine for taxonomic profiling and for certain methods of functional profiling. However, Illumina reads are generally short and therefore can not provide us with much data on larger constructs that are in the metagenomic samples, e.g. genes. While it is possible to predict from which gene a sequence read might originate, the short nature of the query can sometimes lead to ambiguous results. 

Additionally, depending on the application it can become computationally intensive to analyse large numbers of reads. Here, we are only using samples with 1 million reads. Some metagenome samples consist of 50-100 million+ read pairs. If such a sample belonged to a set of 100 samples, that would be up to 10 billion read pairs, or 2 trillion bases of sequence data, with many of these being redundant.

For this reason, it is sometimes advantageous to assemble the reads into contigs, using a meta-genome assembler. This has the dual effect of:

- Reducing the overall size of the data for analysis. If a metagenome was sequenced at 50x depth, then by assembling it you could theoretically reduce the amount of sequence to analyse by 50-fold.
- Increase the size of the fragments you will analyse. This is the main advantage of an assembly, as the ~100 bp reads can be pieced together to form 100,000 kb+ contigs. These contigs will contain complete genes, operons and regulatory elements: Reconstructed genome sections.

Here, we will carry out a couple of assemblies on our dataset.

## Metagenome assembly: Mamba
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/mamba_logo.png", auto_pdf = TRUE)
```

We will use the `shotgun_meta` conda environment so use a terminal where this is activated or open a new one and run `. useshotgun`.

## A primer on short read assembly
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/dna_laptop.png", auto_pdf = TRUE)
``` 

Illumina reads are too short and numerous to use traditional overlap-layout-consensus assemblers as such an approach would be far too computationally intensive. Instead, we use De Bruijn graph based assemblers. Briefly, these operate as follows:

1. All reads are broken down into k-length overlapping fragments (k-mers). e.g. if we choose a k-mer size of 5 bp, the following two sequences (blue) would be broken down into the k-mers below them (red):

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/reads_aligned.png", auto_pdf = TRUE)
``` 

2. All k-mers are linked to other k-mers which match with a k-1 length overlap (i.e. that overlap by all but one base):

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/graph_simple.png", auto_pdf = TRUE)
``` 

3. Paths are routed through the graph and longer contigs are generated:

```{r, fig.align = 'center',out.width= '80%', echo=FALSE }
knitr::include_graphics(path = "figures/graph_complex.png", auto_pdf = TRUE)
``` 

The example here is a vast oversimplification of the complexity of a De Bruijn graph (i.e. there are no branches!). Routing through the graph is never as simple as this as some k-mers will lead to multiple k-mers, which can result in the break point of a contig. This is especially true for complex metagenomic data.

Generally speaking, the shorter the k-mer, the more branches there will be, the trickier the graph is to resolve, so the resulting contigs are smaller. Assemblers usually perform better with longer k-mer lengths but even then there might not be enough depth of sequencing to generate all k-mers that form overlaps, therefore leading to break points. Finding the right k-mer size usually involves testing several. 

Fortunately, the assembler we will use, `MEGAHIT`, allows us to build an assembly using multiple k-mer lengths iteratively. The other great advantage about `MEGAHIT` is that it is quick and efficient.  We will use `MEGAHIT` on our data soon, but first there is an additional processing step for our sequences...

<!--chapter:end:14-Metagenome_assembly.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Stitching read pairs
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/stitch.png", auto_pdf = TRUE)
``` 

Longer k-mers generally perform better for assemblies. However, our maximum read length is 100 bp so we are limited to a maximum k-mer length of 99 bp. Thankfully we can get even longer k-mers if we stitch our read pairs together.

__Note__: This method will not work if your reads have no overlap. If you are not sure if your reads have overlap ask the team who sequenced them.

A read pair consists of two sequences read from each end of a fragment of DNA (or RNA). If the two sequences meet and overlap in the middle of the fragment, there will be a region of homology. We can use this to merge the two reads together (See next image).

First, we obtain our forward and reverse reads, derived from different ends of the same fragment. Second, we look for sufficient overlap between the 3' ends of our sequences. Third, if there is sufficient overlap, we combine, or stitch, the two reads together to form one long sequence.

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/merging_paired_reads.png", auto_pdf = TRUE)
``` 

Once we have longer stitched reads, we can increase the k-mer length for our assembly. 

There are a number of pieces of software that can be used to stitch reads (e.g. `Pear`,  `Pandaseq`) but today we will use one called `FLASH`:

## FLASH: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_red.png", auto_pdf = TRUE)
``` 

Make a new output directory for the stitched reads and run `FLASH`:

```{bash eval=FALSE}
#Change directory to home
cd ~
#Make and move into new directory
mkdir 5-Stitched
cd 5-Stitched
#Run flash
flash  -o K1 -z -t 12 -d . \
../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_red.png", auto_pdf = TRUE)
```

- `-o`: Sets the prefix of the output files.
- `-z`: The input is zipped.
- `-t`: Number of threads to use.
- `-d`: The directory the output files will be placed.
- The last 2 flag-less parameters are the forward and reverse read files for stitching.

## FLASH: Output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_red.png", auto_pdf = TRUE)
``` 

Once `FLASH` has finished running, it will display on screen how well the stitching process went, in this case a low amount of reads were combined. Have a look what files have been generated.

```{bash eval=FALSE}
ls
```

We have three new fastq.gz files. One containing the stitched reads (`K1.extendedFrags.fastq.gz`) and two containing the reads from pairs that could not be combined (`K1.notCombined_1.fastq.gz` and `K1.notCombined_2.fastq.gz)`.

We can also see what the new read lengths are:

```{bash eval=FALSE}
less K1.histogram
```

Scroll down with the down key and you will see that we are looking at a histogram showing the proportion of stitched reads at different lengths. 

## FLASH: MCQs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/question_bubble_red.png", auto_pdf = TRUE)
```

```{r, echo = FALSE}
opts_p <- c("__101__", "__177__", answer="__188__")
```
1. What length has the highest proportion of stitched reads? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__101__", "__177__", "__188__")
```
2. What length has the lowest proportion of stitched reads? `r longmcq(opts_p)`

<!--chapter:end:15-Stitching_reads.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Megahit Assembly
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/puzzle.png", auto_pdf = TRUE)
``` 

We will use our stitched and unstitched reads to produce an assembly with`MEGAHIT`. 

## MEGAHIT
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_blue.png", auto_pdf = TRUE)
```

Create a new directory to store our assembly in. 

```{bash eval=FALSE}
cd ..
mkdir 6-Assembly
cd 6-Assembly
```

Now run the metagenome assembler `MEGAHIT` using our newly stitched read data.

```{bash eval=FALSE}
megahit \
-r ../5-Stitched/K1.extendedFrags.fastq.gz \
-1 ../5-Stitched/K1.notCombined_1.fastq.gz \
-2 ../5-Stitched/K1.notCombined_2.fastq.gz \
-o K1 \
-t 12 \
--k-list 29,49,69,89,109,129,149,169,189
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_blue.png", auto_pdf = TRUE)
```

- `-r`: Single-end reads to be used for assembly. We are using our successfully stitched reads.
- `-1`: Forward reads of paired end reads to be used for assembly. We are using the reads that did not stitch as they still have useful information.
- `-2`: Reverse reads of paired end reads to be used for assembly. We are using the reads that did not stitch as they still have useful information.
- `-o`: Output directory.
- `-t`: Number of threads to be used for process.
- `--k-list`: K-mer list.

The __k-mer list__ instructs `MEGAHIT` to first generate an assembly using a k-mer size of 29 bp and when that is complete, integrate the results into an assembly using a k-mer size of 49 bp, and so on up to a final iteration using a k-mer size of 189 bp. This large range of k-mer lengths should give us a good assembly, given the data. However, it may take a while to run. This might be a good time to read on or take a break whilst the command runs. 

If you need a command prompt (your current one is busy because MEGAHIT is running), right click on the main screen, choose `Applications` -> `Shell` -> `bash`.

Once the assembly is completed, we can look at the output FASTA file containing the contigs:

```{bash eval=FALSE}
less K1/final.contigs.fa
```

There is not much to see. When happy, quit the `less` (`q`) and carry on to `QUAST`.

## QUAST
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/inspect.png", auto_pdf = TRUE)
``` 

We can generate some metrics based on the assembly with `QUAST`,  but first we will create a directory for the `QUAST` output.

```{bash eval=FALSE}
#Create QUAST output directory
#The option -p will create a directory and any required
#parent directories
mkdir -p quast/K1
```

### QUAST: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_green.png", auto_pdf = TRUE)
``` 

Now to run `QUAST`. 

```{bash eval=FALSE}
quast -o quast/K1 K1/final.contigs.fa
```

### QUAST: visualise
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/eye_green.png", auto_pdf = TRUE)
```

`QUAST` will run relatively quickly. Once complete view the output with `firefox`.

```{bash eval=FALSE}
firefox quast/K1/report.html
```

The report tells us quite a bit about the assembly quality. Two definitions that you may not be aware are __N50__ and __L50__. To calculate these values:

- Order the contigs from largest to smallest.
- Total up the sizes from biggest downwards.
- The contig we reach where our total is at least 50% of the size of the whole assembly is the N50 contig.
- __N50__ equals the length of the N50 contig. 
- __L50__ is the number of contigs with a length equal to or greater than N50.

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/n50_n90.png", auto_pdf = TRUE)
``` 

### QUAST: MCQs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/question_bubble_green.png", auto_pdf = TRUE)
```

```{r, echo = FALSE}
opts_p <- c("__39.88__", "__18,650__",answer="__16,187,295__")
```
1. What is the total length of the assembly? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__39.88__", answer="__18,650__","__16,187,295__")
```
2. How many contigs does the assembly consist of? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__39.88__", "__18,650__","__16,187,295__")
```
3. What is the GC% of the assembly? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__1,363__","__2,178__","__87,586__")
```
4. What is the N50 of the assembly? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__1,363__",answer="__2,178__","__87,586__")
```
5. What is the L50 of the assembly? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__1,363__","__2,178__",answer="__87,586__")
```
6. what is the length of the largest contig? `r longmcq(opts_p)`

__Questions__
- How do the contig metrics compare to the original reads? 

## Metagenome assembly summary
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/sum_black.png", auto_pdf = TRUE)
```

We now have an assembly. It is not a brilliant one due to us only having used 1 million reads. In real analysis we would prefer fewer but longer contigs. We will explore some tools we can use with our metagenome assembly in the next chapters.

There is also a `metaQUAST` specifically for metagenome assemblies but it requires reference assemblies be provided.

<!--chapter:end:16_Megahit.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Genome binning {#binning}
```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/yarn_binning.png", auto_pdf = TRUE)
``` 

A metagenome assembly consists of contigs from many different genomes. At this stage we don't know which contigs are from which species. We could try to taxonomically classify each contig but there are 2 problems with this approach:

1. Some contigs may be misclassified which can lead to multiple contigs from the same genome/organism being classified as various taxa.
2. Databases are incomplete and so some contigs will not be classified at all (microbial dark matter).

To alleviate these issues genomic binning can be carried out. This will cluster contigs into bins based on:

- __Coverage__: Contigs with similar coverage are more likely to be from the same genome.
- __Composition__: Contigs with similar GC content are more likely to belong to the same genome.

Genomic binning has been used to discover many new genomes. Additionally, it makes downstream analyses quicker as the downstream steps will be carried out on the sets of bins rather than on one large metagenome assembly.

Binning produces "bins" of contigs of various quality (e.g. draft, complete). These bins are also know as MAGs (Metagenome-assembled genomes). In other words a MAG is a single assembled genome that was assembled with other genomes in a metagenome assembly but later separated from the other assemblies. The term MAG  has been adopted by the GSC (Genomics Standards Consortium).

It is recommended to ensure you do not have a poor quality metagenome assembly. Binning requires contigs of good length and good coverage. Extremely low coverage and very short contigs will be excluded from binning.

## MetaBAT2
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/bat.png", auto_pdf = TRUE)
``` 

We will use `MetaBAT2` for our genome binning. It is a relatively new binning tool with three major upsides that makes it very popular:

1. It has very reliable default parameters meaning virtually no parameter optimisation is required.
2. It performs very well amongst genome binners.
3. It is computationally efficient compared to other binners (requires less RAM, cores etc.)

Make a new directory and move into it.

```{bash eval=FALSE}
#Make directory
mkdir -p ~/7-Binning/K1
#Move into it
cd ~/7-Binning/K1
```

### MetaBAT2: depth calculation
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/submarine.png", auto_pdf = TRUE)
``` 

To carry out effective genome binning `MetaBAT2` uses coverage information of the contigs. To calculate depth we need to align the reads to the metagenome assembly.

For the alignment we will use `bwa`. We need to index our assembly file prior to alignment.

```{bash eval=FALSE}
bwa index ~/6-Assembly/K1/final.contigs.fa
```

Next we will align our trimmed paired reads we used to create the stitched reads. We will carry this out with the `bwa mem` command. `bwa mem` is a good aligner for short reads. If you are using long reads (PacBio or Nanopore) `minimap2` will be more appropriate.

```{bash eval=FALSE}
bwa mem ~/6-Assembly/K1/final.contigs.fa \
~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz > \
K1.sam
```

After alignment we need to get the file ready for the contig depth summarisation step. This requires converting the `sam` file to a `bam` (binary form of a `sam` file) file and then sorting the `bam` file.

```{bash eval=FALSE}
# Convert sam to bam file
samtools view -bu K1.sam > K1.bam
# Created sorted bam file
samtools sort K1.bam > K1.sort.bam
```

Now we can summarise the contig depths from the sorted `bam` files with `MetaBAT2`'s `jgi_summarize_bam_contig_depths` command.

```{bash eval=FALSE}
jgi_summarize_bam_contig_depths --outputDepth K1.depth.txt K1.sort.bam
```

You can have a look at the depth file and you will notice there are many contigs with low coverage (<10) and of short length (<1500).

```{bash eval=FALSE}
less K1.depth.txt
```

To get a better look we will open the file in `R` and look at a summary of the file's table.

Activate `R`:

```{bash eval=FALSE}
R
```

Now in `R` we will read in the file and get a `summary()` of it.

```{r eval=FALSE}
#Read in the table as an object called df (short for data frame)
#We want the first row to be the column names (header=TRUE)
#We do not want R to check the column names and "fix" them (check.names=FALSE)
df <- read.table("K1.depth.txt", header=TRUE, check.names=FALSE)
#Create a summary of the data
summary(df)
```

The last command gave us summary information of all the columns. This includes the minimum, maximum, mean, median, and Inter-Quartile Range (IQR) values. 

We can see the values of the `contigLen` and `totalAvgDepth` are very low. However, this is most likely due to a bunch of short and low coverage contigs which will be ignored by `MetaBAT2`. Therefore we will remove rows with information on contigs shorter than 1500 and rerun the summary. `MetaBAT2`'s documentation dictates the minimum contig length should be >=1500 with its default being 2500.

```{r eval=FALSE}
#Set the new object "df_min1500len" as all rows
#where the value in the column "contigLen" of "df"
#Is greater than or equal to 1500
df_min1500len <- df[df$contigLen >= 1500,]
#Summary of our new data frame
summary(df_min1500len)
```

That is looking better. The minimum average coverage for `MetaBAT2` is 1 and our minimum value is 2.844 with a maximum of 92.967. Now you can quit R and continue.

```{r eval=FALSE}
#quit R
q()
#On the prompt to save your workspace press "n" and then enter.
```

__Note__: One of the reasons for our short contigs is that we only used a subset of our sequencing dataset for this tutorial due to time concerns.

### MetaBAT2: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_orange.png", auto_pdf = TRUE)
``` 

With our assembly and its depth information we can run `MetaBAT2` for binning.

```{bash eval=FALSE}
#make a diretcory for the bins
mkdir bins
#Run MetaBAT2
metabat2 \
--inFile ~/6-Assembly/K1/final.contigs.fa \
--outFile bins/K1 \
--abdFile K1.depth.txt \
--minContig 1500
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_orange.png", auto_pdf = TRUE)
```

- `--inFile`: Input metagenome assembly fasta file.
- `--outFile`: Prefix of output files.
- `--abdFile`: Base depth file.
- `--minContig`: Minimum size of contigs to be used for binning.
  - The default is 2500.
  - We used the minimum value of 1500 as we are using tutorial data. We recommend using the default in your own analysis.

### MetaBAT2: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_orange.png", auto_pdf = TRUE)
``` 

List the contents of the output directory and you'll see there is 1 fasta file with the prefix of `K1`. This is a bin that will hopefully contain 1 MAG (Metagenome-Assembled Genome). In your future analysis you may get many bins, each hopefully only having one MAG.

```{bash eval=FALSE}
ls bins
```

## CheckM
```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/CheckM.png", auto_pdf = TRUE)
``` 

`CheckM` is a useful tool to assess the quality of assembled bacterial and archaeal genomes. This can be used on assemblies produced from single cell, single isolate, or metagenome data. Additionally, it can be used to identify bins that are likely candidates for merging. This occurs when one genome has been separated into different bins.

An important part of CheckM is the ubiquitous and single-copy genes it utilises. It has sets of these genes for different phylogenetic lineages. With these it can determine:

- What lineage a bin/MAG belongs to. 
   - Does it contain genes only found in _Escherichia_?
- How complete the bin/MAG is. A set of lineage specific genes should all be found in a genome belonging to the lineage (ubiquitous).
   - What percentage of these lineage specific genes are present in the MAG? 
   - \>95% is very good
   - \>80% is good
   - \>70% is ok
   - <70% is poor to poorer
- How contaminated the bin/MAG is.
   - Only one copy of each gene should be present (single-copy).
   - Are there any markers for other lineages present?

### CheckM: Mamba
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/mamba_logo.png", auto_pdf = TRUE)
``` 

Due to program version conflicts we will use the `checkm` conda environment for this section.

Open a new terminal and activate the `checkm` environment.

```{bash eval=FALSE}
. usecheckm
```

Ensure you are in the correct directory.

```{bash eval=FALSE}
cd ~/7-Binning/K1/
```

### CheckM: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_black.png", auto_pdf = TRUE)
``` 

`CheckM` has many different commands. We will use one of the common workflows it provides called `lineage_wf`. This carries out five of its commands in a workflow (i.e. the next step uses output from the previous step).

1. `tree` - Places bins in the reference genome tree. This reference tree comes with `CheckM`.
2. `tree_qa` - Assess the phylogenetic markers found in each bin.
3. `lineage_set` - Infers lineage-specific marker sets for each bin. 
4. `analyze` - Identifies marker genes in bins.
5. `qa` - Assesses the bins for contamination and completeness.

Run the `CheckM` command (this will take a while):

```{bash eval=FALSE}
checkm lineage_wf \
--tab_table -f MAGS_checkm.tsv \
-x fa \
bins/ checkm_output
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_black.png", auto_pdf = TRUE)
```

- `--tab_table` : Prints results to a tab separated table.
- `-f` : File name to print result to (if not specified results will go to stdout).
- `-x` : Suffix/extension of bin files. Other files are ignored in the specified bin directory.
   - `fa` is used as our `MetaBAT2` analysis produced fasta files that end in `.fa`.
- `bins` : The second last argument is the `bin_dir`, the directory containing all the bins to be analysed in fasta format.
- `checkm_output` : The last argument is the directory to store the output to. This directory should not exist prior to running.

### CheckM: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_black.png", auto_pdf = TRUE)
``` 

As we have only used a subset of data the results are not very good. We'll therefore look at premade results. 
These premade results were produced using the entire K1 dataset. 
First you will need to copy them.

```{bash eval=FALSE}
cd ~/7-Binning
mkdir K1_fullset
cd K1_fullset
ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/binning/K1_fullset/* .
```

Now we can look at the results table that is in your current directory.

```{bash eval=FALSE}
less MAGS_checkm.tsv
```

`r hide("CheckM statistics definitions")`
List of statistics definitions from [CheckM wiki](https://github.com/Ecogenomics/CheckM/wiki/Reported-Statistics)

- __bin id__: unique identifier of genome bin (derived from input fasta file)
- __marker lineage__: indicates the taxonomic rank of the lineage-specific marker set used to estimated genome completeness, contamination, and strain heterogeneity. More detailed information about the placement of a genome within the reference genome tree can be obtained with the tree_qa command. The UID indicates the branch within the reference tree used to infer the marker set applied to estimate the bins quality.
- __# genomes__: number of reference genomes used to infer the lineage-specific marker set
- __markers__: number of marker genes within the inferred lineage-specific marker set
marker sets: number of co-located marker sets within the inferred lineage-specific marker set
- __0-5+__: number of times each marker gene is identified
- __completeness__: estimated completeness of genome as determined from the presence/absence of marker genes and the expected collocalization of these genes (see Methods in the PeerJ preprint for details)
- __contamination__: estimated contamination of genome as determined by the presence of multi-copy marker genes and the expected collocalization of these genes (see Methods in the PeerJ preprint for details)
- __strain heterogeneity__: estimated strain heterogeneity as determined from the number of multi-copy marker pairs which exceed a specified amino acid identity threshold (default = 90%). High strain heterogeneity suggests the majority of reported contamination is from one or more closely related organisms (i.e. potentially the same species), while low strain heterogeneity suggests the majority of contamination is from more phylogenetically diverse sources (see Methods in the CheckM manuscript for more details).
- __genome size__: number of nucleotides (including unknowns specified by N's) in the genome
- __# ambiguous bases__: number of ambiguous (N's) bases in the genome
- __# scaffolds__: number of scaffolds within the genome
- __# contigs__: number of contigs within the genome as determined by splitting scaffolds at any position consisting of more than 10 consecutive ambiguous bases
- __N50 (scaffolds)__: N50 statistics as calculated over all scaffolds
- __N50 (contigs)__: N50 statistics as calculated over all contigs
- __longest scaffold__: the longest scaffold within the genome
- __longest contig__: the longest contig within the genome
- __GC__: number of G/C nucleotides relative to all A,C,G, and T nucleotides in the genome
- __coding density__: the number of nucleotides within a coding sequence (CDS) relative to all nucleotides in the genome
- __translation table__: indicates which genetic code was used to translate nucleotides into amino acids
- __# predicted genes__: number of predicted coding sequences (CDS) within the genome as determined using Prodigal
`r unhide()`

### CheckM: Quality score

One quick way to calculate the overall quality of a bin is with the following equation:

$$
q = comp - (5 * cont)
$$
Where:

- __q__ = Overall quality
- __comp__ = Completeness
- __cont__ = Contamination

A score of at least 70-80% would be the aim, with a maximum/perfect value being 100% (100% completeness, 0% contamination). Therefore let us calculate this for the bins with some bash and `awk` scripting.

#### Tab to comma seperated {-}

First it is good to make a copy of the file in case we make a mistake and want to start over. Additionally, we will make the copy a comma separated file. I find these easier to edit as typing comma characters (`,`) in commands is more reliable than tab characters (`\t`).

This is carried out with `tr` which can translate characters. In the below case we translate/convert the tabs (`\t`) into commas (`,`). The converted output is then redirected (`>`) to the file "MAGS_checkm.csv".

```{bash eval=FALSE}
cat MAGS_checkm.tsv | tr "\t" "," > MAGS_checkm.csv
```

#### Quality file {-}

We will create a new file with only the quality information. We'll start by making a file with only a header.

```{bash eval=FALSE}
echo "Quality" > MAGS_quality.csv
```

#### Calculate quality with awk {-}

Next is the most complicated command. We will be calculating the Overall quality (see calculation above) for each row except the header row.

We will be using a complicated linux based language called `awk`. This is very useful as it can carry out calculations on columns or as `awk` calls them, __fields__.

As this is new and complicated we will build up our command step by step.

__The first step__ is to extract the completeness and contamination fields/columns.

```{bash eval=FALSE}
awk -F, '{print $12,$13}' MAGS_checkm.csv
```

- `-F,`: Indicates the input fields are separated by commas (`,`).
- `''`: All the `awk` options are contained within the quotes.
- `{}`: We can supply a function to `awk` within the braces.
- `print $12,$13`: This function instructs `awk` to print the 12th (completeness) and 13th (contamination) fields. It is common to put commas (`,`) between fields if printing multiple fields.
- `MAGS_checkm.csv`: Our last parameter is the input file. We are not changing the contents of the file, only printing information to screen/stdout.

__We do not want the header__ in our calculation so we will add an extra `awk` option.

```{bash eval=FALSE}
awk -F, 'NR>1 {print $12,$13}' MAGS_checkm.csv
```

- `NR>1`: `NR` stands for number of records. Rows are called records in `awk`. Therefore `NR>1` means `awk` will only carry out the functions on the records numbered greater than 1. I.e. skip row 1, the header row.

__The final step__ is to carry out the overall quality calculation and append the information to the "MAGS_quality.csv" file.

```{bash eval=FALSE}
awk -F, 'NR>1 {print $12 - (5 * $13)}' MAGS_checkm.csv >> MAGS_quality.csv
```

- `{print $12 - (5 * $13)}`: Our new function carries out the overall quality calculation and prints it for each record/row except the first (`NR>1`).
- `>> MAGS_quality.csv`: The printed information is appended (`>>`) to the file "MAGS_quality.csv". We append because we want to retain the header we added to the file earlier.

You can view the file to ensure it worked. The first and second values should be 33.62 and -0.89

```{bash eval=FALSE}
less MAGS_quality.csv
```

#### Add quality to the checkm results file {-}

Now we can combine the files "MAGS_checkm.csv" and "MAGS_quality.csv" with the `paste` command. The `-d ","` option indicates the merged files will be separated by commas (`,`), matching the column separation in "MAGS_checkm.csv".

```{bash eval=FALSE}
paste -d "," MAGS_checkm.csv MAGS_quality.csv > MAGS_checkm_quality.csv
```

### CheckM: MCQs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/question_bubble_blue.png", auto_pdf = TRUE)
```

Viewing the file "MAGS_checkm_quality.csv" attempt the below questions.

__Tip__: You can use the `cut` command to look at specific columns. For example:

```{bash eval=FALSE}
#look at the "Bin Id" and "Quality"
#Convert the printed output's commas to tabs for readability
cut -d "," -f 1,15 MAGS_checkm_quality.csv | tr "," "\t"
```

```{r, echo = FALSE}
opts_p <- c("__root (UID1)__", answer="__k_Bacteria (UID203)__", "__o_Lachnospiraceae__")
```
1. What lineage was assigned to bin __K1.1__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__root (UID1)__", "__k_Bacteria (UID203)__", answer="__o_Lachnospiraceae__")
```
2. What lineage was assigned to bin __K1.22__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__root (UID1)__", "__k_Bacteria (UID203)__", "__o_Lachnospiraceae__")
```
3. What lineage was assigned to bin __K1.8__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__4.17__", answer="__33.62__", "__172__")
```
4. What is the quality value of K1.1? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__4.17__", "__33.62__", "__172__")
```
5. What is the completeness value of __K1.27__? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__4.17__", "__33.62__", answer="__172__")
```
6. How many genomes are within the __K1.12__ bin? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__K1.20__", "__K1.21__", answer="__K1.22__")
```
7. Which bin has the highest quality value (96.38%)? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c(answer="__K1.20__", "__K1.21__", "__K1.22__")
```
8. Which bin has the lowest quality value (-300.67%)? `r longmcq(opts_p)`

```{r, echo = FALSE}
opts_p <- c("__K1.20__", answer="__K1.21__", "__K1.22__")
```
9. Which bin has the highest completeness value (98.27%)? `r longmcq(opts_p)`

## Binning summary
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/sum_black.png", auto_pdf = TRUE)
```

It is always useful to know the quality of your bins so you know which are more reliable than others. With that information you can be more or less certain when concluding your findings.

We have some good quality bins. However, the best bins would only have one genome. Ultimately binning is trying to separate all the genomes from each other. A better metagenome assembly would most likely have led to better binning.

<!--chapter:end:17-Genome_binning.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Assembly functional annotation {#funcanno}
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/pathways.png", auto_pdf = TRUE)
``` 

## Taxonomic annotation
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/classification.png", auto_pdf = TRUE)
``` 

Taxonomic annotation of bins can be carried out with `Kraken2`. As we have already done this for the reads and taxonomic results from read and assembly approaches have similar performances we will not cover it here. Instead we will move straight onto functional annotation with [`Bakta`](https://github.com/oschwengers/bakta).

## Bakta
```{r, fig.align = 'center',out.width= '25%', echo=FALSE }
knitr::include_graphics(path = "figures/bakta_logo.png", auto_pdf = TRUE)
``` 

We will carry out `Bakta` functional annotation using. `Bakta` can annotate bacterial genomes and plasmids from both isolates and MAGs.

Make a new directory and move into it.

```{bash eval=FALSE}
mkdir ~/8-Annotation
cd ~/8-Annotation
```

### Bakta: run
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_blue.png", auto_pdf = TRUE)
``` 

Now we can annotate one of the bins.

The below will take a long time to run (>1 hour). Instead of running it skip onto the next step to copy pre-made output to continue with. This command is here so you know what to run in your own future analyses.

```{bash eval=FALSE}
bakta \
--db /pub14/tea/nsc206/NEOF/databases/bakta/db/ \
-o K1.1 \
~/7-Binning/K1_fullset/bins/K1.1.fa
```

#### Parameters {-}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/parameter_blue.png", auto_pdf = TRUE)
```

- `--db`: Location of `Bakta` database. You will need to install this in your own installation. Instructions are in the [appendix](#mamba_install).
- `-o`: The output directory. This must not exist before running the command.
- The last parameter is the fasta file containing the genome/plasmid you would like annotated.

### Bakta: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_blue.png", auto_pdf = TRUE)
``` 

Move into the newly created K1.1 directory and list the files. Each of the files has the prefix "K1.1" and contain the following information:

- __<prefix>.tsv__: annotations as simple human readble TSV
- __<prefix>.gff3__: annotations & sequences in GFF3 format
- __<prefix>.gbff__: annotations & sequences in (multi) GenBank format
- __<prefix>.embl__: annotations & sequences in (multi) EMBL format
- __<prefix>.fna__: replicon/contig DNA sequences as FASTA
- __<prefix>.ffn__: feature nucleotide sequences as FASTA
- __<prefix>.faa__: CDS/sORF amino acid sequences as FASTA
- __<prefix>.hypotheticals.tsv__: further information on hypothetical protein CDS as simple human readble tab separated values
- __<prefix>.hypotheticals.faa__: hypothetical protein CDS amino acid sequences as FASTA
- __<prefix>.json__: all (internal) annotation & sequence information as JSON
- __<prefix>.txt__: summary as TXT
- __<prefix>.png__: circular genome annotation plot as PNG
  - These are only useful for complete/near complete circular genomes.
  - I would suggest looking at [GenoVi](https://github.com/robotoD/GenoVi/wiki/User-guide#tutorials-) for circular genome plots.
- __<prefix>.svg__: circular genome annotation plot as SVG

View the summary file for bin K1.1.

```{bash eval=FALSE}
less K1.1/K1.1.txt
```

`r hide("Summary fields")`
Sequence infomration

- Length: Number of bases.
- Count: Number of contigs/scaffolds.
- GC: GC%.
- N50: [N50](#quast-visualise).
- N ratio: Ratio of N bases to non-N bases.
- coding density: Percentage of bases within coding regions.

Annotation information.

- tRNAs: Transfer RNAs.
- tmRNAs: Transfer-messenger RNA.
- rRNAs: Ribosomal RNAs.
- ncRNAs: Non-coding RNAs.
- ncRNA regions: Non-coding RNA regions.
- CRISPR arrays: CRISPR arrays.
- CDSs: Coding sequences.
- pseudogenes: Segments of DNA that structurally resembles a gene but is not capable of coding for a protein
- hypotheticals: Hypothetical genes, which are predicted solely by computer algorithms, are experimentally uncharacterized genes.
- signal peptides: Short peptides (usually 16-30 amino acids long) normally present at the N-terminus of most newly synthesized proteins that are destined toward the secretory pathway.
- sORFs: Short open reading frames (<100 amino acids).
- gaps: Gaps in the genome assembly.
- oriCs: Chromosome replication origin for bacteria.
- oriVs: Plasmid replication origin.
- oriTs: An origin of transfer (oriT) is a short sequence ranging from 40-500 base pairs in length. It is necessary for the transfer of DNA from a gram-negative bacterial donor to recipient during bacterial conjugation.
`r unhide()`

View the gff file for bin K1.1.

```{bash eval=FALSE}
less K1.1/K1.1.gff3
```

The GFF file is a tab delimited file contain annotation information for the features in the assembly/bin. In this case it is a GFF3 file (most curent version of GFF).

There is quite a lot of information contained in each row so instead of listing all the columns here please have a look at the official documentation:

https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md

Link the pre-made results for all the K1 bins.

```{bash eval=FALSE}
#Remove you K1.1 annotation
rm -r k1.1
#Link all data
ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/bakta/K1_fullset/* .
```

`r hide("Loop used to analyse all bins")`
The code below is for your future analysis, do not run it now as it will take to long.
```{bash eval=FALSE}
ls -1 ~/7-Biinning/K1_fullset/bins/*fa | while read f ; \
do s=$(basename $f | sed "s/.fa//") ; echo $s ; \
bakta --db /pub14/tea/nsc206/NEOF/databases/bakta/db/ \
-o ${s} \
~/7-Binning/K1_fullset/bins/K1.1.fa
```
`r unhide()`

A quick thing we can do with these files is to see if any of the bins contain a specific annotation. For example, if we wanted to know if there were any ATP-binding proteins in any of the bins we could carry out the below command.

```{bash eval=FALSE}
grep "ATP-binding protein" */*gff3 | less
```

We can now view the lines containing "ATP-binding protein" with the start of the line containing the file name the line belongs to.

In your future analyses you can expect these files further with excel, R, or visualisation software like `IGV` (https://software.broadinstitute.org/software/igv/GFF).

What if you want to know about pathways?

## MinPath
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/pathways_2.png", auto_pdf = TRUE)
```

`MinPath` can predict `MetaCyc` metabolic pathways. These pathway are made up of sets of enzymes that catalyse various reactions.

Ensure you start in the `~/8-Annotation/K1_fullset` directory.

### MinPath: EC extraction
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/exctractor.png", auto_pdf = TRUE)
```

Before we can estimate the pathways we need to extract the EC numbers predictions from the GFF file. EC (Enzyme Commission) numbers are a numerical classification for enzymes based on the reaction they catalyse.

Unless you know the EC scheme well they are generally not helpful by themselves. An example EC number is EC 3.1.1.The numbers represents the group the enzyme belongs to with the first number being the biggest group. From highest to the lowest grouping 3.1.1. represents:

- __3__: Hydrolase enzymes.
- __3.1__: Hydrolase enzymes that act on ester bonds.
- __3.1.1__: Carboxylic Ester Hydrolases.

With all that information we will extract the EC annotations from the GFF files. First we'll create a directory for the output and move into the GFF directory.

```{bash eval=FALSE}
#Create EC directory
mkdir EC
```

We will now use a loop with various parts to create an EC annotation files.
The input`.ec` files required for `MinPath` are tab delimited with two columns:

1. Protein/sequence id. E.g. GDGAPA_12670.
2. EC annotation. E.g. 6.1.1.4.

__Note__: The lack of `\` at the end of most of the ines is intentional. 
The below is all one command over multiple lines but loops work slightly different and don't need `\` in certain parts. 
Ensure you do press enter at the end of each line.

```{bash eval=FALSE}
ls -1 */*gff3 | sed "s|.*/||" | sed "s/.gff3//" | while read bin
do
cat ${bin}/${bin}.gff3 | grep "EC:" | cut -f 9 | sed "s/ID=//" \
sed "s/;.*EC:/\t/" | sed "s/,.*//" >../EC/${bin}.ec
done
```

#### Code explanation

That is quite a bit of code. You don't need to understand it as it should always work for `Bakta` output. 
If you are not currently interested you can skip to the [`MinPath`: run section](#minpathrun)

If you are interested in how it works we'll break it down with examples for you to run. 

The first part lists all our `.gff` files on one line each (`ls -1 *gff`).
Next, we remove the directory name (`sed "s|.*||"`).
Then the suffix `.gff` is substituted with nothing `sed "s/.gff//"`. 
This gives us the name of each bin (e.g. `K1.1`, `K1.2`).

```{bash eval=FALSE}
#List all gff files on one (-1) each
ls -1 */*gff3
#Remove the directory name
#You can use any character as the divider in sed
#Useful when you want to move slashes from a file name
ls -1 */*gff3 | sed "s|.*/||"
#List all the file prefixes (on one line each)
ls -1 */*gff3 | sed "s|.*/||" | sed "s/.gff3//"
```

With the above code we can loop through each file prefix and use the variable `bin` (arbitrarily chosen) which contains the file prefix. 
This is carried out with `while read bin`. 
All the lines between the `do` (start of the loop) and `done` (end of loop) are line run in the loop.

Run the loop with an echo command to show we are using the `${bin}` variable to specify the input and output files.

```{bash eval=FALSE}
ls -1 */*gff3 | sed "s|.*/||" | sed "s/.gff3//" | while read bin
do
echo "${bin}/${bin}.gff3 ../EC/${bin}.ec"
done
```

Now to look at the command within the loop:

`cat ${bin}.gff3 | grep "EC:" | cut -f 9 | \`
`sed "s/ID=//" | sed "s/;.*;Dbxref=/,/" | \`
`sed "s/,.*EC:/\t/" | sed "s/,.*//" >../EC/${bin}.ec`

A good way to figure out what a pipe workflow is doing, is by building it up step by step.
Run the first part of workflow and then add the next section, run, repeat.
This shows you how each new section is affecting the output. 
Additionally, it is always good to run it on one file and `head` the output so we have a manageable amount of data to look at.

We'll do that with the `K1.1.gff3` file.

__Note__: Remember we are adding `head` to the end for ease of viewing.

__Tips__: 

- Use the up arrow to go back to previously run commands that you can then edit.
- Remember the `clear` command.

```{bash eval=FALSE}
#Grab every line containing "EC:"
cat K1.1/K1.1.gff3 | grep "EC:" | head
#Cut out the 9th column/field (-f) (i.e. only keep the 9th column)
#This is the attributes field in GFF3
#This contains a plethora of information including the EC annotation if present
#cut uses tabs as the default column/field delimiter
cat K1.1/K1.1.gff3 | grep "EC:" | cut -f 9 | head
#The gff3 attributes field starts with the ID
#We want to keep this but remove the "ID=" part
cat K1.1/K1.1.gff3 | grep "EC:" | cut -f 9 | sed "s/ID=//" | head
#We don't want any of the info between the ID and the EC number
#Therefore we want to remove everything (.*) between
# the first "," (at the end of the ID info)
# and "EC="
#We'll replace this with a \t to seprarate the ID and EC
# columns with a tab (required by MinPath)
cat K1.1/K1.1.gff3 | grep "EC:" | cut -f 9 |  sed "s/ID=//" | \
sed "s/;.*EC:/\t/" | head
#Finally remove all the info after the EC number
#This info will be after the last ,
cat K1.1/K1.1.gff3 | grep "EC:" | cut -f 9 | sed "s/ID=//" \
sed "s/;.*EC:/\t/" | sed "s/,.*//" | head
```

In the looped command, output is written into files: `> EC/${bin}.ec`.

### MinPath: run {#minpathrun}
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/play_green.png", auto_pdf = TRUE)
``` 

With our `.ec` files we can create our MetaCyc predictions.

First we'll change directory into the EC directory. Then create an output directory for the MetaCyc predictions.

```{bash eval=FALSE}
cd ../EC
mkdir ../MetaCyc
```

Now we can loop through the file suffixes to run MinPath.

```{bash eval=FALSE}
ls -1 *ec | sed "s/.ec//" | while read bin
do
python /pub14/tea/nsc206/git_installs/MinPath/MinPath.py \
-ec ${bin}.ec \
-report ../MetaCyc/${bin}.minpath
done
```

A lot of output will be printed to screen but this can be ignored unless you see warnings.

### MinPath: output
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/output_file_green.png", auto_pdf = TRUE)
``` 

First, change directory into the `MetaCyc` directory.

```{bash eval=FALSE}
#Change directory
cd ../MetaCyc
#List contents
ls
```

From the `CheckM` results we found that bin K1.22 was very good with a quality score >96%. We will therefore have a look at its output.

Have a look at the `.minpath` `-report` file for K1.22.

```{bash eval=FALSE}
less K1.22.minpath
```

The file contains the following columns

1. __Pathway ID__
2. __Pathway reconstruction__: Only available for genomes annotated in MetaCyc database.
3. __Naive__: Indicates if pathway was reconstructed by the naive mapping approach (1) or not (0).
4. __Minpath__: Indicates if the pathway was kept (1) or removed (0) by `MinPath`.
5. __Fam0__: The number of families involved in the pathway.
6. __Fam-found__: Number of families in pathway that were annotated/found.
7. __Name__: Description of pathway.

Quit (`q`) `less` when you are happy.

There are some quick things we can do in bash with these files.

```{bash eval=FALSE}
#Count number of pathways found in each bin with word count
wc -l *minpath
#Grab every line with "PWY-6972" from every file
grep "PWY-6972" *minpath | less
```

### KEGGs
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/KEGG.png", auto_pdf = TRUE)
```

You can also get KEGG information with `MinPath`.
The code:

```{bash eval=FALSE}
#Change to correct directory
cd ~/8-Annotation
#Make directory for KEGGs
mkdir KEGG
#KEGG extractions
ls -1 */*gff3 | sed "s|.*/||" | sed "s/.gff3//" | while read bin
do
cat ${bin}/${bin}.gff3 | grep "KEGG:" | cut -f 9 | sed "s/ID=//" \
sed "s/;.*KEGG:/\t/" | sed "s/,.*//" >../KEGG/${bin}.kegg
done
#Make directory for KEGG minpath output
mkdir KEGG_minpath
#Change directory to KEGG
cd KEGG
#Run MinPath
ls -1 *kegg | sed "s/.kegg//" | while read bin
do
python /pub14/tea/nsc206/git_installs/MinPath/MinPath.py \
-ko ${bin}.kegg \
-report ../KEGG_minpath/${bin}.minpath
done
```

With these files you can then investigate what bins have which pathways. Additionally, with more samples analysed you can determine which samples have which pathways present.

<!--chapter:end:18-Assembly_functional_annotation.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# (APPENDIX) Appendix {-}

# Mamba installs
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/mamba_logo.png", auto_pdf = TRUE)
```

## Mamba installation and environment {#mamba_install}

Mamba is a reimplementation of conda.
It is a great tool for installing bioinformatic packages including R packages.

Mamba github: https://github.com/mamba-org/mamba

Mamba installation: https://github.com/conda-forge/miniforge#mambaforge

Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html

To create the mamba environment `shotgun_meta` run the below commands in your bash. You will need to have installed `mamba` first.

```{bash, eval=FALSE}
#shotgun_meta
mamba create -n shotgun_meta
mamba activate shotgun_meta
#Install packages
mamba install -c bioconda fastqc trim-galore multiqc bowtie2 kraken2 krona bracken lefse flash megahit quast metabat2 prokka bbmap bakta circos
#Update krona taxonomy database
ktUpdateTaxonomy.sh
#Install GenoVi via pip
pip install genovi
```

You will need to install the `Bakta` database as well. This requires you have a directory to store the database. In the below example you will need a directory with the path `~/databases/bakta_db`, of course feel free to use a different location you have.

```{bash, eval=FALSE}
bakta_db download --output ~/databases/bakta --type full
```

Install [MinPath](https://github.com/mgtools/MinPath) via `git`.
I suggest creating a directory called "git_installs" in your home directory and running this code there.

```{bash, eval=FALSE}
#git_installs directory
mkdir ~/git_installs
cd ~/git_installs
#MinPath
git clone https://github.com/mgtools/MinPath
```

To run `MinPath` in your own machines you will need to use the full path of the python file.
If you installed these in your "~/git_installs" examples are below.

```{bash, eval=FALSE}
#MinPath help page
python ~/git_installs/Minpath/MinPath.py -h
```

To create the mamba environment `checkm` run the below commands in your bash. You will need to have installed `mamba` first.

```{bash, eval=FALSE}
#checkm
mamba create -n checkm
mamba activate checkm
mamba install -c bioconda checkm-genome
```

To create the mamba environment `biobakery` run the below commands in your bash. You will need to have installed `mamba` first.

```{bash, eval=FALSE}
#biobakery3
mamba create -n biobakery3
mamba activate biobakery3
#Add required channels
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --add channels biobakery
#Install packages
mamba install -c biobakery humann
mamba install -c bioconda hclust2 lefse
```

Next you will need to install the `HUMAnN` and `MetaPhlAn` databases.

```{bash, eval=FALSE}
#Ensure you have biobakery3 mamba environment activated
#Update HUMAnN databases
humann_databases --download chocophlan full /path/to/databases --update-config yes
humann_databases --download uniref uniref90_diamond /path/to/databases --update-config yes
humann_databases --download utility_mapping full /path/to/databases --update-config yes
#Install MetaPhlAn databases
metaphlan --install
```

Installing the `MetaPhlAn` databases may not work with the command above. If not follow the below instrcutions.

- Go to the following link: http://cmprod1.cibio.unitn.it/biobakery4/metaphlan_databases/
- Download the following files (or more recent versions if available):
  - __mpa_vOct22_CHOCOPhlAnSGB_202212.md5__
  - __mpa_vOct22_CHOCOPhlAnSGB_202212.tar__
  - __bowtie2_indexes/mpa_vOct22_CHOCOPhlAnSGB_202212_bt2.md5__
  - __bowtie2_indexes/mpa_vOct22_CHOCOPhlAnSGB_202212_bt2.tar__
- Put these in the `MetaPhlAn` databases directory. This will be in your mamba environment directory.
  - E.g.: ~/mambaforge/envs/biobakery3/lib/python3.7/site-packages/metaphlan/metaphlan_databases
  - Note: this may not be your exact path.
- Run `metaphlan --install` again.
- Untar the CHOCOPhlan tar file
  - Change directory to your `MetaPhlAn` database directory.
  - `tar -xvf mpa_vOct22_CHOCOPhlAnSGB_202212.tar`

Once the databses are all setup you can test `HUMAnN`.

```{bash, eval = FALSE}
humann_test
```


# Next steps
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/step.png", auto_pdf = TRUE)
``` 

Below are some good links to start with before carrying out your own projects.

- `KrakenTools`: A suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. 
  - https://github.com/jenniferlu717/KrakenTools
- A review of computational tools for generating metagenome-assembled genomes from metagenomic sequencing data
  - https://www.sciencedirect.com/science/article/pii/S2001037021004931
- A review paper on metagenome assembly approaches
   - Genome-resolved metagenomics using environmental and clinical samples: https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbab030/6184411
- bioBakery tools for meta'omic profiling
   - https://github.com/biobakery/biobakery
- Assessing the performance of different approaches for functional and taxonomic annotation of metagenomes
   - https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6289-6
- MicrobeAnnotator: Easy-to-use pipeline for the comprehensive metabolic annotation of microbial genomes.
   - https://github.com/cruizperez/MicrobeAnnotator
- MetaEuk: Functional annotation of eukaryotic metagenome
   - https://github.com/soedinglab/metaeuk
_ Methods for Metagenomic data visualisation and analysis
   - https://www.researchgate.net/publication/318252633_Methods_for_The_Metagenomic_Data_Visualization_and_Analysis

# Manuals
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/manual.png", auto_pdf = TRUE)
``` 

Conda: https://conda.io/projects/conda/en/latest/user-guide/getting-started.html

FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/

MultiQC: https://multiqc.info/

Trim Galore: https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/

Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml

samtools: http://www.htslib.org/

BBTools: https://jgi.doe.gov/data-and-tools/bbtools/

Kraken2: https://github.com/DerrickWood/kraken2/wiki/Manual

Krona: https://github.com/marbl/Krona/wiki/KronaTools

Bracken: https://ccb.jhu.edu/software/bracken/index.shtml?t=manual

LEfSe: https://huttenhower.sph.harvard.edu/lefse/

HUMAnN 3.0: https://huttenhower.sph.harvard.edu/humann/

MetaPhlAn 4.0: https://github.com/biobakery/MetaPhlAn/wiki/MetaPhlAn-4

Biobakery: https://github.com/biobakery/biobakery

hclust2: https://github.com/SegataLab/hclust2

MegaHit: https://github.com/voutcn/megahit

BWA: https://github.com/lh3/bwa

minimap2: https://github.com/lh3/minimap2

MetaBAT2: https://bitbucket.org/berkeleylab/metabat/src/master/

CheckM: https://github.com/Ecogenomics/CheckM/wiki

PhyloPhlAn: https://github.com/biobakery/phylophlan/wiki

Prokka: https://github.com/tseemann/prokka

MinPath: https://github.com/mgtools/MinPath/blob/master/readme

MetaCyc: https://metacyc.org/

# Obtaining Read Data

The following commands can be used to obtain the sequence data used in this practical, directly from the EBI metagenomics site. It is worth noting that these are the full set of data, not like the miniaturised version you have used in the tutorial.

```{bash eval=FALSE}
wget -O K1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_1.fastq.gz
wget -O K1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_2.fastq.gz
wget -O K2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_1.fastq.gz
wget -O K2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_2.fastq.gz
wget -O K3_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505095/ERR505095_1.fastq.gz
wget -O K3_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505095/ERR505095_2.fastq.gz
wget -O K4_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505096/ERR505096_1.fastq.gz
wget -O K4_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505096/ERR505096_2.fastq.gz
wget -O K5_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505097/ERR505097_1.fastq.gz
wget -O K5_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505097/ERR505097_2.fastq.gz
wget -O K6_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505098/ERR505098_1.fastq.gz
wget -O K6_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505098/ERR505098_2.fastq.gz
wget -O K7_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505099/ERR505099_1.fastq.gz
wget -O K7_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505099/ERR505099_2.fastq.gz
wget -O K8_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505100/ERR505100_1.fastq.gz
wget -O K8_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505100/ERR505100_2.fastq.gz
wget -O K9_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505101/ERR505101_1.fastq.gz
wget -O K9_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505101/ERR505101_2.fastq.gz
wget -O K10_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505103/ERR505103_1.fastq.gz
wget -O K10_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505103/ERR505103_2.fastq.gz
wget -O K11_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505105/ERR505105_1.fastq.gz
wget -O K11_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505105/ERR505105_2.fastq.gz
wget -O K12_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505106/ERR505106_1.fastq.gz
wget -O K12_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505106/ERR505106_2.fastq.gz
wget -O W1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_1.fastq.gz
wget -O W1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_2.fastq.gz
wget -O W2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505088/ERR505088_1.fastq.gz
wget -O W2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505088/ERR505088_2.fastq.gz
wget -O W3_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505084/ERR505084_1.fastq.gz
wget -O W3_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505084/ERR505084_2.fastq.gz
wget -O W4_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505085/ERR505085_1.fastq.gz
wget -O W4_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505085/ERR505085_2.fastq.gz
wget -O W5_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505086/ERR505086_1.fastq.gz
wget -O W5_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505086/ERR505086_2.fastq.gz
wget -O W6_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505087/ERR505087_1.fastq.gz
wget -O W6_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505087/ERR505087_2.fastq.gz
wget -O W7_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505089/ERR505089_1.fastq.gz
wget -O W7_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505089/ERR505089_2.fastq.gz
wget -O W8_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505091/ERR505091_1.fastq.gz
wget -O W8_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505091/ERR505091_2.fastq.gz
wget -O W9_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505092/ERR505092_1.fastq.gz
wget -O W9_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505092/ERR505092_2.fastq.gz
wget -O W10_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505093/ERR505093_1.fastq.gz
wget -O W10_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505093/ERR505093_2.fastq.gz
wget -O W11_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505094/ERR505094_1.fastq.gz
wget -O W11_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505094/ERR505094_2.fastq.gz
wget -O W12_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505083/ERR505083_1.fastq.gz
wget -O W12_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505083/ERR505083_2.fastq.gz
```

<!--chapter:end:19-Appendix.Rmd-->

