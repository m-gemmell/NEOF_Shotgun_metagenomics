[["01-Shotgun_metagenomics.html", "Shotgun Metagenomics Chapter 1 Introduction", " Shotgun Metagenomics Sam Haldenby and Matthew Gemmell 2021-05-19 Chapter 1 Introduction This practical session aims to introduce you to the analysis of Shotgun metagenomic data. The topics covered are: Overview Raw data Trimming data Host removal Taxonomic profiling Functional profiling Metagenome assembly Gene prediction Functional annotation Co-assembly Comparative analysis "],["02-Overview.html", "Chapter 2 Overview 2.1 What is metagenomics? 2.2 Why metagenomics? 2.3 Metagenomics vs Metagenetics 2.4 Tutorial overview", " Chapter 2 Overview 2.1 What is metagenomics? Meta /mt/ : prefix meaning higher or beyond Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome. 2.2 Why metagenomics? Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species neighbours, i.e. what else is in its natural environment? Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture. 2.3 Metagenomics vs Metagenetics Broadly speaking, there are two families of metagenomic analysis: Amplicon-based: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions. Shotgun: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referenced as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session. 2.4 Tutorial overview 2.4.1 Basics This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. Virtually the entire tutorial will be carried out on the command line, which you will hopefully now be more comfortable with. 2.4.2 Structure We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. So, there will be no instructor telling you what to type and click: Instead, everything you require to carry out the practical is written in the document. Take your time; its important to spend some time understanding why you are running the commands, rather than simply typing them out. If at any point you are having trouble or have a question, let one of us know and well provide 1-to-1 assistance. 2.4.3 Content This practical is broken up into the following broad sections. Raw data: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples. Trimming data: This entails preprocessing our data to ensure that it is of good quality. Taxonomic profiling: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples. Functional profiling: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples. Metagenome assembly: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will stitch the reads together to ensure we get the best assembly possible. Gene prediction: We will take our metagenome assembly, search for genes Functional annotation: and then functionally annotate them with information from various databases. We will then visualise some of the output. Co-assembly: Instead of just looking at the functional composition of one metagenome sample, we will discuss methods of combining all samples to carry out a co-assembly and then obtain normalised gene coverage statistics for use in comparative analyses between samples. Comparative analysis: Using our data from the previous step, we will look at a couple of different ways of comparing the functional profiles of our samples All the analyses here are just examples of how you could interrogate a metagenomic dataset: There are, of course, many other ways to tackle such a set. Dont worry if you dont manage to finish the whole practical! The more commonly used analyses have been put at the front of the practical (Section 1-4) with the less standard ones being placed towards the end. We will provide you with all of the intermediate and results files on request. "],["03-Start.html", "Chapter 3 Before we start", " Chapter 3 Before we start During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command (ensure this starts with a full stop and a space .): . useshotgun Note: The use scripts in this workshop are custom scripts that setup conda environments. You can look at the script above script with less /usr/local/bin/useshotgun is you are interested in its contents. Also, theres a chance youre currently not in your home directory, so lets make sure you are with the following command: cd ~ "],["04-Cluster_Introduction.html", "Chapter 4 Cluster Introduction 4.1 Logon instructions 4.2 The Terminal Window", " Chapter 4 Cluster Introduction 4.1 Logon instructions For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent. You will now be in a logged-in Linux VNC desktop with two terminals. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Ubuntu. The following link is a guide to install Ubuntu: https://www.ubuntu.com/download/desktop/install-ubuntu-desktop. If you use a USB you need to create a bootable USB stick. The following link will assist: https://www.ubuntu.com/download/desktop/create-a-usb-stick-on-windows 4.2 The Terminal Window In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different. Already there is useful information for us on the terminal window. nsc006: This is the login name, also known as the username. In this case nsc006 is a demonstrators account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into. gauss03: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others ~ is a shortcut to the users home directory. Everything after the $ is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen, choose Applications -&gt; Shell -&gt; bash "],["05-Raw_data.html", "Chapter 5 Raw data 5.1 Obtaining the data 5.2 Checking quality control", " Chapter 5 Raw data The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets. Have a browse of some of the projects by selecting one of the biomes on this page. We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558). 5.1 Obtaining the data First, we need to create a directory to put the data in and then change directory to it. mkdir 1-Raw cd 1-Raw Now we can generate a symbolic links (i.e. shortcut) to the raw sequence data files, which will appear in the current directory: ln -s /pub39/tea/matthew/NEOF/Shotgun_metagenomics/raw_fastq/* . All that this command did is symbolic link (like a shortcut in Windows) to the read files that we will be using (The appendix of this document contains the commands used to download these files directly from the EBI metagenomics site). If you would like to know more about symbolic links please check out: https://linuxize.com/post/how-to-create-symbolic-links-in-linux-using-the-ln-command/. Now, check they are there with: ls There should be six files in the directory, two for each sample in the dataset. e.g. K1_R1.fastq.gz The file ID has three components: K1 is the sample ID. R1 is for the forward reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). fastq.gz tells us that this is a zipped FASTQ file. The sample labelling indicates the type treatment samples. The three samples are: K1: Fecal sample of individual of Korean diets K2: Fecal sample of individual of Korean diets W1: Fecal sample of individual of Western diets So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here: This results in two sequences generated for each sequenced fragment: One reading in from the 3 end (R1) and the other reading in from the 5 end (R2). FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files: zcat K1_R1.fastq.gz | head -n 4 | less -S The pipe symbol ( | ) is used to pass the output of one command as input to the next command. So, this command (1) shows the unzipped contents of the FASTQ file, (2) displays only the first 4 lines, and (3) displays them without wrapping lines (with S, for easy viewing). The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for +) and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters. To return to the command prompt, press q. Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample). At a later point in the tutorial, you will be asked to link to results derived from the full dataset for further processing. 5.2 Checking quality control We can generate and visualise various sequence data metrics for quality control purposes using FastQC. We will run FastQC on the R1 and R2 reads separately as it is good to visualise them in two different reports. R1 and R2 reads have different quality patterns, generally due to the poorer quality of R2. Run FastQC on the files: #R1 fastqc #Make an output directory mkdir R1_fastq #Run fastqc on all the R1.fastq.gz files #* matches any pattern, #*R1.fastq.gz matches any file that ends R1.fastq.gz in the current directory #-t 3 indicates to the program to use 3 threads, chosen as there are three R1 files fastqc -t 3 -o R1_fastqc *R1.fastq.gz #R2 fastqc #Make output directory mkdir R2_fastq #Run fastqc fastqc -t 3 -o R2_fastqc *R2.fastq.gz Once the FastQC commands are run we can run MultiQC to creative interactive html reports for the outputs. #R1 multiqc fastqc report #Create output directory mkdir R1_fastqc/multiqc #Create multiqc output multiqc -o R1_fastqc/multiqc R1_fastqc #R2 multiqc fastqc report #Create output directory mkdir R2_fastqc/multiqc #Create multiqc report multiqc -o R2_fastqc/multiqc R2_fastqc Once completed, view the R1 output (NB: The &amp; runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open): firefox R1_fastqc/multiqc/multiqc_report.html \\ R2_fastqc/multiqc/multiqc_report.html &amp; The FastQC report (via MultiQC) contains a number of metrics. The Sequence Quality Histograms shows the sequence quality across the length of the reads, you can hover over each line to show which sample it belongs to. Note how quality decreases as the length of the read increases. While this is normal with Illumina sequencing, we will improve the situation a bit in the next chapter. Briefly inspect the FastQC reports (R1 and R2) for yourself  There are examples of typical (and atypical!) FastQC data in the appendix of this document. Once you have finished looking, minimise the Firefox window. MultiQC can create html reports for the output of many tools. See more here: https://multiqc.info/. "],["06-Trimming_data.html", "Chapter 6 Quality control 6.1 Removing adapters and low quality bases 6.2 Rename the files 6.3 Inspect the trimmed data", " Chapter 6 Quality control Now that weve obtained the raw data and had a look at it, we should now clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets. Remove low quality bases from the end of the reads. These are more likely to be incorrect, so are best trimmed off. Remove adapters. Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. Remove host sequences. If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome. Here, we do not need to do this, as the dataset contains barely any human genome sequences. 6.1 Removing adapters and low quality bases First go back to your home directory and create a new directory where we will clean the sequences up: cd .. This will move you one directory up, i.e. back to your home directory. Alternatively, you could use cd ~ which will take you to your home directory. This is a good idea if you ever get lost! mkdir 2-Trimmed cd 2-Trimmed You are now in your newly created directory. Here we will run Trim Galore! which carries out both of these steps. trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz This is a longer command so weve split it across multiple commands (a \\ at the end of a line allows you to press return without running the command, meaning you can continue to add to that command. When this happens, the $ changes to a &gt;. Note that if you do use the \\ character, the next character immediately after it must be return. If you use \\ in the middle of a line without pressing return afterwards, it will break the command! This command will remove any low quality regions from the end of both reads in each read pair (quality score &lt; 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too. We use the two read files for sample K1 as input, from the previous directory we were in. Task: Run this command two more times, but for the other two samples (K2 and W1) 6.2 Rename the files Once that is complete if you run: ls you will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. e.g. K1_R1_val_R1.fq.gz could be shortened to K1_R1.fq.gz. So, well rename all of the files with the mv command: mv K1_R1_val_1.fq.gz K1_R1.fq.gz mv K1_R2_val_2.fq.gz K1_R2.fq.gz mv K2_R1_val_1.fq.gz K2_R1.fq.gz mv K2_R2_val_2.fq.gz K2_R2.fq.gz mv W1_R1_val_1.fq.gz W1_R1.fq.gz mv W1_R2_val_2.fq.gz W1_R2.fq.gz Tip: If you want to edit and reuse previous commands, press the up arrow key. Task: Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt) 6.3 Inspect the trimmed data To see what difference the trimming made, run FastQC and multiqc again on the trimmed output files and view it. #R1 fastqc and multiqc mkdir R1_fastqc fastqc -t 3 -o R1_fastqc *R1.fq.gz mkdir R1_fastqc/multiqc multiqc -o R1_fastqc/multiqc R1_fastqc Task: Run fastqc and multiqc for the R2 files and then iew the R1 and R2 multiQC reports with firefox. How does the quality = compare to the untrimmed data? "],["07-Host_removal.html", "Chapter 7 Host removal 7.1 Index reference 7.2 Alignment 7.3 Unmapped read extraction 7.4 Re-pair", " Chapter 7 Host removal It is good practice to remove any host sequences from your data before analysis. A good method for this is to align/map your reads to a reference of your host genome and remove the mapped sequences (i.e sequences we believe to belong to the host). If there is no host genome available before you start your sample collections and sequencing it may be a good idea to attempt to sequence and assemble the host genome. Below is a small example on how to carry out host removal. The below uses a section of a human (host of our samples) assembly. First step is to copy over the reference fasta file you will use. cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/GRCh38_slice.fasta 7.1 Index reference This process will use the bowtie2 aligner. Prior to alignment/mapping we need to index our reference. bowtie2-build GRCh38_slice.fasta GRCh38_slice.fasta If you use ls you will now see a bunch of files starting with GRCh38_slice.fasta and ending with various suffixes that contain bt. These are the index files which allow us to use the reference with bowtie2. 7.2 Alignment With the indexed reference we will align the K1 reads to the reference. This createa a bam file that contains alignment and read information (mapped.bam). bowtie2 -x test.fasta -1 R1.fastq.gz -2 R2.fastq.gz \\ -p12 2&gt; out.log | samtools view -bSh &gt; mapped.bam 7.3 Unmapped read extraction Next step is extract the reads that did not map form the mapped.bam file (unmapped reads). samtools fastq -f 4 -1 K1_R1.u.fastq -2 K1_R2.u.fastq mapped.bam The above step may make unmatched paired files. This occurs when a read from R2 is removed but the matching read in R1 is not removed, or vice versa. This will cause issues for further analysis. 7.4 Re-pair The below BBTools (https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/) command will re-pair the reads by removing a read were its pair is missing and making sure the order of the reads is identical in the 2 paired files. repair.sh in1=K1_R1.u.fastq in2=K1_R2.u.fastq \\ out1=K1_R1.final.fastq out2=K1_R2.final.fastq \\ outs=singletons.fastq The file singletons.fastq contains the left over singletons (a sequence missing a pair) and can normally be ignored. As our data has pretty much no human data we will skip this step for the other samples and use the trimmed data for the downstream analysis. In a real analysis project you would use a whole genome reference for your host. However, that would have taken too long for this practical. The most current Human reference (when this was written) is GRCh38. We used a random 10kb section to align our reads to. For more resources on the Human reference please see: https://www.ncbi.nlm.nih.gov/genome/guide/human/ The assembly we used was: https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz "],["08-Taxonomic_profiling.html", "Chapter 8 Taxonomic profiling 8.1 Kraken2 8.2 Bracken 8.3 LEfSe biomarker detection", " Chapter 8 Taxonomic profiling There are a number of methods for determining the species composition of a metagenomic data-set, but for the purposes of this practical we will use Kraken &amp; Bracken (Bayesian Reestimation of Abundance with KrakEN). Kraken classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. We will be using Kraken 1 as Kraken 2 is still in beta. Bracken uses the taxonomic labels assigned by Kraken to compute the abundance of species in a set of DNA sequences. First, well make a new directory for it and move into it, after returning home: cd .. mkdir 3-Taxonomy cd 3-Taxonomy 8.1 Kraken2 Prior to running kraken we need to set a variable so kraken2 knows where to look for the databases it will use. export KRAKEN2_DB_PATH=/pub39/tea/matthew/NEOF/Shotgun_metagenomics/kraken2_db Note: You can look at the contents of the above directory to see it currently contains the MiniKraken database. This database contains only a subset of the bacteria, archaea, and viral kraken2 libraries. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full Kraken2 database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. See the appendix for a link to the Kraken2 manual. Now, run Kraken2 on sample K1 by running the following command. Note: As there is little human sequence in our data we will not use host removed data. kraken2 --paired --db minikraken2_v1_8GB \\ --output K1.kraken --report K1.kreport2 \\ ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz While this is running, lets look at what those command line options do: ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz : the trimmed read pairs for K1, which we use as input. --paired : Indicate that we are providing paired reads to Kraken. Internally, Kraken will concatenate the R1 and R2 reads into one sequence with an N between them. --db : Specify the kraken database to be used for taxonomic classification. Previous to the command we set the KRAKEN_DB_PATH so in this case the command will look for the directory called minikraken_4GB within the KRAKEN_DB_PATH. Alternatively the full path of the required database could be provided. --threads : How many CPUs the process will use. --output : This is the output file. There are two major output formats from Kraken2: --output, .kraken: Each sequence (or sequence pair, in the case of paired reads) classified by Kraken 2 results in a single line of output. Kraken 2s output lines contain five tab-delimited fields; from left to right, they are: C/U: a one letter code indicating that the sequence was either classified or unclassified. The sequence ID, obtained from the FASTA/FASTQ header. The taxonomy ID Kraken 2 used to label the sequence; this is 0 if the sequence is unclassified. The length of the sequence in bp. In the case of paired read data, this will be a string containing the lengths of the two sequences in bp, separated by a pipe character, e.g. 98|94. A space-delimited list indicating the LCA mapping of each k-mer in the sequence(s). For example, 562:13 561:4 A:31 0:1 562:3 would indicate that: the first 13 k-mers mapped to taxonomy ID #562 the next 4 k-mers mapped to taxonomy ID #561 the next 31 k-mers contained an ambiguous nucleotide the next k-mer was not in the database the last 3 k-mers mapped to taxonomy ID #562 Note: that paired read data will contain a |:| token in this list to indicate the end of one read and the beginning of another. --report, .kreport2: The report output format. This is required for bracken. It is tab-delimited with one line per taxon. The fields of the output, from left-to-right, are as follows: Percentage of fragments covered by the clade rooted at this taxon Number of fragments covered by the clade rooted at this taxon Number of fragments assigned directly to this taxon A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., G2 is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank. NCBI taxonomic ID number Indented scientific name The output to screen will show how many sequences are classified. This will be lower than normal as we are using the mini Kraken database. In a real analysis you may use the option --confidence which represents the Confidence score threshold. The default is 0.0, which is the lowest, with the maximum value being 1. A good place to start may be 0.1. Too many classifications are removed if you attempt it with this dataset, due to the minikraken database used, so we will not use this. More info on the confidence scoring can be found at: https://github.com/DerrickWood/kraken2/wiki/Manual#confidence-scoring Task: Once the Kraken2 command has finished running, run it on the other two samples. Hint: You will need to change all instances of K1 to K2 or W1 in the above command 8.1.1 Krona plot Krona is an interactive metagenome species abundance visualisation tool. We can use the Kraken2 report files to create our Krona plots. With the below command we can inport our Kraken2 taoxnomy (within the report file) into a krona html. ktImportTaxonomy.sh -o kraken2.krona.html *.kreport2 -o is our output html file, and the final argument *.kreport2 represents all of our .kreport2 files in the current directory: The * is a wild-card, meaning any characters any number of times, so this identifies the files K1.kreport2 K2.kreport2 and W1.kreport2. You will get a warning that not all taxonomy IDs were found. We will ignore this but in your own future installations this shoudl be addressed with the updateTaxonomy.sh command. Now we can view our interactive chart in a web browser. firefox kraken2.krona.html &amp; Can you tell which sample looks the most different in terms of bacterial species that are present and absent? 8.2 Bracken Bracken (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by Kraken to compute estimated abundances of species in a metagenomic sample. Just like with Krona we can use the kraken2 report files to run bracken. bracken -d $KRAKEN_DB_PATH/minikraken2_v1_8GB \\ -i K1.kreport -o K1.bracken -r 100 -l S -t 5 Lets look at what those command line options do: -d : Specify the Kraken database that was used for taxonomic classification. In this case bracken requires the variable $KRAKEN_DB_PATH so the option is provided the full path to the kraken database. For clarity try the command ls ${KRAKEN_DB_PATH}/minikraken_4GG -i : The Kraken report file, this will be used as the input. -o : The output bracken file. Information about its contents is below. -r 100: This is the ideal length of the reads that were used in the kraken classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as it is a paired library of 100 bp reads. -l S: This specifies the taxonomic level/rank of the Bracken output. In this case S is equal to species with the other options being D,P,C,O,F and G. -t 5: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10). The output file of Bracken contains the following columns: Name: Name of taxonomy at the specified tax level. Taxonomy ID: NCBI taxonomy id Level ID: Letter signifying the taxonomic level of the classification Kraken assigned read: Number of reads assigned the taxonomy by Kraken2. Added reads with abundance reestimation: Number of reads added to the taxonomy by Bracken abundance reestimation. Total reads after abundance reestimation: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis. Fraction of total reads: Relative abundance of the taxonomy. Use less or vim to look at the bracken output. Task: Repeat the above commands for K2 and W1 To make full use of Bracken output, it is best to merge the output into one table. However before we do this well copy the Bracken output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples. cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/bracken/* . Now to merge all the Bracken files. combine_bracken_outputs.py --files *.bracken -o all.bracken This output file contains the first three columns: - name = Organism group name. This will be based on the TAX_LVL chosen in the bracken command and will only show the one level - taxonomy_id = Taxonomy id number - taxonomy_lvl = A single string indicating the taxonomy level of the group. (D,P,C,O,F,G,S). Following these columns are two columns for each sample. ${SampleName}.bracken_num: The number of reads after abundance reestimation ${SampleName}.bracken_frac: Relative abundance of the group in the sample We want a file with only the first column (organism name) and the bracken_num column for each sample. We will therefore use the following commands. First we create a sequence of numbers that will match the bracken_num column numbers. These start at column 4 and are every even numbered column after this. In this case seq will create every 2 numbers from the numbers 4 to 50 with commas (,) as separators (-s). Note: The number 50 is chosen as 3 (1st three info columns) + 24*2 (24 samples with 2 columns each) = 50. bracken_num_columns=$(seq -s , 4 2 50) echo $bracken_num_columns Now to use the variable to extract the bracken_num columns plus the first column (species names). cat all.bracken | cut -f 1,$bracken_num_columns &gt; all_num.bracken 8.3 LEfSe biomarker detection We will use LEfSe (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. LEfSe couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions. In essence it allows for the detection of biomarkers when comparing sample groups. In the LEfSe terminology the sample groups are called the class. We need to format our bracken file to be ready for LEfSe. First we will copy the file so we have a backup in case we do anything wrong. cp all_num.bracken all_num.lefse.bracken This next part must be done for further commands to work Using your favourite text editor (e.g. nano, vim, etc.) add the following line to the top of your all_num.lefse.bracken file. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator. diet K K K K K K K K K K K K W W W W W W W W W W W W Note: The above is diet followed by 12 K and 12 W The singular line should match the order of your samples within the file. This is the metadata line that LEfSe will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples. LEfSe requires python2 whilst we have been using packages that required python3. We therefore need to use a different conda environment. We are currently using the conda env called shotgun_meta as represented by (shotgun_meta). This was activated with the command . useshotgun. Open a new terminal (right click on the main screen background, choose Applications -&gt; Shell -&gt; bash) and run the following in the new terminal to activate the lefse conda environment. #Setup environment . uselefse #Change directory cd ~/3-Taxonomy We need to further format and preprocess our file with a LEfSe script. lefse-format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000 all_num.lefse.bracken : Input Bracken file. all_num.lefse : Output file formatted for the run_lefse command, which we will soon run. -c 1 : Specifies the row with the class info. This is used to determine which samples will be compared against which samples. In this case it is the first row with the Ks and Ws. -u 2 : Specifies the row with the sample names. This is the second row in this case. -o 1000000 : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. LEfSe developers recommend 1000000 (1 million) when very low values a present. We generally always use 1 million for consistency. Now to run LEfSe. All we need to do is run the command with the formatted input and provide an output file name. run_lefse.py all_num.lefse all_num.lefse.out The output file is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a -. Biomarker name Log of highest class average, i.e. Get the class with the greater amounts of the biomarker, average the counts and then get the log of this value. Class with the greater amounts of biomarker LDA effect size: A statistical figure for LEfSe. Only features with an LDA &gt;2 are detected as biomarkers by default. The higher the LDA effect size is the more of an effect the biomarker causes. p-value: biomarkers must have a p-value of &lt;0.05 to be considered significant. The LDA effect size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes. Next we can visualise the output. lefse-plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png --dpi 200 : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. Weve chosen 200 as it is good quality and we will not be publishing these results. --format png : Format of output file. png is a commonly used file format for images. all_num.lefse.out : LEfSe output to visualise. biomarkers.png : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance. Look at the figure with the program okular: okular biomarkers.png Which species causes the biggest effect in the W class and in the K class? Which class has more biomarkers associated with it? Note: In this instance green Bars represent biomarkers in higher abundance in the W samples whilst the red bars represent biomarkers in higher abundance in the K samples. "],["09-Functional_profiling.html", "Chapter 9 Functional profiling 9.1 HUMAnN2 9.2 Statistical comparison between samples", " Chapter 9 Functional profiling It is also possible to investigate functional differences between metagenome (and metatranscriptome) samples by directly interrogating the read data. We will now look at how this can be done with a package called HUMAnN2 (The HMP Unified Metabolic Analysis Network 2), a pipeline designed to accurately profile the presence/absence and abundance of microbial pathways in metagenomic sequencing data. We need a new conda environment again. Open a new terminal (right click on the main screen background, choose Applications -&gt; Shell -&gt; bash) and run the below: . usehumann2 9.1 HUMAnN2 First, we will carry out an example run of the software and briefly explore the output files. HUMAnN2 can take a long time to run so we will use a small amount of example data. Make a new directory and move into it, after moving back to your home directory. cd ~ mkdir 4-FunctionalProfiling cd 4-FunctionalProfiling Now we will perform the run with HUMAnN2 so we can inspect the output files. First, copy over some test data. $DB is simply an alias used for a directory containing database files used in this tutorial. cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/humann2/demo.fq.gz . This is a demonstration FASTQ file that we will use. It will be small enough to run HUMAnN2 in a reasonable time. humann2 \\ --input demo.fq.gz \\ --output demo.humann2 \\ --threads 10 Here, we are telling the software to use demo.fq.gz as input and to create a new output directory where results will be generated. As the software runs, you might notice that as part of the process, HUMAnN2 runs MetaPhlAn2. The purpose of this is to identify what species are present in the sample, so that it can then tailor generate an appropriate database of genes (from those species) to map against. It will carry out this alignment against the gene database, then a protein database, and finally compute which gene families are present to determine which functional pathways are present and how abundant they are. Once the run has completed, change into the newly created output directory and list the files that are there. cd demo.humann2 ls You will see that there are three files and one directory. The directory (demo_humann2_temp) contains intermediate temporary files and can be disregarded here. Sometimes, these files can be useful for debugging, however. The three files are: demo_genefamilies.tsv: A table file showing the number of reads mapping to each UniRef90 gene family, Values are normalised by the length of each gene family (i.e. RPK, or Reads per Kilobase). Additionally, the values are stratified so that they show the overall community abundance but also a breakdown of abundance per species detected. This allows researchers to delve into species specific functions, rather than only looking at the metagenomic functions as a whole, demo_pathabundance.tsv: As above, a table file showing the normalised abundance of MetaCyc pathways. These abundances are calculated based on the UniRef90 gene family mapping data and are also stratified by species. demo_pathcoverage.tsv: Similar to above, except instead of abundances of pathways, this table shows the coverage, or completeness, of pathways. For example, a pathway may contain 5 components (or genes/proteins) Pathway1 : A  B  C  D  E 100% complete A species identified in the sample may only have four of the components, though, meaning that the pathway is only 80% complete (represented as 0.8) Pathway1 : A  B  C  D  E 80% complete The basic format of these three output files is the same, so lets take a look at the pathway abundance table. less demo_pathabundance.tsv You will see that there are two columns. The first shows the pathway (or UNMAPPED where reads could not be aligned, and UNINTEGRATED where reads could be aligned, but to targets not implicated in any pathways). The second column shows the abundance. Press q to exit and lets look at one specific pathway, COA-PWY-1 (a coenzyme A biosynthesis II pathway). grep COA-PWY-1 demo_pathabundance.tsv This shows two entries with two different values, I.e. COA-PWY-1: coenzyme A biosynthesis II (mammalian) 6.3694267516 COA-PWY-1: coenzyme A biosynthesis II (mammalian)|g__Bacteroides.s__Bacteroides_vulgatus 4.7961630695 This is an example of the species stratification mentioned above. The first line shows the abundance of this pathway across the whole sample, but the second line shows the abundance contributed from Bacteroides_vulgatis. Have a look at the other two output files; note the similar layout. Finally, return to the parent directory i.e. 4-FunctionalProfiling cd .. 9.2 Statistical comparison between samples Looking at the functional profile of one sample in isolation is usually not very informative. First, there is nothing to compare it to and second, there are no biological replicates. We will therefore use all the Korean and Western diet samples. It would take many hours to analyse all of the data using HUMAnN2 and is outside of the scope of this practical. For this reason, samples were analysed prior to the workshop to generate the output files we covered above. For the purposes of this comparison, we will look at the pathway abundances only. First copy over the results data directory and have a look in it (when copying directories, we need to use r with the cp command, to recursively copy all files across) cp r /pub39/tea/matthew/NEOF/Shotgun_metagenomics/DietPathAbundance . ls DietPathAbundance You will see there are 12 files prefixed with K and 12 prefixed with W, for the Korean diet and Western diet samples, respectively. Take a look in one of the files; you might notice that there is no species stratification. The reason for this is that for this test, all pathway abundances have been collapsed to a community level, ignoring differences in species. When you analyse your own data-sets, you can leave this stratification in to identify significant differences between functional profiles at a species level. 9.2.1 Combining data First, we need to combine these 24 tables into one large results table. HUMANn2 provides a tool to do this: humann2_join_tables --input DietPathAbundance/ --output diet.tsv This command will look for all tables in the DietPathAbundance directory and generate a large, 24 column table called diet.tsv. You can take a look to see that this has worked correctly. less -S diet.tsv 9.2.2 Renormalising data The next step is to renormalise the data. Currently, all of the abundance values are only normalised within each sample, i.e. accounting for the size of the pathways/length of genes. However, they are not normalised between samples, and this is very important. For example, if we had sequenced two samples, A and B, and we obtained 5 million reads for sample A and 20 million reads for sample B, without normalisation, it might look that there was up to 4x as much functional activity in sample B! To correct for this, we normalise the abundance values based on the number of reads in each samples, i.e. counts per million, or cpm. (We could also normalise to relative abundance where all abundances for each sample add up to 1). humann2_renorm_table \\ --units cpm \\ --input diet.tsv \\ --special n \\ --output diet.cpm.tsv This command generates the normalised data in the new table diet.cpm.tsv. The --special n option tells the script to remove all unmapped and unassigned values from the table. 9.2.3 Visualisation Now that we have our data normalised, we can visualise the dataset to see how the two groups look, i.e. do samples in the same diet group appear to correlate well with each other and are they distinguishable from those from the other diet group? To do this, we will draw a PCA plot. So that the plot will show which samples are from which group, we will need to provide the script with some metadata. This has been prepared and can be copied over as follows cp /pub39/tea/matthew/NEOF/Shotgun_metagenomics/diet.metadata.tsv . This is a table file where the first column is the sample name with subsequent columns representing categories of metadata. Here, the column of interest is the second one, Diet, and samples are labelled as either Western or Korean. We can now run the PCA plot generation script and look at the result. PCA_plot.r diet.cpm.tsv diet.metadata.tsv Diet diet.pca.pdf This command takes our data table as the first argument, our metadata table as the second argument, the metadata category of interest as the third argument and the output file as the final argument. Note: PCA_plot.r is a custom R script that will be provided after the course. Now we can view the plot. okular diet.pca.pdf &amp; You should see 3 plots, each one plotting 2 of the first 3 principal components against each other (see following image). The axis of each plot also shows how much of the overall variance that that particular component accounts for. Blue and red dots show the Western and Korean diet samples, respectively, and samples that cluster closely together are more similar to each other. From this, we can see that there is some separation between the two dietary groups, but that they are not completely separated, i.e. there is some overlap. This can happen frequently with datasets such as this, that are likely to be much less controlled than laboratory samples and therefore show more between-group variance. Questions: Is there anything unexpected about any of the samples, from these plots? If so, what might be a sensible course of action before proceeding with statistical analysis? 9.2.4 Finding statistically significant differences For the final part of this section, we will see if there are any statistically significant differences between the two sample groups. There are several ways in which this can be achieved but we will carry out LEfSe again. Go back to your LEfSe terminal (or create a new one and use . uselefse). Then change directory to 4-FunctionalProfiling) First make a new copy of the file we will work on: cp diet.cpm.tsv diet.cpm.lefse.tsv Change the file diet.cpm.lefse.tsv with you favourite text editor to make it LEfSe compatible. 2 edits are required: Change the # Pathway in the first line to name. Add a new line at the top for the diet metadata (same as we did for the Bracken data) As you have carried out LEfSe already we will have one code box showing all the commands. #LEfSe format lefse-format_input.py diet.cpm.lefse.tsv diet.cpm.lefse -c 1 -u 2 -o 1000000 #Run LEfSe run_lefse.py diet.cpm.lefse diet.cpm.lefse.out #Produce LEfSe plot lefse-plot_res.py --dpi 200 --format png diet.cpm.lefse.out biomarkers.png #View plot okular biomarkers.png Look at the output and see what pathways count as biomarkers for the 2 groups. That completes the non assembly approach to shotgun metagenomic analysis. The next chapters will cover an assembly approach. "],["10-Metagenome_assembly.html", "Chapter 10 Metagenome assembly 10.1 A primer on short read assembly 10.2 Stitching read pairs 10.3 Assembly", " Chapter 10 Metagenome assembly So far we have directly analysed the read data itself which is perfectly fine for taxonomic profiling and for certain methods of functional profiling. However, Illumina reads are generally short and therefore can not provide us with much data on larger constructs that are in the metagenomic samples, e.g. genes. While it is possible to predict from which gene a sequence read might originate, the short nature of the query can sometimes lead to ambiguous results. Additionally, depending on the application it can become computationally intensive to analyse large numbers of reads. Here, we are only using samples with 1 million reads. Some metagenome samples consist of 50-100 million+ read pairs. If such a sample belonged to a set of 100 samples, that would be up to 10 billion read pairs, or 2 trillion bases of sequence data, with many of these being redundant. For this reason, it is sometimes advantageous to assemble the reads into contigs, using a meta-genome assembler. This has the dual effect of: Reducing the overall size of the data for analysis. If a metagenome was sequenced at 50x depth, then by assembling it you could theoretically reduce the amount of sequence to analyse by 50-fold. Increase the size of the fragments you will analyse. This is the main advantage of an assembly, as the ~100 bp reads can be pieced together to form 100,000 kb+ contigs. These contigs will contain complete genes, operons and regulatory elements: Reconstructed genome sections. Here, we will carry out a couple of assemblies on our dataset. 10.1 A primer on short read assembly Illumina reads are too short and numerous to use traditional overlap-layout-consensus assemblers as such an approach would be far too computationally intensive. Instead, we use De Bruijn graph based assemblers. Briefly, these operate as follows: All reads are broken down in to k-length overlapping fragments (k-mers). e.g. if we choose a k-mer size of 5 bp, the following two sequences (blue) would be broken down into the k-mers below them (red): (A plot need to go here, see old manual) All k-mers are linked to other k-mers which match with a k-1 length overlap (i.e. that overlap by all but one base: GGCATGCATGCCATGCATGCATGCAGGCAGGCAGGA Paths are routed through the graph and longer contigs are generated: __G__GCAT__G__CATGC__C__ATGC__A__TGCA__T__GCAG__G__CAGGCAGGA  GGCATGCAGGA The example here is a vast oversimplification of the complexity of a De Bruijn graph (i.e. there are no branches!). Routing through the graph is never as simple as this as some k-mers will lead to multiple k-mers, which can result in the break point of a contig. This is especially true for complex metagenomic data. Generally speaking, the shorter the k-mer, the more branches there will be, the trickier the graph is to resolve, so the resulting contigs are smaller. Assemblers usually perform better with longer k-mer lengths but even then there might not be enough depth of sequencing to generate all k-mers that form overlaps, therefore leading to break points. Finding the right k-mer size usually involves testing several. Fortunately, the assembler we will use, MEGAHIT, allows us to build an assembly using multiple k-mer lengths iteratively. The other great advantage about MEGAHIT is that it is quick and efficient. We will use MEGAHIT on our data soon, but first there is an additional processing step for our sequences 10.2 Stitching read pairs As mentioned, longer k-mers generally perform better, but as our maximum read length is 100 bp, we are limited to a maximum k-mer length of 99 bp. However, we can get even longer k-mers if we stitch our read pairs together. Remember that a read pair consists of two sequences read from each end of a fragment of DNA (or RNA). If the two sequences meet in the middle of the fragment and then overlap, there will be a region of homology which we can use to merge the two reads in the pair together (See next image). First, we obtain our forward and reverse reads, derived from different ends of the same fragment. Second, we look for sufficient overlap between the 3 ends of our sequences. Third, if there is sufficient overlap, we combine, or stitch, the two reads together to form one long sequence. Once we have longer stitched reads, we can increase the k-mer length for our assembly. There are a number of pieces of software that can be used to stitch reads (e.g. Pear, Pandaseq) but today we will use one called FLASH: Make a new output directory for the stitched reads and run FLASH: cd .. mkdir 5-Stitched cd 5-Stitched flash -o K1 -z -t 12 -d . \\ ../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz Here, we are telling flash to use an output file name prefix of K1, that the input is zipped, that the output directory is here (.) and to use the two read files for Sample K1. Once FLASH has finished running, it will display on screen how well the stitching process went, in this case a low amount of reads were combined. Have a look what files have been generated ls We have three new fastq.gz files. One containing the stitched reads (K1.extendedFrags.fastq.gz) and two containing the reads from pairs that could not be combined (K1.notCombined_1.fastq.gz and K1.notCombined_2.fastq.gz). We can also see what the new read lengths are: less K1.histogram Scroll down with the down key and you will see that we are looking at a histogram showing the proportion of reads at different lengths. We can now start assembling our stitched reads for this sample. 10.3 Assembly Create a new directory to store our assembly in and run the metagenome assembler MEGAHIT using our newly stitched read data. cd .. mkdir 6-Assembly cd 6-Assembly megahit \\ -r ../5-Stitched/K1.extendedFrags.fastq.gz \\ -1 ../5-Stitched/K1.notCombined_1.fastq.gz \\ -2 ../5-Stitched/K1.notCombined_2.fastq.gz \\ -o K1 \\ -t 12 \\ --k-list 29,49,69,89,109,129,149,169,189 Here, you have instructed MEGAHIT to use both the stitched and unstitched reads, to output the assembly in a subdirectory called K1 and to use 12 CPUs. The last option --k-list instructs MEGAHIT to first generate an assembly using a k-mer size of 29 bp and when that is complete, integrate the results into an assembly using a k-mer size of 49 bp, and so on up to a final iteration using a k-mer size of 189 bp. This large range of k-mer lengths should give us a good assembly, given the data. However, it may take a while to run so this might be a good time to either read on, or go back and look at some of the questions/suggestions in green that you have missed. If you need a command prompt (your current one is gone because MEGAHIT is running), Right-click on the grey background  Applications  Terminal Emulators  Xterm, to open a new terminal. Once the assembly is completed, we can look at the output FASTA file containing the contigs: less K1/final.contigs.fa We can also generate some metrics based on the assembly assembly.py \\ -i K1/final.contigs.fa \\ -o K1/final.contigs.stats \\ -c stats less K1/final.contigs.stats This stats file tells us quite a bit about the assembly quality. Two definitions that you may not be aware of are N50 and N50 length (or, somewhat confusingly, L50 and N50, respectively!). If we were to order our contigs from largest to smallest, and total up the sizes from biggest downwards, the contig we reach where our total is 50% of the size of the whole assembly is the N50 contig (the smaller the number the better). The N50 length is the length of this contig; a weighted median contig length. (add vis to explain N50) How do the contig metrics compare to the original reads? Now we have an assembly, albeit not a brilliant one due to us only having used 1 million reads, we can start to explore it. "],["11-Gene_prediction.html", "Chapter 11 Gene Prediciton 11.1 Running the software", " Chapter 11 Gene Prediciton If we want to do functional analysis of our new metagenome assembly, we will first want to identify putative genes within it. 11.1 Running the software A popular tool for identifying genes (and therefore proteins) in assemblies based on sequence composition is Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm). Create a new directory and run Prodigal on your assembly: cd .. mkdir 7-GenePredictions cd 7-GenePredictions prodigal -p meta \\ -i ../6-Assembly/K1/final.contigs.fa \\ -a K1.faa -d K1.fna -f gff -o K1.gff Here, we tell prodigal to run in metagenome mode (default is single genome mode). We also ask it to generate an amino acid FASTA file with protein predictions (K1.faa) in addition to our gene FASTA file (K1.fna). Finally, we instruct Prodigal to output a GFF (General Feature File), which contains all the gene prediction information. After a few minutes, take a look at the output files ls We have 3 new files, as requested. less -S K1.gff less K1.fna This file contains our predicted genes less K1.faa and this one contains our predicted proteins. For each gene, a corresponding protein with the same ID is predicted. In both of these files, you will notice that the headers are quite long and complex. This can interfere with downstream applications, so well rename these now to avoid trouble later on RenameHeaders.py --fasta K1.fna --name K1 \\ --zeros 6 &gt; K1.newHeaders.fna Now take a look at the new, re-headered gene FASTA file less K1.newHeaders.fna Much tidier. Rename the headers in the protein FASTA file in a similar way to the command used above Before we move on, well check how many genes have been predicted in our mini-metagenome for Sample K1. grep -c &quot;^&gt;&quot; K1.newHeaders.fna This command searches for any line starting with &gt;. The -c flag tells grep to output the number of lines found with this pattern, i.e. the number of headers and therefore the number of genes. "],["12-Functional_annotation.html", "Chapter 12 Functional annotation 12.1 Annotation types 12.2 Running Cognizer 12.3 Visualisation", " Chapter 12 Functional annotation Now that we have a set of genes/proteins predicted in our metagenomic sample, we can functionally annotate them. As always, there are a variety of methods for obtaining functional annotations (e.g. rpsBLAST vs CDD/KOG, BLASTKoala). Today we will use an all-in-one solution called Cognizer which assigns functions inferred from mapping to COG categorisations (KEGG, COG, SEED, GO, Pfam). This framework tool was actually intended for use with reads as input rather than gene/protein sequences: It will take reads and runs a translated BLAST (BLASTP) search against a custom protein database, before determining a variety of functional annotations for each read. Unfortunately, this is a very processor intensive and slow process so we are cutting the workload down dramatically by instead providing the software with our predicted protein sequences. Even then, this process takes too long to complete in a tutorial session, so we will instead run the program on a subset of our data, i.e. 2000 proteins. To do this, we will use Seqtk (Sequence tool kit) seqtk sample K1.newHeaders.faa 2000 &gt; K1.sample.faa This command simply tells Seqtk to select 2000 entries from K1.newHeaders.faa and output them in K1.sample.faa. This resulting set of proteins will be much more manageable. 12.1 Annotation types Cognizer is a very useful tool because it assigns multiple different functional annotations in one run. Below is a brief description of these annotation types. COG: Clusters of Orthologous Groups database generated by comparing predicted and known proteins in all completely sequenced microbial genomes. COG Functional Description: Based on single letter annotation, these descriptions categorise the function of the gene/protein. A table of these can be found in the appendix of this document. KEGG: A database resource for understanding high-level functions. Annotations range from small molecules up to whole biological pathways. Today we will touch upon KEGG Orthology (KO systems) which can be used to construct KEGG pathway modules (functional units) which are part of the larger KEGG pathways. Pfam: A database comprising a large collection of protein families, each represented by multiple sequence alignments and hidden Markov models. GO: Gene Ontology database that classifies functions along three aspects: molecular function (i.e. activities of gene products), cellular function (i.e. where the gene product is active) and biological process (e.g. pathways) FIG: Set of protein sequences that are similar along their full length (not just based on a domain). All of the proteins within a single FIG family (FIGfam) are believed to implement the same function SEED: Database created in 2004 for genome annotation annotation purposes, which predicts gene function and allows for the discovery of new pathways. 12.2 Running Cognizer Make a new directory to store the annotations in. cd .. mkdir 8-GeneAnnotations cd 8-GeneAnnotations cognizer_aa -d $DB/Cognizer/ -e -10 \\ -i ../7-GenePredictions/K1.sample.faa -t 12 -o K1.sample Here, we have asked cognizer to annotate out proteins with 12 CPUs and output any BLAST matches with an e-value of 1x10-10 or lower to an output directory named K1.sample. Once Cognizer has finished, see what files it has generated: ls K1.sample You will see there are six stat files which contain the number of incidences of proteins matching an annotation from each of the above databases. The one other file, assignments.txt, contains a table showing all of the query protein sequences and the annotations found for that protein. Have a look in some of the .stat files (e.g. cog.stat) to see what the annotations look like. Take a look at assignments.txt too to see if you can match up the columns with the annotation types 12.3 Visualisation Before we move on to the next part of the tutorial, we will visualise some of the functional annotations we have assigned to our metagenome. 12.3.1 COG categories We can quickly create a graph to show the frequency of genes assigned to each of the COG categories. First, we need to tabulate and calculate the frequencies of the results which reside at the top of the K1.sample/cog.stat file. getCogProportions.sh K1.sample/cog.stat K1 &gt; K1.sample.cog.tsv The second argument, K1, is to tell the script the ID of the sample for graphing purpose. We can now draw our graph: CogBarChart.R -i K1.sample.cog.tsv -o K1.sample.cog.pdf okular K1.sample.cog.pdf &amp; _PLOT HERE The bar-plot tells us the proportion of genes belonging to each COG functional category, which you can look up in the appendix of this document., but thats not particularly informative unless we can compare it with something: We will come to this soon. 12.3.1.1 KEGG pathways The above section told us the proportion of genes assigned to different COG functional categories, but nothing about the relationship between genes. To do this we will use the KEGG annotations that we obtained, but instead of just using a subset of the data, we will use annotations generated from the whole of sample K1, calculated prior to the workshop. Copy the full file over: cp $DB/FullAnalysis2019/8-GeneAnnotations/K1.f/assignments.txt \\ K1.full.assignments.txt If you take a look at the file, you will see that the first column consists of the gene/protein id, and the fourth column consists of one or more KO (KEGG orthologue) identifiers. For now, these are the only column we need so we will trim the rest away. getKeggFromAssignments.sh \\ K1.full.assignments.txt &gt; K1.full.kegg.tsv This has created a two column file: gene id and KO id, which we can submit on the KEGG website. There, it will reconstruct functional pathways highlighting the presence or absence of the identified KEGG orthologues. Open Firefox firefox &amp; and navigate to www.genome.jp/kegg/tool/map_pathway.html. Here, press the Browse button and select your home directory from the right side panel (Places). NB: Your home directory will start with nsc. Then, select 8-GeneAnnotations  K1.full.kegg.tsv, then press Exec. PLOT HERE After a few moments, a long list of pathways will appear which you can click on to view. The number in brackets after the pathway shows the number of KEGG orthologue matches identified in this pathway. Try selecting some of the pathways and examine the pathway diagrams. Components marked in green have been identified in the sample and using this data we can identify complete and broken pathways of interest. See image below. PLOT HERE Ultimately, there is a lot of information here, but it only becomes fully useful if we can either make it more quantitative, compare it to other samples, or both. Currently, the data we have is not representative of what is in the sample beyond a qualitative measure, i.e. we know that Gene A might be present in K1, but we dont know how much of Gene A is there. Once we have this information and the same information for K2, W1, and other samples we can start making some real comparisons. This will be covered in the following section. "],["13-Co_assembly.html", "Chapter 13 Co-assembly and coverage 13.1 Running the co-assembly, gene predictions and annotations 13.2 Generation of coverage metrics", " Chapter 13 Co-assembly and coverage Instead of looking at one sample alone, we will now interrogate all 3 samples together. One option for this might be to assemble each metagenome separately, and then compare functional annotations. This has the potential to lead to artefactual results, however. For example, Gene A might be seen in K1, but not in K2 or W1, but does that mean that Gene A is not present? It could be the case that Gene A is present in K2 and W1, but that there is insufficient sequencing depth of the gene in these samples and it therefore does not get assembled. Therefore, a second option is to assemble all of our metagenomes together. This can both help and hinder the assembly process: If there are an abundance of small variants between samples, these may confuse the assembler and result in shorter contigs. Conversely, the extra coverage afforded by using multiple samples together can also lead to some contigs being much longer, allowing more complete reconstruction of genomes in the populations. However, this does tend to rely on the content of the metagenomic samples being coassembled being similar in composition. One major benefit is that once we have our co-assembled metagenome, we can look at the coverage of each gene per sample, allowing for direct comparison between samples. 13.1 Running the co-assembly, gene predictions and annotations To assemble all three of our samples together, all we do is provide the MEGAHIT assembler with all three datasets simultaneously. Due to time and computational constraints we will not be doing this now, and will instead copy over the assembly which was created prior to the workshop. cd .. cd 6-Assembly cp -r $DB/PartialRun2019/6-Assembly/K1K2W1 ./ At this point, we would now predict genes (Section 5.1) and then carry out functional annotation (Section 6.2) in exactly the same manner as before, aside from file names). Again, these steps will be too time consuming to carry out in the time available so we will copy over these pre-prepared files instead. cd .. cd 7-GenePredictions cp $DB/PartialRun2019/7-GenePrediction/K1K2W1.* ./ cd .. cd 8-GeneAnnotations cp -r $DB/PartialRun2019/8-GeneAnnotations/K1K2W1 ./ 13.2 Generation of coverage metrics Now we have a co-assembly of all the samples, a gene prediction set from all the samples, and accompanying gene annotations. However, we currently dont have any data telling us the abundance of genes from each sample. To get this quantitative data, we will align our trimmed reads to our gene prediction FASTA file. This will tell us the depth of coverage for each gene, and therefore an estimate of abundance in each sample. To carry out these mappings we will use the short read mapper, Bowtie 2. If you recall, this is the same mapper that MetaPhlAn uses internally to map reads against the clade specific marker database. Once we have mapped the reads, we will lightly filter the data by searching for and removing PCR duplicated reads: These are identical read pairs that arise from amplification of fragments during library preparation, and are not biologically informative. Following this step, we will calculate sequencing depth for each gene, for each sample. Lets make a new directory and get started: cd .. mkdir 9-Coverage cd 9-Coverage Before we can map our reads to our genes, we need to index the genes FASTA so that it can be used by Bowtie 2 bowtie2-build ../7-GenePredictions/K1K2W1.newHeaders.fna \\ K1K2W1.genes This will generate a set of 6 files which will be utilised as a reference by Bowtie2. We can now align our reads to it. bowtie2 -x K1K2W1.genes \\ -p 12 \\ -1 ../2-Trimmed/K1_R1.fq.gz -2 ../2-Trimmed/K1_R2.fq.gz | \\ samtools view -bSh - | samtools sort - K1 While this is running, we can look at what this command is doing. The first part is instructing Bowtie 2 to map the trimmed files for K1 against our new reference genes files using 12 CPUs. The output of this is a SAM file which contains all the results of the read mappings. However, it is in human readable text format which makes it very large. So, we instead pass the results to a program called SAMtools which will convert it from the text SAM format to binary, more compressed and computer friendly BAM format. We then use SAMtools again to sort the output, based on the read mapping locations against the genes. Once this is complete, we will remove PCR duplicate reads. java -jar $BIN/MarkDuplicates.jar \\ I=K1.bam \\ O=K1.rmdup.bam \\ M=K1.metrics \\ ASSUME_SORTED=True \\ REMOVE_DUPLICATES=True \\ MAX_FILE_HANDLES=50 \\ VALIDATION_STRINGENCY=LENIENT Here, we are telling a Java program called MarkDuplicates (part of the Picard suite of tools) to search and remove duplicates from our BAM alignment file, and output the filtered set to a second BAM file, called K1.rmdup.bam. Finally, we will index this last BAM file and extract the number of reads mapping to each gene. samtools index K1.rmdup.bam samtools idxstats K1.rmdup.bam &gt; K1.coverage Rather than repeat the above steps for the other two samples, we will instead copy over the pre-calculated coverages that were prepared prior to this workshop. cp $DB/PartialRun2019/9-Coverage/K2.coverage K2.coverage cp $DB/PartialRun2019/9-Coverage/W1.coverage W1.coverage We can see how many reads map to each gene by looking at the resulting indexed BAM file. less K1.coverage This shows us a table with the first column representing the gene ID and the second column representing the gene length. The third column tells us how many reads mapped to this gene and the final column tells us how many reads mapped, but with low quality and therefore were rejected. We now have coverage statistics for each sample based on reads per gene. However, we should normalise these values. First we need to account for the total number of reads: If Gene A has 5 and 10 reads mapped to it in K1 and K2 respectively, it does not mean that there is more of Gene A in K2; it may be that the K2 sample has twice as many reads in total, so is proportionally the same. Secondly, we will account for the length of the genes: We would expect much longer genes to have more reads mapping to them than very short genes, so we can balance this out by accounting for gene length. The following script takes our BAM counts and normalises them to RPKM values, i.e. reads per kilobase of gene, per million reads. IdxStatsToRPKM K1.coverage &gt; K1.rpkm IdxStatsToRPKM K2.coverage &gt; K2.rpkm IdxStatsToRPKM W1.coverage &gt; W1.rpkm If you look at one of our .rpkm files, you will see that the values have been converted, i.e. normalised. We are now able to directly compare the functional profiles of our three samples. Finally, we can join these three files into one table. First, we will create a table header echo -e &quot;Feature\\tK1\\tK2\\tW1&quot; &gt; all.rpkm.tsv (\\t refers to a tab character, and we need to tell echo to convert \\t into a tab character with e, for expand). Then, we will grab the relevant columns from our files and append them to our table header paste K1.rpkm K2.rpkm W1.rpkm | cut -f1,2,4,6 &gt;&gt; all.rpkm.tsv This command places our 3 rpkm files side by side, then selects the first column (i.e. the gene Ids) and the rpkm values from the samples and appends them to our table. NB: Note the &gt;&gt; rather than a single &gt;. This appends to a file, rather than overwriting it. less all.rpkm.tsv Using this file as a base, we are now able to directly compare the functional profiles of our three samples. "],["14-Comparative_analysis.html", "Chapter 14 Comparative analysis 14.1 COG categories 14.2 KEGG pathways", " Chapter 14 Comparative analysis While we now have a co-assembly, annotations and coverage data, they are only based on 1 million read pairs from each sample. If we want accurate functional data, we should be using the much larger, full data-set. Fortunately, this has also been pre-prepared prior to this workshop, so you can now make a new directory which we will copy everything we need into, i.e. the functional annotation files, and the coverage files (rpkm.tsv) cd .. mkdir 10-FullSet cd 10-FullSet cp $DB/FullAnalysis2019/8-GeneAnnotations/K1K2W1.f/* ./ cp $DB/FullAnalysis2019/9-Coverage/all.rpkm.tsv ./ 14.1 COG categories First, we will look again at COG categories, except this time we will see a quantitative profile of each sample. To do this, we will look up which genes belong to which COG category in our assignments.txt file, and convert gene coverage to functional coverage, by consolidating the coverage values of all genes belonging to a category. GeneCoverageToFunctionalCoverage \\ --type cogFn \\ --cognizer assignments.txt \\ --table all.rpkm.tsv &gt; all.cogFn.rpkm.tsv less all.cogFn.rpkm.tsv As you can see, we now have an abundance value for each COG category for each gene, so we will now plot this data on a heatmap and a correlation heatmap. Heatmap.R -i all.cogFn.rpkm.tsv \\ -o cogFn.heatmap.pdf \\ -t &quot;COG Categories&quot; okular cogFn.heatmap.pdf PLOT The heatmap shows that there might be a difference between the samples in terms of relative abundance of COG categories, especially between the K1,K duo and the W1 sample. We will verify this with the correlation heatmap. CorrelationHeatmap.R -i all.cogFn.rpkm.tsv \\ -o cogFn.corrHeatmap.pdf \\ -t &quot;COG Categories&quot; --text okular cogFn.corrHeatmap.pdf &amp; PLOT What does the correlation heatmap suggest? 14.2 KEGG pathways We can generate KEGG module coverages in the same way that we just created COG category coverages. GeneCoverageToFunctionalCoverage --type kegg \\ --cognizer assignments.txt \\ --table all.rpkm.tsv &gt; all.kegg.rpkm.tsv Now that we have pathway module coverage values, we can visualise them using Cytoscape. We could use the KEGG pathway mapper as we did earlier, but Cytoscape allows for much greater customisation and more importantly, simple methods to display relative abundances of modules. Before we run Cytoscape, we will need two things: (a) a pathway map and (b) a modified RPKM table. For now, we will use the folate biosynthesis pathway which has the ID ko00790. We can download the pathway file from the KEGG website using the following command wget &quot;http://rest.kegg.jp/get/ko00790/kgml&quot; -O ko00790.xml The wget command allows you to download files from the internet via the command line. The URL is part of the KEGG REST API, and instructs the server to get ko00790 pathway in kgml format (a type of XML). We specify the output file name with O. Lets check that all worked and that the new file, ko00790.xml exists. ls You will now have an XML file containing this pathway, which we can import into Cytoscape. Now we just need to add a single column to our table which will tell Cytoscape how to use the data within it. This column contains the text: barchart: attributelist=K1,K2,W1 This instructs Cytoscape to generate barcharts from the rpkm data from our samples. There are many more options that can be used here (see enhancedGraphics Cytoscape options online) but we will keep it simple for demonstration purposes. To modify our table, use the following script: prepareTableForCytoscape.sh all.kegg.rpkm.tsv &gt; \\ all.kegg.rpkm.cyto.tsv Now we are ready to run Cytoscape. Cytoscape If this is the first time running Cytoscape, you will first need to install two apps. To do this, go to Apps on the menu bar and search for KEGGscape, and install. Once this is completed, search for enhancedGraphics and install that too. (see images below) PLOTS Once thats done, go to File  Import  Network  File. Here you may need to first select your home directory (the small house button), then navigate to 10-FullSet/ko00790.xml and press open. Press OK on the next pop-up and the map will load. Currently, there are no annotations relating to our data on it, so lets load the table we prepared into Cytoscape via File  Import  Table  File. Select all.kegg.rpkm.cyto.tsv and click Open. You will be presented with a window as shown below. PLOT Here, be sure to set Key Column for Network to KEGG_NODE_LABEL, so that it can match up our annotation data with the pathway map. Once done, click OK. Finally, click the Style tab on the left panel  Properties  Show All. PLOT Then, select Custom Graphics 1 from the list, and set the options as Column=chart and Mapping Type=Passthrough Mapping as shown below. PLOT Now, you will notice that some of the nodes on the pathway map have been annotated with bar charts. These charts show the relative abundance of that particular node in the pathway for each of the samples. PLOT You can navigate around by first selecting the Network tab on the left panel and then by dragging the box on the image in the lower part of the panel. Additionally, you can zoom using the mouse-wheel. Spend some time exploring this map and other maps. e.g. the two-component system map ko02020 which can be downloaded as before. For other ideas of maps to look at, revisit section 6.3.2 and download some of the maps from the Pathway Reconstruction Results (the pathway ids are marked next to the pathway names; just prefix them with ko) "],["15-Appendix.html", "Chapter 15 Appendix 15.1 COG Cateories 15.2 Obtaining Read Data", " Chapter 15 Appendix ##Manuals Kraken2: https://github.com/DerrickWood/kraken2/wiki/Manual Krona: https://github.com/marbl/Krona/wiki/KronaTools Bracken: https://ccb.jhu.edu/software/bracken/index.shtml?t=manual LEfSe: https://huttenhower.sph.harvard.edu/lefse/ HUMAnN2: https://github.com/biobakery/biobakery/wiki/humann2 Biobakery: https://github.com/biobakery/biobakery 15.1 COG Cateories CELLULAR PROCESSES AND SIGNALING [D] Cell cycle control, cell division, chromosome partitioning [M] Cell wall/membrane/envelope biogenesis [N] Cell motility [O] Post-translational modification, protein turnover, and chaperones [T] Signal transduction mechanisms [U] Intracellular trafficking, secretion, and vesicular transport [V] Defense mechanisms [W] Extracellular structures [Y] Nuclear structure [Z] Cytoskeleton INFORMATION STORAGE AND PROCESSING [A] RNA processing and modification [B] Chromatin structure and dynamics [J] Translation, ribosomal structure and biogenesis [K] Transcription [L] Replication, recombination and repair METABOLISM [C] Energy production and conversion [E] Amino acid transport and metabolism [F] Nucleotide transport and metabolism [G] Carbohydrate transport and metabolism [H] Coenzyme transport and metabolism [I] Lipid transport and metabolism [P] Inorganic ion transport and metabolism [Q] Secondary metabolites biosynthesis, transport, and catabolism POORLY CHARACTERIZED [R] General function prediction only [S] Function unknown 15.2 Obtaining Read Data The following commands can be used to obtain the sequence data used in this practical, directly from the EBI metagenomics site. It is worth noting that these are the full set of data, not like the miniaturised version you have used in the tutorial. wget -O K1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_1.fastq.gz wget -O K1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_2.fastq.gz wget -O K2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_1.fastq.gz wget -O K2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_2.fastq.gz wget -O W1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_1.fastq.gz wget -O W1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_2.fastq.gz "]]
